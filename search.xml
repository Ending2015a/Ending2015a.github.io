<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>[Note] Position Based Fluids</title>
    <url>/Ending2015a/49786/</url>
    <content><![CDATA[<html><head></head><body><p><img src="https://i.imgur.com/k5HI4sV.png" alt></p>
<blockquote>
<p>原論文：M.Macklin and M. Müller. Position Based Fluids. ACM Transactions on Graphics, 2013.</p>
</blockquote>
<a id="more"></a>
<iframe width="100%" height="600" src="https://www.docdroid.net/K8Qac1F/pbf-sig-preprint-pdf" frameborder="0" allowtransparency allowfullscreen></iframe>

<h2 id="1-Introduction" class="heading-control"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction<a class="heading-anchor" href="#1-Introduction" aria-hidden="true"></a></h2><p>對於互動的環境來說，流體模擬的穩定性非常重要。Smoothed Particle Hydrodynamics (SPH) 是一個在互動環境中被廣泛使用的 Particle-Based 方法，然而他卻有個致命的缺點：當粒子周圍的相鄰粒子過少時會很難維持住這個算法的穩定性，尤其以在流體表面、或是在邊緣的流體粒子更常發生。將每個 Time step 調到足夠小，或是增加足夠的粒子數量即使可避免掉這項問題，卻會大幅度增加計算成本。</p>
<p>Position Based Dynamics (PBD) 在遊戲開發或是影片製作中都是很受歡迎的一套物理模擬方式，作者選擇使用 PBD 正是因為其具有 Unconditionally Stable 以及穩定性等性質。這篇 Paper 將介紹如何使用 PBD Framework 來模擬 Incompressible flow，並克服上述在 Free Surfaces 發生 Particle Deficiency 的問題。</p>
<h2 id="2-Related-Work" class="heading-control"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work<a class="heading-anchor" href="#2-Related-Work" aria-hidden="true"></a></h2><ul>
<li>Muller [2003] 等人在 <a href="http://matthias-mueller-fischer.ch/publications/sca03.pdf">Particle-Based Fluid Simulation for Intereactive Applicatoins</a> 中提出使用 Smoothed Particle Hydrodynamics (SPH) 模擬具有 Viscosity 跟 Surface Tension 的流體</li>
<li>為了維持住 Incompressibility，<a href="https://cg.informatik.uni-freiburg.de/publications/2007_SCA_SPH.pdf">Weakly Compressible SPH (WCSPH)</a> [2007] 與標準 SPH 所使用的 Stiff Equation 會大大限制住 Time step 的長度。</li>
<li><a href="https://graphics.ethz.ch/~sobarbar/papers/Sol09/Sol09.pdf">Predictive-Corrective Incompressible SPH</a> [2009] 利用 Iterative Jacobi-style 方法，藉由不斷迭代、累積流體壓力並逐步施力的方式，來確保流體能夠在較長的Time step 上也能夠穩定存在，而不需要額外設置 Stiffness value 且可以分散掉不斷矯正相鄰粒子密度的計算成本。</li>
<li>Bodin [2012] 將 Incompressibility 組成一個 Velocity Constraints 的線性方程，並使用 Gauss-Seidel Iteration 解出線性方程來確保流體密度的一致。相反的，Position-Based 跟 PCISPH 則是使用類似 Jacobi Iteraion 的方法，解出非線性方程，並不斷重新估計誤差與梯度。</li>
<li>Hybrid Method，像是 Fluid Implicit-Particle (FLIP) 則是使用 Grid 來解出流體壓力並將流體速度擴散到粒子上模擬。<a href="https://www.cs.ubc.ca/~rbridson/docs/zhu-siggraph05-sandfluid.pdf">Zhu 與 Bridson</a> [2005] 結合 PIC/FLIP 方法。Raveendran [2011] 使用 Coarse grid 方法混合 SPH 趨近 divergence free velocity field。</li>
<li>Clavet [2005] 使用 Position Based 方法模擬 Viscoelastic Fluids。但是他們的方法只是 Conditionally Stable。</li>
<li>Position Based Dynamics [2007] 基於 Verlet integration 提供一個在遊戲中模擬物理的方法。</li>
</ul>
<h2 id="3-Enforcing-Incompressibility" class="heading-control"><a href="#3-Enforcing-Incompressibility" class="headerlink" title="3. Enforcing Incompressibility"></a>3. Enforcing Incompressibility<a class="heading-anchor" href="#3-Enforcing-Incompressibility" aria-hidden="true"></a></h2><p>對每一個粒子使用 Density Constraint 來確保流體密度不變。Constraint 是每個粒子位置與鄰近粒子位置的函數，位置寫作 $\mathbf{p}_1,\cdots,\mathbf{p}_n$，以下是對第$i$個粒子的 Constraint Function：</p>
<script type="math/tex; mode=display">
C_i\big(\mathbf{p}_1,\cdots,\mathbf{p}_n\big)=\frac{\rho_i}{\rho_0}-1\tag{1}</script><p>其中 $\rho_0$是靜止密度 (Rest Density)，$\rho_i$ 是由 SPH density estimator 計算出來的粒子密度:</p>
<script type="math/tex; mode=display">
\rho_i=\sum_jm_jW\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\tag{2}</script><p>使用 <strong>Poly6</strong> 作為 Density Estimator 的 Kernel。並使用 <strong>Spiky Kernel</strong> 計算 gradient</p>
<div class="note warning">
            <p><strong>註</strong>: 由於原論文中考慮粒子質量 $m_j$ 為定值且相同，因此在後續推倒公式中皆省略不寫，但是在本篇 Note 中會將他歸至原位。</p>
          </div>
<div class="note info">
            <p><strong>註:</strong> Poly 6 為 $6^{th}$ degree polynomial kernel 的簡稱，以下為其函數:</p><script type="math/tex; mode=display">W_{\text{poly6}}\big(\mathbf{r},h\big)=\frac{315}{64\pi h^9}\begin{cases}    \big(h^2-\lVert\mathbf{r}\rVert^2\big)^3, &0\le\lVert\mathbf{r}\rVert\le h\\    0, &\lVert\mathbf{r}\rVert\gt h\end{cases}</script><p>Poly 6 的 Gradient:</p><script type="math/tex; mode=display">\nabla W_{\text{poly6}}\big(\mathbf{r},h\big)=-\frac{945}{32\pi h^9}\mathbf{r} \big(h^2-\lVert\mathbf{r}\rVert^2\big)^2</script><p>Poly 6 的 Laplacian:</p><script type="math/tex; mode=display">\nabla^2 W_{\text{poly6}}\big(\mathbf{r},h\big)=-\frac{945}{32\pi h^9}\big(h^2-\lVert\mathbf{r}\rVert^2\big)\big(3h^2-7\lVert\mathbf{r}\rVert^2\big)</script>
          </div>
<div class="note info">
            <p><strong>註:</strong> Spiky Kernel 函數:</p><script type="math/tex; mode=display">W_{\text{spiky}}\big(\mathbf{r},h\big)=\frac{15}{\pi h^6}\begin{cases}    \big(h-\lVert\mathbf{r}\rVert\big)^3, &0\le\lVert\mathbf{r}\rVert\le h\\    0, &\lVert\mathbf{r}\rVert\gt h\end{cases}</script><p>Spiky Kernel 的 Gradient:</p><script type="math/tex; mode=display">\begin{gathered}\nabla W_{\text{spiky}}\big(\mathbf{r},h\big)=-\frac{45}{\pi h^6}\frac{\mathbf{r}}{\lVert\mathbf{r}\rVert}\big(h-\lVert\mathbf{r}\rVert\big)^2,\\\lim_{r\rightarrow0^-}\nabla W_{\text{spiky}}\big(\mathbf{r},h\big)=\frac{45}{\pi h^6},\\\lim_{r\rightarrow0^+}\nabla W_{\text{spiky}}\big(\mathbf{r},h\big)=-\frac{45}{\pi h^6},\\\end{gathered}</script><p>Spiky Kernel 的 Laplacian:</p><script type="math/tex; mode=display">\begin{gathered}\nabla^2 W_{\text{spiky}}\big(\mathbf{r},h\big)=-\frac{90}{\pi h^6}\frac{1}{\lVert\mathbf{r}\rVert}\big(h-\lVert\mathbf{r}\rVert\big)\big(h-2\lVert\mathbf{r}\rVert\big),\\\lim_{r\rightarrow0}\nabla^2 W_{\text{spiky}}\big(\mathbf{r},h\big)=-\infty\end{gathered}</script>
          </div>
<p>PBD 的目標在於找出粒子位置的修正項 $\Delta\mathbf{p}$來滿足Constraint:</p>
<script type="math/tex; mode=display">
C\big(\mathbf{p}+\Delta\mathbf{p}\big)=0\tag{3}</script><p>使用牛頓法來求解:</p>
<script type="math/tex; mode=display">
\Delta\mathbf{p}\approx\nabla C(\mathbf{p})\lambda \tag{4}</script><script type="math/tex; mode=display">
\begin{align}
C\big(\mathbf{p}+\Delta\mathbf{p}\big) &\approx C\big(\mathbf{p}\big)+\nabla C^T\Delta\mathbf{p}=0\tag{5}\\
&\approx C\big(\mathbf{p}\big)+\nabla C^T\nabla C\lambda=0\tag{6}
\end{align}</script><div class="note info">
            <p><strong>註</strong>: $\lambda$ 是一個變量。這個思路其實很簡單，我們要追求每一個 Time step 都要滿足約束函數 $C$，但是粒子的位置 $\mathbf{p}$ 不一定會滿足，因此我們必須求出修正項 $\Delta \mathbf{p}$ 來修正粒子位置直到滿足約束條件。而修正項可以定義為往 $\nabla C(\mathbf{p})$ 方向乘上一個微小變量 $\lambda$。根據 Taylor 一階展開式，$C\big(\mathbf{p}+\Delta\mathbf{p}\big)\approx C\big(\mathbf{p}\big)+\nabla C^T\Delta\mathbf{p}$ 最後再將 $\Delta \mathbf{p}$ 帶入，得到式(6)。</p>
          </div>
<div class="note info">
            <p><strong>註</strong>: $\nabla C^T$ 中的 $T$ 是 Transpose，由於 $\nabla C$ 是向量，$\nabla C^T\Delta\mathbf{p}$ 表示的是向量$\nabla C$與向量$\Delta\mathbf{p}$ 的內積，因此 $\nabla C$ 會需要 Transpose。</p>
          </div>
<p>根據 SPH 方法，我們可以將粒子 $i$ 對粒子 $k$ 的約束函數 $C$ 梯度定義為：</p>
<script type="math/tex; mode=display">
\nabla_{\mathbf{p}_k}C_i=\frac{1}{\rho_0}\sum_j m_j\nabla_{\mathbf{p}_k}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\tag{7}</script><p>根據 $i$ 與 $k$ 的關係分為:</p>
<script type="math/tex; mode=display">
\nabla_{\mathbf{p}_k}C_i=\frac{1}{\rho_0}
\begin{cases}
    \displaystyle \sum_j m_j\nabla_{\mathbf{p}_k}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big) &\text{if $k=i$}\\
    \displaystyle -m_j\nabla_{\mathbf{p}_k}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big) &\text{if $k=j$}
\end{cases} \tag{8}</script><div class="note info">
            <p><strong>註</strong>: $i$ 是自身粒子，$j$ 是鄰近粒子。由於 Kernel $W$ 是 $\mathbf{p}_i-\mathbf{p}_j$ 的函數，因此對於其他 $k\ne i$ 與 $k\ne j$ 的粒子來說其梯度皆為 $0$，只有 $k=i$ 與 $k=j$ 會滿足條件。至於對 $\mathbf{p}_i$ 或 $\mathbf{p}_j$ 的梯度差別只在於方向: 由於對象都是座標 $\mathbf{p}$，因此梯度皆為針對座標每個維度作微分，只是由於 $W$ 是 $\mathbf{p}_i-\mathbf{p}_j$ 的函數，如果是對 $\mathbf{p}_i$ 取 $W$ 梯度則根據 Chain Rule 得到 $\nabla_{\mathbf{p}_i}W=W’\nabla_{\mathbf{p}_i}(\mathbf{p}_i-\mathbf{p}_j)=W’$，而對 $\mathbf{p}_j$ 取 $W$ 梯度則得到 $\nabla_{\mathbf{p}_j}W=W’\nabla_{\mathbf{p}_j}(\mathbf{p}_i-\mathbf{p}_j)=-W’$，因此式(8)在 $k=j$的情況下加了一個負號，將方向導正。</p>
          </div>
<p>將式(8)代入式(6)求得變量 $\lambda$:</p>
<script type="math/tex; mode=display">
\lambda_i=-\frac{C_i\big(\mathbf{p}_i,\cdots,\mathbf{p}_j\big)}{\sum_k\big|\nabla_{\mathbf{p}_k}C_i\big|^2}\tag{9}</script><div class="note info">
            <p><strong>註:</strong> $|\nabla_{\mathbf{p}_k}C_i|^2$ 是因為式(6)中的 $\nabla C^T\nabla C$</p>
          </div>
<p>由於約束函數(1)是非線性，當粒子逐漸分離時，Kernel的邊緣值會趨近0，導致式(9)的梯度分母逐漸趨近0，進而造成整體模擬的不穩定性。使用 Constraint Force Mixing (CFM) 則可以避免這個情況，將式(6)改寫為:</p>
<script type="math/tex; mode=display">
C\big(\mathbf{p}+\Delta\mathbf{p}\big) \approx  C\big(\mathbf{p}\big)+\nabla C^T\nabla C\lambda+\varepsilon\lambda=0\tag{10}</script><p>其中 $\varepsilon$ 是使用者自訂的 Relaxation Parameter，$\lambda_i$變成:</p>
<script type="math/tex; mode=display">
\lambda_i=-\frac{C_i\big(\mathbf{p}_i,\cdots\mathbf{p}_j\big)}{\sum_k\big|\nabla_{\mathbf{p}_k}C_i\big|^2+\varepsilon}\tag{11}</script><div class="note info">
            <p><strong>註:</strong> 簡而言之就是在分母的部份加上一個不為 $0$ 的微小常數來確保分母不會為 $0$</p>
          </div>
<p>最後修正項 $\Delta \mathbf{p}_i$ 的函數變成:</p>
<script type="math/tex; mode=display">
\Delta \mathbf{p}_i=\frac{1}{\rho_0}\sum_j\big(\lambda_i+\lambda_j\big)m_j\nabla W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\tag{12}</script><div class="note info">
            <p><strong>註:</strong> </p><script type="math/tex; mode=display">\begin{align}\Delta \mathbf{p}_i     &= \lambda_i\nabla_{\mathbf{p}_i}C_i+\sum_j\lambda_j\nabla_{\mathbf{p}_j}C_i\\    &= \frac{1}{\rho_0}\sum_j \lambda_i m_j\nabla_{\mathbf{p}_i}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big) + \Big(-\frac{1}{\rho_0}\sum_j\lambda_j m_j\nabla_{\mathbf{p}_j}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\Big)\\    &= \frac{1}{\rho_0}\sum_j \lambda_i m_j\nabla_{\mathbf{p}_i}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big) + \frac{1}{\rho_0}\sum_j\lambda_j m_j\nabla_{\mathbf{p}_i}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big) \\    &= \frac{1}{\rho_0}\sum_j\big(\lambda_i+\lambda_j\big)m_j\nabla_{\mathbf{p}_i}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\end{align}</script><p>其中第2行到第3行是因為:</p><script type="math/tex; mode=display">\because\nabla_{\mathbf{p}_j}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)=-\nabla_{\mathbf{p}_i}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)</script>
          </div>
<h2 id="4-Tensile-Instability" class="heading-control"><a href="#4-Tensile-Instability" class="headerlink" title="4. Tensile Instability"></a>4. Tensile Instability<a class="heading-anchor" href="#4-Tensile-Instability" aria-hidden="true"></a></h2><p>SPH 中常見的問題就是因鄰近粒子數量不足無法達到 Rest Density，而產生負壓力所導致的粒子的聚集效應 (粒子間的排斥力變成吸力)。</p>
<p><img src="https://i.imgur.com/xWMEnJj.jpg" alt><br><em>上圖為聚集效應產生的不自然現象 / 下圖為施加人工壓力項的結果</em></p>
<p>一種解決方案是對壓力做 Clamping 讓壓力不為負值，但是這會讓粒子的內聚力衰弱。<br></p><div class="note info">
            <p><strong>註:</strong><br></p><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">clamp(x) {</span><br><span class="line">    return x>0 ? x:0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
          </div><p></p>
<p>其他解決方案:</p>
<ul>
<li>Clavet [2005] 使用 second near pressure term</li>
<li>Alduan and Otaduy [2011] 使用 discrete element (DEM) forces</li>
<li>Schechter and Bridson [2012] 使用 ghost particles</li>
</ul>
<p>此篇論文使用 Monaghan [2000] 的方法，加上一個人工壓力項:</p>
<script type="math/tex; mode=display">
s_{corr}=-k\bigg(\frac{W(\mathbf{p}_i-\mathbf{p}_j, h)}{W(\Delta \mathbf{q}, h)}\bigg)^n\tag{13}</script><p>其中 $\Delta \mathbf{q}$ 是固定的距離，$k$ 是微小的正常數。通常取 $|\Delta \mathbf{q}|=0.1h, \cdots,0.3h$，$k=0.1$，$n=4$。將此項納入修正項:</p>
<script type="math/tex; mode=display">
\Delta \mathbf{p}_i=\frac{1}{\rho_0}\sum_j\big(\lambda_i+\lambda_j+s_{corr}\big)m_j\nabla W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\tag{14}</script><p>這個醜陋的項會保持粒子略低於 Rest Density，最終產生出類似 Surface Tension 的效果。</p>
<h2 id="5-Vorticity-Confinement-and-Viscosity" class="heading-control"><a href="#5-Vorticity-Confinement-and-Viscosity" class="headerlink" title="5. Vorticity Confinement and Viscosity"></a>5. Vorticity Confinement and Viscosity<a class="heading-anchor" href="#5-Vorticity-Confinement-and-Viscosity" aria-hidden="true"></a></h2><p>PBD 方法通常會產生額外的 Damping 造成渦流快速消散。Fedkiw [2001] 引進 Vorticity Confinement 方法來克服模擬煙霧時的消散問題(Numerical Dissipation)，後來由 Lentine [2011] 當作 Energy Conserving 引入流體模擬。Hong [2008] 展示如何將 Vorticity Confinement 導入 Hybrid 的模擬方法。</p>
<p><img src="https://i.imgur.com/AeZSiRb.png" alt><br><em>左邊沒有加 Vorticity Confinement / 右邊有加 Vorticity Confinement</em></p>
<p>此篇文章使用 Vorticity Confinement 來補充流失的能量。此方法需要先計算粒子位置的 Vorticity，Monaghan [1992]:</p>
<script type="math/tex; mode=display">
\omega_i=\nabla\times\mathbf{v}=\sum_j m_j\mathbf{v}_{ij}\times\nabla\mathbf{p}_j W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\tag{15}</script><script type="math/tex; mode=display">
\mathbf{f}^{\text{vorticity}}_i=\varepsilon\big(\mathbf{N}\times\omega_i\big)\tag{16}</script><p>其中 $\mathbf{N}=\frac{\eta}{|\eta|}$，$\eta=\nabla|\omega|_i$， $\mathbf{v}_{ij}=\mathbf{v}_{j}-\mathbf{v}_{i}$ 。<br></p><div class="note info">
            <p><strong>註:</strong> 這邊有點小 Tricky，原論文裏面也沒有寫清楚，首先這邊目標是加速粒子的渦流速度，因此他先算出旋度向量 $\omega_i$，也就是遵守右手定則的旋轉軸心方向 (大姆指指的方向)。接著用 $\omega_i$ 的 Gradient L2-norm 算出往旋轉軸心方向的向量 $\eta$，算出單位向量 $\mathbf{N}$，最後再用外積算出旋轉方向並乘上 $\varepsilon$ 算出 Vorticity forces</p>
          </div><p></p>
<div class="note info">
            <p><strong>註:</strong></p><script type="math/tex; mode=display">\begin{align}\eta &= \nabla|\omega|=\nabla\bigg(\sum_{k=1}^n\omega_k^2\bigg)^{\frac{1}{2}}\\    &= \sum_{j=1}^n\frac{\partial}{\partial \omega_j}\bigg(\sum_{k=1}^n\omega_k^2\bigg)^{\frac{1}{2}}\hat{\omega_j}\\    &= \frac{1}{2}\sum_{j=1}^n\frac{2\omega_j}{\Big(\displaystyle\sum_{k=1}^n\omega_k^2\Big)^{\frac{1}{2}}}\hat{\omega_j}\\    &=\sum_{j=1}^n\frac{\omega_j}{|\omega|}\hat{\omega_j}\end{align}</script>
          </div>
<p>此外，此篇論文也使用 XSPH viscosity 來模擬流體的黏滯力:</p>
<script type="math/tex; mode=display">
\mathbf{v}_i^{new}=\mathbf{v}_i+c\sum_jm_j\mathbf{v}_{ij}\cdot W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\tag{17}</script><p>其中，參數 $c$ 通常設為 $0.1$。</p>
<h2 id="6-Algorithm" class="heading-control"><a href="#6-Algorithm" class="headerlink" title="6. Algorithm"></a>6. Algorithm<a class="heading-anchor" href="#6-Algorithm" aria-hidden="true"></a></h2><p><img src="https://i.imgur.com/PZz8Zvo.png" width="500px"></p>
<h2 id="7-Rendering" class="heading-control"><a href="#7-Rendering" class="headerlink" title="7. Rendering"></a>7. Rendering<a class="heading-anchor" href="#7-Rendering" aria-hidden="true"></a></h2><p>使用 GPU based ellipsoid splatting technique 詳情見 <a href="https://zhuanlan.zhihu.com/p/38280537">液体渲染：一种屏幕空间方法</a></p>
</body></html>]]></content>
      <categories>
        <category>Computer Graphics</category>
        <category>Physics Simulation</category>
        <category>Fluid Simulation</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Paper Note</tag>
        <tag>Computer Graphics</tag>
        <tag>Physics Simulation</tag>
        <tag>Fluid Simulation</tag>
      </tags>
  </entry>
  <entry>
    <title>[Note] Quantum Computation and Quantum Information - Chapter 3: Introduction to compuer science</title>
    <url>/Ending2015a/51732/</url>
    <content><![CDATA[<html><head></head><body><p>上課筆記，原文書:</p>
<blockquote>
<p>Quantum Computation and Quantum Information, Michael A. Nielsen & Isaac L. Chuang</p>
</blockquote>
<a id="more"></a>
<h2 id="3-1-Models-for-computation" class="heading-control"><a href="#3-1-Models-for-computation" class="headerlink" title="3.1 Models for computation"></a>3.1 Models for computation<a class="heading-anchor" href="#3-1-Models-for-computation" aria-hidden="true"></a></h2><blockquote>
<p>待補</p>
</blockquote>
<h3 id="3-1-1-Turing-machines" class="heading-control"><a href="#3-1-1-Turing-machines" class="headerlink" title="3.1.1 Turing machines"></a>3.1.1 Turing machines<a class="heading-anchor" href="#3-1-1-Turing-machines" aria-hidden="true"></a></h3><blockquote>
<p>待補</p>
</blockquote>
<h3 id="3-1-2-Circuits" class="heading-control"><a href="#3-1-2-Circuits" class="headerlink" title="3.1.2 Circuits"></a>3.1.2 Circuits<a class="heading-anchor" href="#3-1-2-Circuits" aria-hidden="true"></a></h3><blockquote>
<p>待補</p>
</blockquote>
<h2 id="3-2-The-analysis-of-computational-problems" class="heading-control"><a href="#3-2-The-analysis-of-computational-problems" class="headerlink" title="3.2 The analysis of computational problems"></a>3.2 The analysis of computational problems<a class="heading-anchor" href="#3-2-The-analysis-of-computational-problems" aria-hidden="true"></a></h2><h3 id="3-2-1-How-to-quantify-computational-resources" class="heading-control"><a href="#3-2-1-How-to-quantify-computational-resources" class="headerlink" title="3.2.1 How to quantify computational resources"></a>3.2.1 How to quantify computational resources<a class="heading-anchor" href="#3-2-1-How-to-quantify-computational-resources" aria-hidden="true"></a></h3><ul>
<li>$f(n)$ is $O(g(n))~\Rightarrow~\exists~c>0, n_0>0, ~\text{s.t.}~ 0 < f(n) \le cg(n), \forall n>n_0$</li>
<li>$f(n)$ is $\Omega(g(n))~\Rightarrow~\exists~c>0, n_0>0, ~\text{s.t.}~ 0 < cg(n)\le f(n), \forall n>n_0$</li>
<li>$f(n)$ is $\Theta(g(n))~\Rightarrow~\exists~c_1>0, c_2>0, n_0>0, ~\text{s.t.}~ 0 < c_1g(n)\le f(n)\le c_2g(n), \forall n>n_0$<ul>
<li>$f(n)$ is both $O(g(n))$ and $\Omega(g(n))$</li>
</ul>
</li>
</ul>
<h3 id="3-2-2-Computational-complexity" class="heading-control"><a href="#3-2-2-Computational-complexity" class="headerlink" title="3.2.2 Computational complexity"></a>3.2.2 Computational complexity<a class="heading-anchor" href="#3-2-2-Computational-complexity" aria-hidden="true"></a></h3><blockquote>
<p>待補</p>
</blockquote>
<h3 id="3-2-3-Decision-problems-and-the-complexity-classes-mathbf-P-and-mathbf-NP" class="heading-control"><a href="#3-2-3-Decision-problems-and-the-complexity-classes-mathbf-P-and-mathbf-NP" class="headerlink" title="3.2.3 Decision problems and the complexity classes $\mathbf{P}$ and $\mathbf{NP}$"></a>3.2.3 Decision problems and the complexity classes $\mathbf{P}$ and $\mathbf{NP}$<a class="heading-anchor" href="#3-2-3-Decision-problems-and-the-complexity-classes-mathbf-P-and-mathbf-NP" aria-hidden="true"></a></h3><blockquote>
<p>待補</p>
</blockquote>
<h3 id="3-2-4-A-plethora-of-complexity-classes" class="heading-control"><a href="#3-2-4-A-plethora-of-complexity-classes" class="headerlink" title="3.2.4 A plethora of complexity classes"></a>3.2.4 A plethora of complexity classes<a class="heading-anchor" href="#3-2-4-A-plethora-of-complexity-classes" aria-hidden="true"></a></h3><blockquote>
<p>待補</p>
</blockquote>
<h3 id="3-2-5-Energy-and-computation" class="heading-control"><a href="#3-2-5-Energy-and-computation" class="headerlink" title="3.2.5 Energy and computation"></a>3.2.5 Energy and computation<a class="heading-anchor" href="#3-2-5-Energy-and-computation" aria-hidden="true"></a></h3><blockquote>
<p>待補</p>
</blockquote>
<h2 id="3-3-Perspectives-on-computer-science" class="heading-control"><a href="#3-3-Perspectives-on-computer-science" class="headerlink" title="3.3 Perspectives on computer science"></a>3.3 Perspectives on computer science<a class="heading-anchor" href="#3-3-Perspectives-on-computer-science" aria-hidden="true"></a></h2><blockquote>
<p>待補</p>
</blockquote>
</body></html>]]></content>
      <categories>
        <category>Quantum Computing</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Course Note</tag>
        <tag>Quantum Computing</tag>
        <tag>Quantum Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>[Note] Quantum Computation and Quantum Information - Midterm exam</title>
    <url>/Ending2015a/33975/</url>
    <content><![CDATA[<html><head></head><body><p>期中考詳解，原文書:</p>
<blockquote>
<p>Quantum Computation and Quantum Information, Michael A. Nielsen & Isaac L. Chuang</p>
</blockquote>
<ul>
<li>班平均：57.7</li>
<li>標準差：13.9</li>
</ul>
<a id="more"></a>
<h2 id="Problam-1" class="heading-control"><a href="#Problam-1" class="headerlink" title="Problam 1"></a>Problam 1<a class="heading-anchor" href="#Problam-1" aria-hidden="true"></a></h2><div class="note info">
            <p>Defien the following states</p><script type="math/tex; mode=display">\begin{align*}\vert y_+\rangle = \left(\begin{matrix}1/\sqrt 2\\i/\sqrt2\end{matrix}\right) \quad \vert y_-\rangle = \left(\begin{matrix}1/\sqrt 2\\-i/\sqrt2\end{matrix}\right)\end{align*}</script><p>Calculate the following expressions and write out the answer in matrix notations.</p><p>(a) $\vert 0\rangle\langle y_-\vert$</p><p>(b) $\vert 1\rangle\langle+\vert y_+\rangle$</p><p>(c) $\vert y_+\rangle\otimes\vert +\rangle$</p><p>(d) $\langle 1\vert \otimes \langle -\vert$</p><p>(e) $(H\otimes X)(\vert 0\rangle\otimes \vert y_-\rangle)$</p><p>(f) $(\langle-\vert\otimes \langle1\vert)(H\otimes Z)(\vert 0\rangle\otimes\vert y_+\rangle)$</p>
          </div>
<div class="note success">
            <p>(a)</p><script type="math/tex; mode=display">\vert 0\rangle\langle y_-\vert = \left(\begin{matrix}1\\0\end{matrix}\right)\left(\begin{matrix}1/\sqrt2 & -i/\sqrt2\end{matrix}\right) = \left(\begin{matrix}1/\sqrt2 & -i/\sqrt2 \\ 0 & 0\end{matrix}\right)</script><p>(b)</p><script type="math/tex; mode=display">\vert 1\rangle\langle+\vert y_+\rangle = \left(\begin{matrix}0\\1\end{matrix}\right)\left(\begin{matrix}1/\sqrt2 & 1/\sqrt2\end{matrix}\right)\left(\begin{matrix}1/\sqrt2\\i/\sqrt2\end{matrix}\right)=\left(\begin{matrix}0 \\(1+i)/2 \end{matrix}\right)</script><p>(c)</p><script type="math/tex; mode=display">\vert y_+\rangle\otimes \vert +\rangle = \left(\begin{matrix}1/\sqrt2\\i/\sqrt2\end{matrix}\right) \otimes \left(\begin{matrix}1/\sqrt2\\1/\sqrt2\end{matrix}\right)=\left(\begin{matrix}1/2\\1/2\\i/2\\i/2\end{matrix}\right)</script><p>(d)</p><script type="math/tex; mode=display">\langle 1\vert \otimes \langle -\vert = \left(\begin{matrix} 0 & 1\end{matrix}\right)\otimes \left(\begin{matrix}1/\sqrt2 & -1/\sqrt2\end{matrix}\right) = \left(\begin{matrix}0&0&1/\sqrt2&-1/\sqrt2\end{matrix}\right)</script><p>(e)</p><script type="math/tex; mode=display">(H\otimes X)(\vert 0\rangle \otimes \vert y_-\rangle) = H\vert 0\rangle\otimes X\vert y_-\rangle = \left(\begin{matrix}1/\sqrt2\\1/\sqrt2\end{matrix}\right) \otimes \left(\begin{matrix}-i/\sqrt2\\1/\sqrt2\end{matrix}\right)=\left(\begin{matrix}-i/2\\1/2\\-i/2\\1/2\end{matrix}\right)</script><p>(f)</p><script type="math/tex; mode=display">(\langle-\vert\otimes \langle1\vert)(H\otimes Z)(\vert 0\rangle\otimes\vert y_+\rangle) = \langle -\vert H\vert 0\rangle\langle 1\vert Z\vert y_+\rangle=0</script>
          </div>
<h2 id="Problem-2" class="heading-control"><a href="#Problem-2" class="headerlink" title="Problem 2"></a>Problem 2<a class="heading-anchor" href="#Problem-2" aria-hidden="true"></a></h2><div class="note info">
            <p>Calculate the outcome probabilities and corresponding post measurement states of the following measurement settings.</p><p>(a) The state $\vert +\rangle$ in computational basis</p><p>(b) The state $\frac{1}{\sqrt2}(\vert 0\rangle\otimes\vert +\rangle+\vert 1\rangle\otimes \vert -\rangle)$ in computational basis</p><p>(c) The state is $\vert +\rangle$, measurement $\{M_1, M_2\}$ with $M_1=\displaystyle\frac{1}{2}\left(\begin{matrix}1 & -i\\i & 1\end{matrix}\right)$, $M_2=I-M_1$</p>
          </div>
<div class="note success">
            <p>(a)</p><script type="math/tex; mode=display">\begin{align*}\begin{cases}    Pr(0)= 1/2&\Rightarrow \vert \psi\rangle=\vert 0\rangle\\    Pr(1)= 1/2&\Rightarrow \vert \psi\rangle=\vert 1\rangle\\\end{cases}\end{align*}</script><p>(b)</p><script type="math/tex; mode=display">\frac{1}{\sqrt2}(\vert 0\rangle\otimes\vert +\rangle+\vert 1\rangle\otimes \vert -\rangle) = \left(\begin{matrix}1/2\\1/2\\1/2\\-1/2\end{matrix}\right)</script><script type="math/tex; mode=display">\begin{align*}\begin{cases}    Pr(00)= 1/4&\Rightarrow \vert \psi\rangle=\vert 00\rangle\\    Pr(01)= 1/4&\Rightarrow \vert \psi\rangle=\vert 01\rangle\\    Pr(10)= 1/4&\Rightarrow \vert \psi\rangle=\vert 10\rangle\\    Pr(11)= 1/4&\Rightarrow \vert \psi\rangle=\vert 11\rangle\end{cases}\end{align*}</script><p>(c)</p><script type="math/tex; mode=display">\begin{align*}\begin{cases}    Pr(1)= \langle+\vert M_1^\dagger M_1\vert +\rangle = 1/2 &\Rightarrow \vert \psi\rangle=\displaystyle\frac{1}{2}\left(\begin{matrix}1-i\\1+i\end{matrix}\right)\\    Pr(2)= \langle+\vert M_2^\dagger M_2\vert +\rangle = 1/2 &\Rightarrow \vert \psi\rangle=\displaystyle\frac{1}{2}\left(\begin{matrix}1+i\\1-i\end{matrix}\right)\\\end{cases}\end{align*}</script>
          </div>
<h2 id="Problem-3" class="heading-control"><a href="#Problem-3" class="headerlink" title="Problem 3"></a>Problem 3<a class="heading-anchor" href="#Problem-3" aria-hidden="true"></a></h2><div class="note info">
            <p>Let $\vert \psi\rangle = \frac{1}{\sqrt2}(\vert 0\rangle\otimes\vert 0\rangle+\vert 1\rangle\otimes\vert 1\rangle)$</p><p>(a) Calculate $\langle \psi\vert Z\otimes (X+Z)/\sqrt2\vert \psi\rangle$</p><p>(b) What are the eigenvalues of $(X+Z)/\sqrt2$ and $Z\otimes (X+Z)/\sqrt2$?</p><p>(c) Suppose Alice is holding the first part of $\vert \psi\rangle$ and Bob is holding the second part. If Alice measures $Z$ on her part and Bob measures $(X+Z)/\sqrt2$,what is the probabilities that their measurements have the same outcome?</p>
          </div>
<div class="note success">
            <p>(a)<br>Because $(X+Z)/\sqrt2 = H$</p><script type="math/tex; mode=display">\begin{align*}\langle \psi\vert Z\otimes H\vert \psi\rangle = \langle \psi\vert \left( \frac{1}{\sqrt2}(\vert 0\rangle \otimes \vert +\rangle -\vert 1\rangle \otimes \vert -\rangle)\right) = \frac{1}{2}(\frac{1}{\sqrt2}+\frac{1}{\sqrt2})=\frac{1}{\sqrt2}\end{align*}</script><p>(b)</p><ul><li>$(X+Z)/\sqrt2=H \Rightarrow \lambda=\pm 1$</li><li>$Z\otimes H \Rightarrow \lambda=\pm 1$</li></ul><p>(c)<br>Outcome table ($\lambda$)</p><div class="table-container"><table><thead><tr><th>$Z\otimes I$</th><th>$I\otimes (X+Z)/\sqrt2$</th><th>$Z\otimes (X+Z)/\sqrt2$</th></tr></thead><tbody><tr><td>$+1$</td><td>$+1$</td><td>$+1$</td></tr><tr><td>$+1$</td><td>$-1$</td><td>$-1$</td></tr><tr><td>$-1$</td><td>$+1$</td><td>$-1$</td></tr><tr><td>$-1$</td><td>$-1$</td><td>$+1$</td></tr></tbody></table></div><p>Let $Pr(+)$ and $Pr(-)$ denotes Alice and Bob get the same outcome and different outcomes.</p><script type="math/tex; mode=display">\begin{align*}\begin{cases}    Pr(+)+Pr(-)=1\\    \langle Z\otimes (X+Z)/\sqrt2\rangle = Pr(+)(+1)+Pr(-)(-1)= \displaystyle\frac{1}{\sqrt2}\end{cases}\end{align*}</script><p>$\Rightarrow Pr(+)=\displaystyle\frac{1}{2}(1+\frac{1}{\sqrt2})$</p>
          </div>
<h2 id="Problem-4" class="heading-control"><a href="#Problem-4" class="headerlink" title="Problem 4"></a>Problem 4<a class="heading-anchor" href="#Problem-4" aria-hidden="true"></a></h2><div class="note info">
            <p>Recall that the Bell basis consist of the following states on two qubits:</p><script type="math/tex; mode=display">\begin{eqnarray}\vert \beta_{00}\rangle=\frac{1}{\sqrt2}(\vert 00\rangle+\vert 11\rangle) \quad& \vert \beta_{01}\rangle=\frac{1}{\sqrt2}(\vert 00\rangle-\vert 11\rangle)\\\vert \beta_{10}\rangle=\frac{1}{\sqrt2}(\vert 10\rangle+\vert 01\rangle) \quad& \vert \beta_{11}\rangle=\frac{1}{\sqrt2}(\vert 10\rangle-\vert 01\rangle)\end{eqnarray}</script><p>Consider the quantum state $\vert +\rangle \otimes \vert \beta_{00}\rangle$ on three qubits. Suppose Alice is holding the first two qubits of $\vert +\rangle\otimes \vert \beta_{00}\rangle$ and Bob is holding the third. If Alice measures her two qubits in the Bell basis, what are the probabilities of each measurement outcome and Bob’s post-measurement state corresponding to each measurement outcome?</p>
          </div>
<div class="note success">
            <script type="math/tex; mode=display">\vert +\rangle\otimes \vert \beta_{00}\rangle = \frac{1}{2}(\vert 000\rangle+\vert011\rangle + \vert 100\rangle + \vert 111\rangle)</script><script type="math/tex; mode=display">\begin{cases}Pr(00)=1/4 &\Rightarrow \vert\psi\rangle = \vert +\rangle\\Pr(01)=1/4 &\Rightarrow \vert \psi\rangle = \vert -\rangle\\Pr(10)=1/4 &\Rightarrow \vert\psi\rangle = \vert +\rangle\\Pr(11)=1/4 &\Rightarrow \vert \psi\rangle=\vert -\rangle\\\end{cases}</script>
          </div>
<h2 id="Problem-5" class="heading-control"><a href="#Problem-5" class="headerlink" title="Problem 5"></a>Problem 5<a class="heading-anchor" href="#Problem-5" aria-hidden="true"></a></h2><div class="note info">
            <p>Show that for any $E$ that is an Hermitian operator on single qubit:</p><script type="math/tex; mode=display">\langle \beta_{00}\vert E\otimes I\vert \beta_{00}\rangle=\langle \beta_{01}\vert E\otimes I\vert \beta_{01}\rangle=\langle \beta_{10}\vert E\otimes I\vert \beta_{10}\rangle=\langle \beta_{11}\vert E\otimes I\vert \beta_{11}\rangle</script>
          </div>
<div class="note success">
            <script type="math/tex; mode=display">\begin{align*}\langle \beta_{00}\vert E\otimes I\vert \beta_{00}\rangle &= \frac{1}{2}(\langle 0\vert E\vert 0\rangle+\langle 1\vert E\vert 1\rangle)\\\langle \beta_{01}\vert E\otimes I\vert \beta_{01}\rangle &= \frac{1}{2}(\langle 0\vert E\vert 0\rangle+\langle 1\vert E\vert 1\rangle)\\\langle \beta_{10}\vert E\otimes I\vert \beta_{10}\rangle &= \frac{1}{2}(\langle 0\vert E\vert 0\rangle+\langle 1\vert E\vert 1\rangle)\\\langle \beta_{11}\vert E\otimes I\vert \beta_{11}\rangle &= \frac{1}{2}(\langle 0\vert E\vert 0\rangle+\langle 1\vert E\vert 1\rangle)\\\end{align*}</script><p>Hence, for any Hermitian operator $E$, the statement holds.</p>
          </div>
<h2 id="Problem-6" class="heading-control"><a href="#Problem-6" class="headerlink" title="Problem 6"></a>Problem 6<a class="heading-anchor" href="#Problem-6" aria-hidden="true"></a></h2><div class="note info">
            <p>Consider the following ensemble:</p><script type="math/tex; mode=display">p_1=2/3, \vert \psi_1\rangle=\vert +\rangle, p_2=1/3, \vert \psi_2\rangle=\vert 0\rangle</script><p>Calculate the following quantities.</p><p>(a) The corresponding density matrix $\rho$.</p><p>(b) The expectation value of $Z$ against $\rho$.</p><p>(c) The outcome probabilities and post measurement states of computational basis measurement.</p><p>(d) The evolved state $\rho’$ obtained after applying $H$ to $\rho$.</p>
          </div>
<div class="note success">
            <p>(a)</p><script type="math/tex; mode=display">\rho=p_1\vert \psi_1\rangle\langle\psi_1\vert + p_2\vert \psi_2\rangle\langle \psi_2\vert = \left(\begin{matrix}2/3 & 1/3\\1/3&1/3\end{matrix}\right)</script><p>(b)</p><script type="math/tex; mode=display">\langle Z\rangle = \sum_ip_i\langle \psi_i\vert Z\vert\psi_i\rangle=\text{tr}(\sum_i p_i\vert\psi_i\rangle\langle \psi_i\vert Z)=\text{tr}(\rho Z)=1/3</script><p>(c)</p><script type="math/tex; mode=display">\begin{cases}    Pr(0) = \text{tr}(M_0^\dagger M_0\rho)= 2/3 &\Rightarrow \rho_0= M_0\rho M_0^\dagger/Pr(0) = \vert 0\rangle\langle 0\vert \\    Pr(1) = \text{tr}(M_1^\dagger M_1\rho)=1/3 &\Rightarrow \rho_1= M_1\rho M_1^\dagger/Pr(1) = \vert 1\rangle\langle 1\vert\\\end{cases}</script><p>(d)</p><script type="math/tex; mode=display">\rho'=H\rho H^\dagger = \frac{1}{6}\left(\begin{matrix}5&1\\1&1\end{matrix}\right)</script>
          </div>
<h2 id="Problem-7" class="heading-control"><a href="#Problem-7" class="headerlink" title="Problem 7"></a>Problem 7<a class="heading-anchor" href="#Problem-7" aria-hidden="true"></a></h2><div class="note info">
            <p>Calculate a purification of</p><script type="math/tex; mode=display">\rho=\left(\begin{matrix}1/2&1/4\\1/4&1/2\end{matrix}\right)</script>
          </div>
<div class="note success">
            <p>The eigenvalue and eignevector of $\rho$</p><script type="math/tex; mode=display">\det(\rho-\lambda I)=0 ~\Rightarrow~ \lambda^2-\lambda+\frac{3}{16} = 0</script><script type="math/tex; mode=display">\Rightarrow\begin{cases}    \lambda_1 = 3/4 &\Rightarrow\vert \lambda_1\rangle = \vert +\rangle\\    \lambda_2 = 1/4 &\Rightarrow\vert \lambda_2\rangle = \vert -\rangle\end{cases}</script><p>The ensemble of $\rho$ is:</p><script type="math/tex; mode=display">\rho=\frac{3}{4}\vert +\rangle\langle+\vert+\frac{1}{4}\vert -\rangle\langle-\vert</script><p>Then, the purification is </p><script type="math/tex; mode=display">\vert \psi\rangle = \frac{\sqrt3}{2}\vert+\rangle\vert0\rangle+\frac{1}{2}\vert-\rangle\vert1\rangle</script><p>Verify:</p><script type="math/tex; mode=display">\begin{align*}\text{tr}_R(\vert \psi\rangle\langle\psi\vert) &= \frac{3}{4}\vert +\rangle\langle+\vert\langle0\vert0\rangle + \frac{1}{4}\vert -\rangle\langle-\vert\langle1\vert1\rangle\\&= \frac{3}{4}\vert +\rangle\langle+\vert+\frac{1}{4}\vert -\rangle\langle-\vert\\&= \rho_A\end{align*}</script><p>Hence, $\vert \psi\rangle$ is a purification of $\rho$.</p>
          </div>
<h2 id="Problem-8" class="heading-control"><a href="#Problem-8" class="headerlink" title="Problem 8"></a>Problem 8<a class="heading-anchor" href="#Problem-8" aria-hidden="true"></a></h2><div class="note info">
            <p>Let $\vert \psi\rangle=\frac{1}{\sqrt2}(\vert0\rangle\vert+\rangle+\vert1\rangle\vert-\rangle)$ be a quantum state on two systems $A$ and $B$. Calculate the reduced density matrix of $\vert \psi\rangle$ on system $A$.</p>
          </div>
<div class="note success">
            <script type="math/tex; mode=display">\begin{align*}\text{tr}_B(\vert \psi\rangle\langle\psi\vert) &= \frac{1}{2}\text{tr}_B\left((\vert0\rangle\vert+\rangle+\vert1\rangle\vert-\rangle)(\langle0\vert\langle+\vert+\langle1\vert\langle-\vert)\right)\\&= \frac{1}{2}(\vert0\rangle\langle0\vert+\vert1\rangle\langle1\vert)\end{align*}</script>
          </div>
<h2 id="Problem-9" class="heading-control"><a href="#Problem-9" class="headerlink" title="Problem 9"></a>Problem 9<a class="heading-anchor" href="#Problem-9" aria-hidden="true"></a></h2><div class="note info">
            <p>Find $k$ such that $n^3+2n^2+\log(n)$ is $\Theta(n^k)$.</p>
          </div>
<div class="note success">
            <p>Suppose $\exists~c_1, c_2$ such that $c_1 n^k\le n^3+2n^2+\log(n)\le c_2n^k,~\forall~n\ge n_0$.<br>Assume $k=3$, let</p><script type="math/tex; mode=display">\begin{cases}    f(n)=n^3+2n^2+\log(n)-c_1 n^3\ge 0\\    f'(n)=3n^2+4n+\frac{1}{n}-3c_1n^2 \ge 0\\\end{cases}</script><p>The two inequality hold for $c_1=1$, $n_0=1$. Let</p><script type="math/tex; mode=display">\begin{cases}    g(n)=n^3+2n^2+\log(n)-c_2 n^3\le 0\\    g'(n)=3n^2+4n+\frac{1}{n}-3c_2n^2 \le 0\\\end{cases}</script><p>The two inequality hold for $c_2=3$, $n_0=1$. </p><p>Hence, $n^3+2n^2+\log(n) = \Theta(n^k)$ if $k=3$.</p>
          </div>
<h2 id="Problem-10" class="heading-control"><a href="#Problem-10" class="headerlink" title="Problem 10"></a>Problem 10<a class="heading-anchor" href="#Problem-10" aria-hidden="true"></a></h2><div class="note info">
            <p>In Simon’s problem, we are given an oracle function $f:\{0,1\}^n\to\{0,1\}^n$ promised that $f(x)=f(y)$ if and only if $x=y\oplus s$ for some $s\ne0^n$. The problem is finding $s$. The Simon’s algorithm generate strings $z_1,z_2,\dots,z_k$ by running the following sub-algorithm.</p><ol><li>Create uniform superposition in the first register, prepare $\vert0\rangle^n$ in the second register.</li><li>apply the oracle</li><li>measure the second register</li><li>apply Hadamard gates on the first register</li><li>measure the first register and output as $z_i$</li></ol><p>Suppose we are running Simon’s algorithm with $n=3$ and $s=110$. Write down all possible output strings $z_i$.</p>
          </div>
<div class="note success">
            <ol><li>Apply $H^{\otimes n}$ on the first register:<script type="math/tex; mode=display">H^{\otimes n} \vert0\rangle^{\otimes n} \otimes \vert 0\rangle^{\otimes n} = \frac{1}{2^{n/2}}\sum_{x\in\{0,1\}^n}\vert x\rangle \otimes \vert 0\rangle^{\otimes n}</script></li><li>Apply oracle:<script type="math/tex; mode=display">O_f \frac{1}{2^{n/2}}\sum_{x\in\{0,1\}^n}\vert x\rangle \otimes \vert 0\rangle^{\otimes n} = \frac{1}{2^{n/2}}\sum_{x\in\{0,1\}^n}\vert x\rangle \otimes \vert f(x)\rangle</script></li><li>Measure the second register:<script type="math/tex; mode=display">\frac{1}{\sqrt2}\left(\vert x\rangle + \vert x\oplus s\rangle\right)</script></li><li>Apply $H^{\otimes n}$ on the first register:<script type="math/tex; mode=display">\frac{1}{\sqrt2}\frac{1}{2^{n/2}}\sum_{z\in\{0, 1\}^n}\left((-1)^{x\cdot z} + (-1)^{(x\oplus s) \cdot z}\right)\vert z\rangle</script></li><li>Measure the first register. Only the states with $(-1)^{x\cdot z} + (-1)^{(x\oplus s) \cdot z} \ne 0$ can be measured. Suppose $n=3$, $s=110$:</li></ol><div class="table-container"><table><thead><tr><th>$x$</th><th>$x\oplus s$</th><th>Possible $z$</th></tr></thead><tbody><tr><td>$000$</td><td>$110$</td><td>$000, 001, 110, 111$</td></tr><tr><td>$001$</td><td>$111$</td><td>$000, 001, 110, 111$</td></tr><tr><td>$010$</td><td>$100$</td><td>$000, 001, 110, 111$</td></tr><tr><td>$011$</td><td>$101$</td><td>$000, 001, 110, 111$</td></tr></tbody></table></div><p>Since $(-1)^{(x\oplus s)\cdot z} = (-1)^{x\cdot z+s\cdot z}$, $z$ must satisfy $s\cdot z ~(\text{mod}~2)=0$. </p><p>The possible $z=000, 001, 110, 111$.</p>
          </div>
<h2 id="Problem-11" class="heading-control"><a href="#Problem-11" class="headerlink" title="Problem 11"></a>Problem 11<a class="heading-anchor" href="#Problem-11" aria-hidden="true"></a></h2><div class="note info">
            <p>Calculate the eigenvalues of $aX+bY+cZ$, where $X,Y,Z$ are the Pauli matrices and $a,b,c$ are real nubmers.</p>
          </div>
<div class="note success">
            <script type="math/tex; mode=display">\begin{align*}&S = aX+bY+cZ = \left(\begin{matrix} c & a-bi\\a+bi&-c\end{matrix}\right)\\\Rightarrow ~&\det(S-\lambda I)=0\\ \Rightarrow ~&(c-\lambda)(-c-\lambda)-(a-bi)(a+bi)=0\\\Rightarrow ~&\lambda^2-c^2-a^2-b^2=0\\\Rightarrow ~&\lambda=\pm\sqrt{a^2+b^2+c^2}\end{align*}</script>
          </div>
<h2 id="Problem-12" class="heading-control"><a href="#Problem-12" class="headerlink" title="Problem 12"></a>Problem 12<a class="heading-anchor" href="#Problem-12" aria-hidden="true"></a></h2><div class="note info">
            <p>Calculate the output state of the following circuit</p><p><img src="https://i.imgur.com/gA4WWCP.png" width="30%"></p>
          </div>
<div class="note success">
            <ol><li>Apply $H$, $\vert 000\rangle \to \frac{1}{\sqrt2}(\vert 000\rangle+\vert 100\rangle)$.</li><li>Apply controlled-not, $\frac{1}{\sqrt2}(\vert 000\rangle+\vert 100\rangle) \to \frac{1}{\sqrt2}(\vert 000\rangle+\vert 110\rangle)$.</li><li>Apply controlled-not, $\frac{1}{\sqrt2}(\vert 000\rangle+\vert 100\rangle) \to \frac{1}{\sqrt2}(\vert 000\rangle+\vert 111\rangle)$.</li></ol>
          </div>
</body></html>]]></content>
      <categories>
        <category>Quantum Computing</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Course Note</tag>
        <tag>Quantum Computing</tag>
        <tag>Quantum Algorithm</tag>
        <tag>Quantum Circuits</tag>
      </tags>
  </entry>
  <entry>
    <title>[Note] Quantum Computation and Quantum Information - Final exam</title>
    <url>/Ending2015a/28314/</url>
    <content><![CDATA[<html><head></head><body><p>期末考詳解，原文書:</p>
<blockquote>
<p>Quantum Computation and Quantum Information, Michael A. Nielsen & Isaac L. Chuang</p>
</blockquote>
<ul>
<li>班平均：64</li>
</ul>
<a id="more"></a>
<h2 id="Problem-1" class="heading-control"><a href="#Problem-1" class="headerlink" title="Problem 1"></a>Problem 1<a class="heading-anchor" href="#Problem-1" aria-hidden="true"></a></h2><div class="note info">
            <p>Recall that the Bell basis consist of the following states on two qubits:</p><script type="math/tex; mode=display">\begin{align*}\vert \beta_{00}\rangle &= \frac{1}{\sqrt 2}(\vert 00\rangle+\vert11\rangle) &     \vert \beta_{01}\rangle &= \frac{1}{\sqrt 2}(\vert 00\rangle-\vert11\rangle)\\\vert \beta_{10}\rangle &= \frac{1}{\sqrt 2}(\vert 10\rangle+\vert01\rangle) &     \vert \beta_{11}\rangle &= \frac{1}{\sqrt 2}(\vert 10\rangle-\vert01\rangle)\end{align*}</script><p>Suppose Alice is holding the first two qubits of $\vert +\rangle \otimes \vert \beta_{11}\rangle$ and Bob is holding the third.</p><p>(a) If Alice applies $(I\otimes H)\text{CNOT}$ to her two qubits, where the first qubit is the control of the $\text{CNOT}$ and the second qubit is the target, what is the state shared by Alice and Bob now?</p><p>(b) After step (a), what is the reduced density matrix of Bob’s state?</p><p>(c) After step (a), Alice measures her two qubits in the Bell basis, what are the probabilities of each measurement outcome and Bob’s post-measurement state corresponding to each measurement outcome?</p>
          </div>
<div class="note success">
            <script type="math/tex; mode=display">\begin{align*}\vert +\rangle\otimes\vert\beta_{11}\rangle    &= \frac{1}{\sqrt 2}(\vert 0\rangle + \vert 1\rangle) \otimes \vert \beta_{11}\rangle\\    &= \frac{1}{\sqrt 2}\big(\vert0\rangle\otimes\vert\beta_{11}\rangle+\vert1\rangle\otimes\vert\beta_{11}\rangle\big)\end{align*}</script><p>(a)</p><p>First apply $\text{CNOT}_\text{Alice}$</p><script type="math/tex; mode=display">\begin{align*}&\frac{1}{\sqrt 2}\big(\vert0\rangle\otimes\vert\beta_{11}\rangle+\vert1\rangle\otimes\vert\beta_{11}\rangle\big)\\&\qquad\xrightarrow{\text{CNOT}_\text{Alice}} \frac{1}{\sqrt 2}\big(\vert0\rangle\otimes\vert\beta_{11}\rangle+\vert1\rangle\otimes\vert\beta_{01}\rangle\big)\end{align*}</script><p>Then, apply $(I\otimes H)_\text{Alice}$</p><script type="math/tex; mode=display">\begin{align*}&&\frac{1}{\sqrt 2}\big(\vert0\rangle\otimes\vert\beta_{11}\rangle+&\vert1\rangle\otimes\vert\beta_{01}\rangle\big)\\&&\qquad\xrightarrow{(I\otimes H)_\text{Alice}}&\frac{1}{2}(\vert0\rangle\otimes(\vert\beta_{01}\rangle-\vert\beta_{10}\rangle)+\vert1\rangle\otimes(\vert\beta_{00}\rangle+\vert\beta_{11}\rangle))\\&&=&\frac{1}{2}(\vert00\rangle\otimes\vert-\rangle-\vert01\rangle\otimes\vert+\rangle+\vert10\rangle\otimes\vert-\rangle+\vert11\rangle\otimes\vert+\rangle)\\&&=&\frac{1}{2}\big((\vert00\rangle+\vert10\rangle)\vert-\rangle+(\vert11\rangle-\vert01\rangle)\vert+\rangle\big)\end{align*}</script><p>(b)</p><p>Let $\vert\psi\rangle=\displaystyle\frac{1}{2}\big((\vert00\rangle+\vert10\rangle)\vert-\rangle+(\vert11\rangle-\vert01\rangle)\vert+\rangle\big)$. The reduced density matrix of Bob’s state is</p><script type="math/tex; mode=display">\begin{align*}\rho_{Bob}    &=\text{tr}_\text{Alice}(\vert\psi\rangle\langle\psi\vert)\\    &=\vert-\rangle\langle-\vert+\vert+\rangle\langle+\vert\\    &=\vert0\rangle\langle0\vert+\vert1\rangle\langle1\vert\end{align*}</script><p>(c)</p><script type="math/tex; mode=display">\begin{align*}\begin{cases}    Pr(00)=\frac{1}{4} &\Rightarrow \vert \psi_\text{Bob}\rangle=\vert0\rangle\\    Pr(01)=\frac{1}{4} &\Rightarrow \vert \psi_\text{Bob}\rangle=-\vert1\rangle\\    Pr(10)=\frac{1}{4} &\Rightarrow \vert \psi_\text{Bob}\rangle=-\vert1\rangle\\    Pr(11)=\frac{1}{4} &\Rightarrow \vert \psi_\text{Bob}\rangle=\vert0\rangle\\\end{cases}\end{align*}</script>
          </div>
<h2 id="Problem-2" class="heading-control"><a href="#Problem-2" class="headerlink" title="Problem 2"></a>Problem 2<a class="heading-anchor" href="#Problem-2" aria-hidden="true"></a></h2><div class="note info">
            <p>Calculate the output state and measurement distribution of the following circuits.</p><p>(a)</p><p><img src="https://i.imgur.com/850JyxW.png" width="200px"></p><p>(b)</p><p><img src="https://i.imgur.com/fKKwBvr.png" width="300px"></p>
          </div>
<div class="note success">
            <p>(a)</p><p><img src="https://i.imgur.com/cv5dvw9.png" width="200px"></p><script type="math/tex; mode=display">\begin{align*}\vert \psi_1\rangle     &= \vert +\rangle\otimes\vert0\rangle\\\vert \psi_2\rangle     &= \frac{1}{\sqrt{2}}(\vert00\rangle+\vert11\rangle)\\\vert \psi_3\rangle    &= \frac{1}{2}(\vert00\rangle+\vert10\rangle+\vert01\rangle-\vert11\rangle)\end{align*}</script><p>Output distribution:</p><script type="math/tex; mode=display">\begin{cases}    Pr(00) = \frac{1}{4}\\    Pr(01) = \frac{1}{4}\\    Pr(10) = \frac{1}{4}\\    Pr(11) = \frac{1}{4}\end{cases}</script><p>(b)</p><p><img src="https://i.imgur.com/wLMNcCP.png" width="300px"></p><script type="math/tex; mode=display">\begin{align*}\vert \psi_1\rangle     &= \vert +\rangle \otimes \vert 0\rangle\\\vert \psi_2\rangle     &= \frac{1}{\sqrt 2}(\vert 00\rangle + \vert 11\rangle)\\\vert \psi_3\rangle    &= \frac{1}{\sqrt 2}(\vert 00\rangle-\vert11\rangle)\\\vert \psi_4\rangle    &= \frac{1}{\sqrt 2}(\vert00\rangle-\vert10\rangle)\\    &= \vert -\rangle\otimes \vert 0\rangle\\\vert \psi_5\rangle    &= \vert 10\rangle\end{align*}</script><p>Output distribution:</p><script type="math/tex; mode=display">Pr(10)=1</script>
          </div>
<h2 id="Problem-3" class="heading-control"><a href="#Problem-3" class="headerlink" title="Problem 3"></a>Problem 3<a class="heading-anchor" href="#Problem-3" aria-hidden="true"></a></h2><div class="note info">
            <p>Suppose we have a one-bit function $f:\{0,1\}\to\{0,1\}$ and associated phase oracle</p><script type="math/tex; mode=display">O_f\vert b\rangle = (-1)^{f(b)}\vert b\rangle~~\text{for}~~ b\in\{0,1\}</script><p>(a) Suppose we run the one-qubit circuit $HO_fH$ on input $\vert 0\rangle$ then measures in the computational basis, what is the output probability distribution in terms of $f(0)$ and $f(1)$?</p><p>(b) Now suppose we implemented quantum gate of $f$ while leaving some “garbage” in the work space, so we have the oracle</p><script type="math/tex; mode=display">O'_f\vert b,c\rangle =(-1)^{f(b)}\vert b,c\oplus b\rangle</script><p>Suppose we run the two qubit quantum circuit $(H\otimes I)O’_f(H\otimes I)$ on the input $\vert 00\rangle$ and measurement the first qubit, what is the output probability distribution?</p>
          </div>
<div class="note success">
            <p>(a)</p><p>Apply $H$</p><script type="math/tex; mode=display">\vert 0\rangle \xrightarrow{H}\vert+\rangle</script><p>Apply oracle $O_f$</p><script type="math/tex; mode=display">\vert +\rangle \xrightarrow{O_f} \frac{1}{\sqrt 2}\left((-1)^{f(0)}\vert 0\rangle + (-1)^{f(1)}\vert 1\rangle\right)</script><p>Apply $H$</p><script type="math/tex; mode=display">\begin{align*}&\frac{1}{\sqrt 2}\left((-1)^{f(0)}\vert 0\rangle + (-1)^{f(1)}\vert 1\rangle\right) \\    &\qquad\xrightarrow{H} \frac{1}{2}\left(\left((-1)^{f(0)}+(-1)^{f(1)}\right)\vert 0\rangle + \left((-1)^{f(0)}-(-1)^{f(1)}\right)\vert 1\rangle\right)\end{align*}</script><p>Output distribution</p><script type="math/tex; mode=display">\begin{cases}    Pr(0) = \frac{1}{4}\left((-1)^{2f(0)}+(-1)^{2f(1)}+2(-1)^{f(0)+f(1)}\right)\\    Pr(1) = \frac{1}{4}\left((-1)^{2f(0)}+(-1)^{2f(1)}-2(-1)^{f(0)+f(1)}\right)\end{cases}</script><p>(b)</p><p>Apply $(H\otimes I)$</p><script type="math/tex; mode=display">\vert 00\rangle \xrightarrow{(H\otimes I)}\vert+\rangle\otimes \vert 0\rangle</script><p>Apply oracle $O’_f$</p><script type="math/tex; mode=display">\vert+\rangle\otimes \vert 0\rangle \xrightarrow{O'_f} \frac{1}{\sqrt 2}\left((-1)^{f(0)}\vert00\rangle+(-1)^{f(1)}\vert11\rangle\right)</script><p>Apply $(H\otimes I)$</p><script type="math/tex; mode=display">\begin{align*}&\frac{1}{\sqrt 2}\left((-1)^{f(0)}\vert00\rangle+(-1)^{f(1)}\vert11\rangle\right)\\&\qquad\xrightarrow{(H\otimes I)}\frac{1}{2}\left((-1)^{f(0)}(\vert00\rangle+\vert10\rangle)+(-1)^{f(1)}(\vert01\rangle-\vert11\rangle)\right)\end{align*}</script><p>Output distribution</p><script type="math/tex; mode=display">\begin{cases}    Pr(0)=\frac{1}{4}\left((-1)^{2f(0)}+(-1)^{2f(1)}\right)\\    Pr(1)=\frac{1}{4}\left((-1)^{2f(0)}+(-1)^{2f(1)}\right)\end{cases}</script>
          </div>
<h2 id="Problem-4" class="heading-control"><a href="#Problem-4" class="headerlink" title="Problem 4"></a>Problem 4<a class="heading-anchor" href="#Problem-4" aria-hidden="true"></a></h2><div class="note info">
            <p>In this problem, we walk through Shor’s algorithm with $N=3\times 5$</p><p>(a) What is the number of positive integers less than $N$ that is coprime to N?</p><p>(b) What is the order $r$ of $x=2$ modulo $N$ (the smallest positive integer $r$ such that $x^r=1(\text{mod}~N)$)? Check that $r$ is even.</p><p>(c) Calculate $x^{r/2}+1$ and $x^{r/2}-1$.</p><p>(d) Calculate $\text{gcd}(x^{r/2}-1, N)$ and confirm it is a factor of $N$.</p><p>(e) Recall the period finding algorithm has the following circuit</p><p><img src="https://i.imgur.com/V0pW97Y.png" width="300px"></p><p>where the controlled gate does  controlled multiplication: $\vert j,y\rangle\to\vert j,y\cdot x^j(\text{mod}~N)\rangle$, and $QFT_t$ is the quantum fourier transform gate on $t$ qubits, with $QFT_t\vert x\rangle=\frac{1}{\sqrt{2^t}}\exp[2\pi ixy/2^t]\vert y\rangle$ for $x\in\{0,1,\dots,2^t-1\}$. Use $t=5$, write down all intermediate states and the output probability distribution of the period finding algorithm. (Hint: recall $\vert 1\rangle=\frac{1}{\sqrt r}\sum^{r-1}_{s=0}\vert u_s\rangle$, where $\vert u_s\rangle=\frac{1}{\sqrt r}\sum^{r-1}_{k=0}\exp[-2\pi isk/r]\vert x^k\rangle$)</p>
          </div>
<div class="note success">
            <p>(a) By Euler’s totient function</p><script type="math/tex; mode=display">\phi(N)=(p-1)(q-1)=2\times4=8</script><p>(b)</p><script type="math/tex; mode=display">2^r=1(\text{mod}~15)~\Rightarrow~ r=4</script><p>$r$ is even.</p><p>(c)</p><script type="math/tex; mode=display">x^{r/2}+1=2^{2}+1=5,\quad x^{r/2}-1=4-1=3</script><p>(d)</p><script type="math/tex; mode=display">\text{gcd}(x^{r/2}-1,N)=\text{gcd}(3,15)=3</script><p>Yes, $3$ is a factor of $N=15$.</p><p>(e)</p><p><img src="https://i.imgur.com/73X0yxx.png" width="300px"></p><script type="math/tex; mode=display">\begin{align*}\vert \psi_1\rangle    &=(H^{\otimes5}\vert0\rangle)\otimes\vert1\rangle\\    &=\frac{1}{\sqrt{32}}\sum^{31}_{j=0}\vert j\rangle\otimes\vert 1\rangle\\\vert \psi_2\rangle    &=\frac{1}{\sqrt{32}}\sum^{31}_{j=0}\vert j\rangle\otimes \vert x^j\rangle\end{align*}</script><p>Recall (from Hint)</p><script type="math/tex; mode=display">\vert 1\rangle=\frac{1}{\sqrt r}\sum^{r-1}_{s=0}\vert u_s\rangle,\quad \vert u_s\rangle=\frac{1}{\sqrt r}\sum^{r-1}_{k=0}e^{-2\pi i\frac{sk}{r}}\vert x^k\rangle</script><p>$r=4~\Rightarrow$</p><script type="math/tex; mode=display">\begin{align*}\Rightarrow&&\vert 1\rangle&=\frac{1}{2}\sum^{3}_{s=0}\vert u_s\rangle, \quad \vert u_s\rangle=\frac{1}{2}\sum^{3}_{k=0}e^{-2\pi i\frac{sk}{4}}\vert x^k\rangle\\&&\vert u_s\cdot x^j\rangle     &= \frac{1}{2}\sum^{3}_{k=0}e^{-2\pi i\frac{sk}{4}}\vert x^{j+k}\rangle\\    &&&= \frac{1}{2}\sum^{3}_{k=0}e^{-2\pi i\frac{s(k'-j)}{4}}\vert x^{k'}\rangle\\    &&&= e^{2\pi i\frac{sj}{4}}\vert u_s\rangle\\\Rightarrow &&\vert \psi_1\rangle    &= \frac{1}{\sqrt{32}}\sum^{31}_{j=0}\vert j\rangle \otimes \frac{1}{2}\sum^3_{s=0}\vert u_s\rangle\\\Rightarrow &&\vert \psi_2\rangle    &=\frac{1}{2\sqrt{32}}\sum^{31}_{j=0}\sum^3_{s=0}\vert j\rangle\otimes e^{2\pi i\frac{sj}{4}}\vert u_s\rangle\\&&QFT_5\vert k\rangle &=\frac{1}{\sqrt{32}}\sum^{31}_{j=0}e^{2\pi i\frac{kj}{32}}\vert j\rangle,\quad e^{2\pi i\frac{sj}{4}}=e^{2\pi i\frac{(8s)j}{32}}\\&&\Rightarrow~~&\frac{1}{\sqrt{32}}\sum^{31}_{j=0}e^{2\pi i\frac{sj}{4}}\vert j\rangle    =\frac{1}{\sqrt{32}}\sum^{31}_{j=0}e^{2\pi i\frac{(8s)j}{32}}\vert j\rangle=QFT_5\vert 8s\rangle\\\Rightarrow&&\vert \psi_3\rangle&=QFT^{-1}_5\vert \psi_2\rangle=\frac{1}{2}\sum^4_{s=0}\vert 8s\rangle\otimes \vert u_s\rangle\end{align*}</script><p>Output distribution:</p><script type="math/tex; mode=display">\{0, 8,16,24\}\sim\text{Uniform}</script>
          </div>
<h2 id="Problem-5" class="heading-control"><a href="#Problem-5" class="headerlink" title="Problem 5"></a>Problem 5<a class="heading-anchor" href="#Problem-5" aria-hidden="true"></a></h2><div class="note info">
            <p>Suppose we have an input of $N$ numbers $x_1,x_2,\dots,x_N$. Give a rough sketch of a quantum algorithm that determines whether all inputs are different in $O(N)$ time. (You can use Grover’s algorithm that searches from any number of marked items as a subroutine.)</p>
          </div>
<div class="note success">
            <p>Reduce to Element distinctness problem</p><ol><li>Divide the inputs into $\sqrt N$ blocks of size $\sqrt N$</li><li>Outer Grover search through each block $s$ and query all elements in it (check collision inside)</li><li>Inner Grover search between pair ($x\in s,y\in s$)</li><li>If $s$ contains an element of the collision, we found it $\Rightarrow Pr=\frac{1}{\sqrt N}$</li></ol><p>Complexity:</p><ul><li>Inner Grover: $\sqrt{N}$</li><li>Outer Grover: $\sqrt{\sqrt N}$</li><li>Total: $\sqrt{\sqrt N}\cdot \sqrt{N}=O(N^{3/4})\le O(N)$</li></ul>
          </div>
</body></html>]]></content>
      <categories>
        <category>Quantum Computing</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Quantum Computing</tag>
        <tag>Quantum Algorithm</tag>
        <tag>Quantum Circuits</tag>
      </tags>
  </entry>
  <entry>
    <title>[Note] AuTO: Scaling Deep Reinforcement Learnign for Datacenter-Scale Automatic Traffic Optimization</title>
    <url>/Ending2015a/41710/</url>
    <content><![CDATA[<html><head></head><body><p><img src="https://i.imgur.com/xLpxJlN.png" alt></p>
<blockquote>
<p>原論文：Li Chen, J. Lingys, Kai Chen and Feng Liu. AuTO: Scaling Deep Reinforcement Learnign for Datacenter-Scale Automatic Traffic Optimization. SIGCOMM 2018.</p>
</blockquote>
<a id="more"></a>
<div class="pdfobject-container" data-target="https://conferences.sigcomm.org/events/apnet2018/papers/auto.pdf" data-height="500px"></div>
<h2 id="1-Introduction" class="heading-control"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction<a class="heading-anchor" href="#1-Introduction" aria-hidden="true"></a></h2><p>Traffic optimization (TO) 是很重要的 datacenter management 問題，包括：</p>
<ul>
<li>flow/coflow scheduling</li>
<li>congestion control</li>
<li>load balancing & routing</li>
</ul>
<p>目前 TO 普遍仰賴專家 heuristics 來 hand-craft，但實際上 TO 很注重 parameter 的 setting。如果 parameter mismatch 的話，TO 的效能反而會變得更差。例如：</p>
<ul>
<li>PIAS 的 thresholds 靠計算過去的 long term flow size distribution 來決定，因此容易發生與現在的 size distribution mismatch 的問題。效能最嚴重可能降低 38.64%</li>
<li>pFabric 也會發生類似的問題，在某些情況下即使很仔細的優化 Threshold，平均 FCT 甚至會下降 30%</li>
<li>Aalo 則因為無法動態 adaptation，因此必須仰賴操作員挑選數值。</li>
</ul>
<div class="note info">
            <p>FCT: Flow Completion Time</p>
          </div>
<p>通常，設定一個好的 TO 非常耗時，甚至會花上 1 週來選定 parameters。因為操作人員必須擁有很好的 insight、application knowledge、traffic statistics (長時間的收集)。通常步驟入下：</p>
<ol>
<li>先建立 monitoring system來蒐集終端系統的資料與數據</li>
<li>蒐集、分析、設定 parameters、在 simulation tools 上測試效果</li>
<li>將最後 heuristics 找出來最佳的 parameter 套用到 System 上</li>
</ol>
<p>因此，能夠自動化 TO 是非常吸引人的事情。此篇 Paper 設計了一套 自動化 TO 的演算法，可以套用到 volumnious、uncertain、volatile data center traffic 的環境，同時也能達到操作員期望的效果。為了測試 DRL 方法的效率，作者建立一個 flow-level centralized TO system，並使用 basic RL algorithm: Policy gradient。經測試後發現，即使使用很好的設備 (GPU) 運行 DRL，也無法在真正的 datacenter 的 scale ($\gt10^5 servers$) 下運行。關鍵在於 computation time ($~100ms$): short flow 在 DRL 下 Decision 前就已經不見了。</p>
<p>因此這篇的目標是 <em>如何使用 DRL-based 方法在 datacenter-scale 下進行自動 TO</em>。首先需要知道的是 flow 的 distribution，大部分的 flow 都是 short flow，淡大部份的 bytes 則來自 long flows。所以 TO 為 short flow 下 decision 必須快，而 long flow 的 decision 更具影響力。</p>
<p>使用 end-to-end DRL 設計 AuTO，並在 datacenter-scale 下使用 commodity hardware (一般商用硬體) 運行。AuTO 是 two-level DRL system，mimicking Peripheral Systems (PS; 周圍神經系統) and Central Systems (CS; 中樞神經系統)。PS 在所有 end-hosts 上運行，負責蒐集 flow information 並下立即的 TO decision 處理 short flow。PS 的 decision 會通過 Central System 來告知其他 host。global traffic information 都會在 CS 上集中處理。CS 也會處理 long flow 的 TO decision。</p>
<p>AuTO 的 scalability 關鍵是將耗時的 DRL 處理與需要立即下判斷的 short flow 分開。因此，使用 Multi-Level Feedback Queue 與給定的 threshold 來 schedule PS 的 flow。利用 MLFQ，PS 能夠根據 local information (bytes-sent and thresholds) 立即下決定，而 thresholds 則是通過 CS 的 DRL 來優化。透過這種方是，global TO decision 是以 MLFQ threshold 的形式從 CS 來通知所有 PS。另外，MLFQ 能夠分離 short flow 與 long flow，long flow 就會交由 CS 的 DRL 來決定他的 routing, rate limiting, 跟 priority。</p>
<div class="note info">
            <ul><li>Multi-Level Feedback Queue:<br>  Multiple FIFO queues are used and the operation is as follows:<ol><li>A new process is inserted at the end (tail) of the top-level FIFO queue.</li><li>At some stage the process reaches the head of the queue and is assigned the CPU.</li><li>If the process is completed within the time quantum of the given queue, it leaves the system.</li><li>If the process voluntarily relinquishes control of the CPU, it leaves the queuing network, and when the process becomes ready again it is inserted at the tail of the same queue which it relinquished earlier.</li><li>If the process uses all the quantum time, it is pre-empted and inserted at the end of the next lower level queue. This next lower level queue will have a time quantum which is more than that of the previous higher level queue.</li><li>This scheme will continue until the process completes or it reaches the base level queue.<ul><li>At the base level queue the processes circulate in round robin fashion until they complete and leave the system. Processes in the base level queue can also be scheduled on a first come first served basis.[4]</li><li>Optionally, if a process blocks for I/O, it is ‘promoted’ one level, and placed at the end of the next-higher queue. This allows I/O bound processes to be favored by the scheduler and allows processes to ‘escape’ the base level queue.</li></ul></li></ol></li></ul>
          </div>
<p>使用 Python 實做 AuTO，因此 AuTO can compatible with popular learning frameworks, such as Keras/TensorFlow。</p>
<p>使用 32 台 server 接上 2 台 switch 來測試 AuTO 的效果。經實驗證實，在 traffic 是 stable load and flow size sidtribution 的情況下，經過 8 小時的 training，AuTO 能夠改善效能 48.14% (與 huristics 比較) 。也顯示 AuTO 能夠穩定的學習且應用到 temporally and spatially heterogeneous traffic: 經過 8 小時的 training，AuTO 能夠減少 8.71%(9.18%) 的 average FCT (與 huristics 比較)。</p>
<h2 id="2-Background-and-Motivation" class="heading-control"><a href="#2-Background-and-Motivation" class="headerlink" title="2. Background and Motivation"></a>2. Background and Motivation<a class="heading-anchor" href="#2-Background-and-Motivation" aria-hidden="true"></a></h2><h3 id="2-1-Deep-Reinforcement-Learning-DRL" class="heading-control"><a href="#2-1-Deep-Reinforcement-Learning-DRL" class="headerlink" title="2.1 Deep Reinforcement Learning (DRL)"></a>2.1 Deep Reinforcement Learning (DRL)<a class="heading-anchor" href="#2-1-Deep-Reinforcement-Learning-DRL" aria-hidden="true"></a></h3><p><img src="https://i.imgur.com/jkGb0Vn.png" alt></p>
<p>使用 policy gradient。</p>
<h3 id="2-2-Example-DRL-for-Flow-Scheduling" class="heading-control"><a href="#2-2-Example-DRL-for-Flow-Scheduling" class="headerlink" title="2.2 Example: DRL for Flow Scheduling"></a>2.2 Example: DRL for Flow Scheduling<a class="heading-anchor" href="#2-2-Example-DRL-for-Flow-Scheduling" aria-hidden="true"></a></h3><p>解釋如何將 Flow scheduling 轉換為 DRL settings</p>
<p><strong>Flow scheduling problem</strong> 假設很多 server 跟一台 switch，network 是 non-blocking，full-bisection bandwidth 且是 proper load-balancing。在這個情況下 flow scheduling problem 可以簡化為決定 flow 的發送順序。preemptive scheduling，strict priority queuing。在每個 server 都建立 $K$ 個等級的 priority queue 來分類 flow，並遵守 strict pripority queuing。這 $K$ 個 queue 能夠透過 switches 控制，每個 flow 的 priority 能夠動態控制。每個 flow 的 packet 都會 tag 上他的 priority number。<br></p><div class="note info">
            <ul><li>Preemptive scheduling: Running process 可以被更高 priority 的 process 中斷</li><li>strict priority queueing: 分成 $K$ 個等級的 Queue，高等級的 process 會優先分配到 Resource。低等級的會餓死。</li></ul>
          </div><p></p>
<p><strong>DRL formulation</strong><br>Action space: Mapping from Active flows to priorities<br>State space: 不考量 routing 跟 load-balancing，state 只包含 flow states。flow states 用 5-tuple 表示，其中 active flow 為 $F^t_a$，deactive float (finished flow) 為 $F^t_d$，tuple 則是 source/desination IP, source/destination port numbers, and transport protocol。active flow 還多一項 priority，而 finished flow 則多兩項：FCT 跟 flow size。</p>
<p>Rewards: Reward 只有在 finished flow 才會給，finished flow $f$ 的 average throughput ${Tput}_f=\frac{\text{Size}_f}{\text{FCT}_f}$。Reward model 成與前一個 timestep $t-1$ 的總 throughput 的 ratio。</p>
<script type="math/tex; mode=display">
r_t = \frac{ \sum_{f^t\in F^t_d} {Tput}^t_f }{ \sum_{f^{t-1}\in F^{t-1}_d} {Tput}^{t-1}_f }</script><p>如果前一個 time step 的 throughput 比較大則表示 the agent has degraded the overall performance。目標是 maximize average throughput。</p>
<p><strong>DRL algorithm</strong> 使用 update rule</p>
<script type="math/tex; mode=display">
\theta \leftarrow \theta + \alpha + \sum_t \nabla_\theta \log\pi_\theta(s_t, a_t)(v_t -baseline)</script><p>其中 $v_t$ 是 empirical reward，$baseline$ 則是 cumulative average of experienced rewards per server。</p>
<h3 id="2-3-Problem-Identified" class="heading-control"><a href="#2-3-Problem-Identified" class="headerlink" title="2.3 Problem Identified"></a>2.3 Problem Identified<a class="heading-anchor" href="#2-3-Problem-Identified" aria-hidden="true"></a></h3><p>使用 PG algorithm，agent 只有一層 hidden layer。使用 two servers，一台負責 train DRL agent，另一台負責用 RPC interface 傳 traffic information，sending rate 設 1000 flows per second。</p>
<p>統計不同 implementation 的 processing latency: finish sending flow information 直到 receiving the DRL action。得到下圖：</p>
<p><img src="https://i.imgur.com/B2pNTBR.png" alt></p>
<p>由上圖可看出，即使只有 1 hidden layer，處理一次 flow 的 delay 還是需要至少 60ms，相當於在 1 Gbps bandwidth 上流過 7.5MB 的資料量。但根據某 well-known traffic trace 網站 與 Microsoft datacenter 的統計，7.5MB 的 flow 分別大於 99.9% 與 95.13% 的 flow。這表示大部分的 DRL action 都將無用。</p>
<h2 id="3-Auto-Design" class="heading-control"><a href="#3-Auto-Design" class="headerlink" title="3. Auto Design"></a>3. Auto Design<a class="heading-anchor" href="#3-Auto-Design" aria-hidden="true"></a></h2><h3 id="3-1-Overview" class="heading-control"><a href="#3-1-Overview" class="headerlink" title="3.1 Overview"></a>3.1 Overview<a class="heading-anchor" href="#3-1-Overview" aria-hidden="true"></a></h3><p>DRL system 最關鍵的問題在於他的蒐集資料與下決策的 long latency。目前主流 datacenter 都使用 $\ge 10 Gbps$ link speed，如此一來 round-trip latency 必須至少是 sub-millisecond 等級。因此問題在於要如何利用 DRL 達成 datacenter scale 的 TO？</p>
<p>近期研究指出，大部分的 flow 都是 short flow，但大部分的 traffic bytes 都是 from long flow。因此，有個方法是，將 short flow 的都委派給 end-host 自己處理，然後 long flow 再用 DRL algorithm 處理。</p>
<p>將 AuTO 設計成 two-level system，模仿 Peripheral and Central Nervous Systems in animals。Peripheral Systems (PS) 在 end-host 上運行，負責蒐集資料、下 short flow 的決策以減少 delay。Central System (CS) 則下 long flow 的 decision。除此之外，PS 下 decision 的條件會經由 CS 蒐集、處理 global traffic information 後設定。</p>
<p><img src="https://i.imgur.com/arRlkCw.png" alt></p>
<h3 id="3-2-Peripheral-System" class="heading-control"><a href="#3-2-Peripheral-System" class="headerlink" title="3.2 Peripheral System"></a>3.2 Peripheral System<a class="heading-anchor" href="#3-2-Peripheral-System" aria-hidden="true"></a></h3><p>AuTO 的 scalability 的關鍵在於，PS 利用 local information 根據 globally informed TO 來下 short flow 的決策。PS 有兩個 module: enforce module and monitoring module。</p>
<p><strong>Enforcement module</strong> 使用 Multi-Level Feedback Queueing (MLFQ) 來 schedule flows。Perform packet tagging in the DSCP field of IP packets at each end-host。總共 $K$ 個 Priorities $P_i$ 其中 $1\le i\le K$，$(K-1)$ 個 threshold $\alpha_j$，$1\le j\le K-1$。設定所有 switchs 根據 DSCP field 執行 strict priority queueing。當一個 flow 在 end-host 產生時，他會先被 tag 為 $P_1$，隨著傳輸的 bytes 增加，Priority 會往後調整 $P_j$ ($2\le j \le K$)。</p>
<p><img src="https://i.imgur.com/isqJQ2o.png" alt></p>
<p>使用 MLFQ 可以有以下特性：</p>
<ul>
<li>PS 可以馬上根據 local information 與 threshold 來下 decisions。</li>
<li>可以適用到 global traffic variations。CS 不直接下 flow 的 decision，而是根據 global information 來控制 PS 的 threshold。</li>
<li>short flow 與 long flow 會很自然的分開，short flow 會在前面一點的 priority queue，long flow 則會到後面的 Queue。CS 可以個別處理 long flow，決定他的 routing、rate limit、 priority。</li>
</ul>
<p><img src="https://i.imgur.com/NCEBLSh.png" alt></p>
<p><strong>Monitoring Module</strong> 負責蒐集 flow size 與 completion times，讓 CS 能夠分析 flow 的 distribution，並制定適當的 threshold。如果出現 long flow 的話 monitoring module 也會負責回報給 CS，讓 CS 能夠下決策。</p>
<h3 id="3-3-Central-System" class="heading-control"><a href="#3-3-Central-System" class="headerlink" title="3.3 Central System"></a>3.3 Central System<a class="heading-anchor" href="#3-3-Central-System" aria-hidden="true"></a></h3><p>CS 上有兩個 DRL agents：short flow RLA 負責優化 thresholds for MLFQ，long flow RLA (lRLA) 負責決定 long flow 的 rates, routes, priorities。sRLA 解決 FCT minimization 問題，使用 Deep Deterministic Policy Gradient。lRLA 使用 PG algorithm (Sec. 2.2)。</p>
<h2 id="4-DRL-Formulation-and-Solutions" class="heading-control"><a href="#4-DRL-Formulation-and-Solutions" class="headerlink" title="4. DRL Formulation and Solutions"></a>4. DRL Formulation and Solutions<a class="heading-anchor" href="#4-DRL-Formulation-and-Solutions" aria-hidden="true"></a></h2><h3 id="4-1-Optimizing-MLFQ-thresholds" class="heading-control"><a href="#4-1-Optimizing-MLFQ-thresholds" class="headerlink" title="4.1 Optimizing MLFQ thresholds"></a>4.1 Optimizing MLFQ thresholds<a class="heading-anchor" href="#4-1-Optimizing-MLFQ-thresholds" aria-hidden="true"></a></h3><p>flow sheduling 在每個 hosts 與 network switches 上執行，使用 $K$ strict priority queues，並在每個 flow 的 IP header 的 DSCP field 設定 priority。根據 Shortest-Job-First (SJF) 的特性，愈長的 flow 則愈低 priority。</p>
<p>其中一項難點就是要如何優化 MLFQ 的 threshold。前作 [8, 9, 14] 都是使用 mathematical analysis。[9] 則建議使用 collected flow-level traces weekly/monthly re-compute threshold。AuTO 則是直接使用 DRL 方式來預測 threshold $\alpha$。<br>Sec 2.2 的 PG 是最基本的 algorithm。Agent 目標在最佳化 policy $\pi_\theta(a|s)$ parameterized by $\theta$。然而，REINFORCE 與其他 PG algorithm 都是 stochastic policies，$\pi_\theta(a|s)=P[a|s;\theta]$。PG 沒有辦法處理 real values 的 action，因此，使用類似 Deterministic Policy Gradient (DPG) 的方法來處理 real value action $\{a_0, a_1, \dots, a_n\}$，給定 state $s$，$\alpha_i=\mu_\theta(s)$。DPG 是一種 actor-critic algorithm 用來訓練 deterministic policy，actor function $\mu_\theta$ 用來表示目前的 policy，critic neural network $Q(s, a)$ 用 Bellman equation 更新。</p>
<p><img src="https://i.imgur.com/pLzg9rQ.png" alt></p>
<p>Agent 的 actor 負責 sample environment 並使用下式更新 parameters $\theta$：</p>
<script type="math/tex; mode=display">
\theta^{k+1} \leftarrow \theta^k+\alpha E_{s\sim\rho^{\mu^k}}\left[\nabla_\theta\mu_\theta(s)\nabla_a Q^{\mu^k}(s, a)\middle|_{a=\mu_\theta(s)}\right]</script><p>其中 $\rho^{\mu^k}$ 是 state distribution at time $k$</p>
<p>Maximize objective function：</p>
<script type="math/tex; mode=display">
\begin{align}
J(\mu_\theta)
    &=\int_\mathcal{S}\rho^\mu(s)r(s,\mu_\theta(s))ds\\
    &=E_{s\sim\rho^\mu}\left[r(s,\mu_\theta(s))\right]
\end{align}</script><script type="math/tex; mode=display">
\begin{align}
\nabla_\theta J(\mu_\theta)
    &=\int_\mathcal{S}\rho^\mu(s)\nabla_\theta\mu_\theta(s)\nabla_a Q^{\mu^k}(s, a)\Big|_{a=\mu_\theta(s)}ds\\
    &=E_{s\sim\rho^u}\left[ \nabla_\theta\mu_\theta(s)\nabla_a Q^{\mu^k}(s, a)\Big|_{a=\mu_\theta(s)} \right]
\end{align}</script><p>使用 DDPG 更新，有 4 個 network：一個 actor $\mu_{\theta^\mu}(s)$，一個 critic $Q_{\theta^Q}(s, a)$，另外兩個是 target network $\mu’_{\theta^{\mu’}}(s)$ 和 $Q’_{\theta^{Q’}}(s, a)$。random mini-batch with size $N$，transition tuple $(s_i, a_i, r_i, s_{i+1})$。當時的 state-of-the-art。</p>
<p><img src="https://i.imgur.com/6U4HTDS.png" width="500px"></p>
<p><strong>DRL formulation</strong> 找到一組 optimal set of threshold $\{\alpha_i\}$ 來 minimize average FCT，把這個目標轉換成 DRL problem。設 cumulative density function of flow size distribution as $F(x)$，因此 $F(x)$ 代表 probability that a flow size is no larger than $x$。$L_i$ 表示 the number of packets a given flow brings in queue $Q_i$ for $i=1,\dots, K$。因此，$E[L_i]\le (\alpha_i-\alpha_{i-1})(1-F(\alpha_{i-1}))$。設 flow arrival rate as $\lambda$，則 packet arrival rate to queue $Q_i$ 是 $\lambda_i=\lambda E[L_i]$。設 $P_1$ service rate $\mu_1=\mu$ 其中 $\mu$ 是 link 的 service rate。則 $Q_1$ 的 idle rate 就是 $(1-\rho_1)$ 其中 $\rho_i=\lambda_i/\mu_i$ 是 $Q_i$ 的 utilization rate。$Q_2$ 的 service rate 則是 $\mu_2=(1-\rho_1)\mu$。<br>得到 $\mu_i=\Pi^{i-1}_{j=0}(1-\rho_{j})\mu$，其中 $\rho_0=0$。因此 $T_i=1/(\mu_i-\lambda_i)$ 就會是 average delay。對於一個 size $[\alpha_{i-1}, \alpha_{i})$ 的 flow 來說，在不同的 queue 會經歷不同的 delay。若說 $T_i$ 是 $Q_i$ 所需的 average spent time，設 $i_{max}(x)$ 代表比 flow size $x$ 還大一個層級的 threshold，則 flow size $x$ 的 average FCT $T(x)$ 會有 upper bound: $\sum^{i_{max}(x)}_{i=1}T_i$。</p>
<div class="note info">
            <ul><li>$\alpha_i$ 是 threshold 其實就是 flow 的 size</li><li>$\mu$ 代表 link 的使用 rate，$\rho_i$ 代表某 queue $Q_i$ 佔用 link 的比例，由於先給 priority 高的使用因此 priority 低的就用剩下的部份，低 priority 的 service rate (佔 link 的比例) 就會是 $\mu_i=\Pi^{i-1}_{j=0}(1-\rho_{j})\mu$ (link 的 total capacity 乘上前面所有 queue 用剩的比例)</li></ul>
          </div>
<p>Let $g_i=F(\alpha_i)-F(\alpha_{i-1})$ 表示 percentage of flows with size $[\alpha_{i-1}, \alpha_{i})$。則可以 formulate FCT minimization problem:</p>
<script type="math/tex; mode=display">
\begin{eqnarray}
&\min_{\{g\}} 
    &\mathcal{T}(\{g\})=\sum^K_{l=1}(g_l\sum^l_{m=1}T_m)=\sum^K_{l=1}(T_l\sum^K_{m=l}g_m)\\
&\text{subject to}~~~
    & g_i\ge0, i=1,\dots,K-1
\end{eqnarray}</script><p><em>State space</em>: states are the set of, the set of all finished flows $F_d$ inthe entire network in the current time step。每個 flow 用 5-tuple 表示：source/destination IP, source/destination port number, transport protocal。finished flows 還包含 FCT 與 flow size。共 7 個 feature。</p>
<p><em>Action space</em>: action space 由 centralized agent, sRLA 處理。action 是一組 MLFQ threshold ${\alpha^t_i}$。</p>
<p><em>Rewards</em>: Reward 是 delayed feedback 來告訴 agent 在前一個 timestep 的 action 如何。reward 設為與前一個 objective (expectation of FCT) 的 ratio $r_t=\frac{\mathcal{T}^{t-1}}{\mathcal{T}^{t}}$，如果效能變差了 $r_t\le1$，變好了則 $r_t\ge 1$。</p>
<p><strong>DRL algorithm</strong> 使用 buffer 儲存 tuple $(s_t, a_t, r_t, s_{t+1})$。</p>
<h3 id="4-2-Optimizing-Long-Flows" class="heading-control"><a href="#4-2-Optimizing-Long-Flows" class="headerlink" title="4.2 Optimizing Long Flows"></a>4.2 Optimizing Long Flows<a class="heading-anchor" href="#4-2-Optimizing-Long-Flows" aria-hidden="true"></a></h3><p>MLFQ thresholds $\alpha_{K-1}$ 將 long flow 與 short flow 區分出來，$\alpha_{K_1}$ 是會動態更新的，不像前作是 fixed [1, 22] 的。lRLA 使用 PG algorithm，差別只在於 Action space。</p>
<p><em>Action space</em>: 對每個 active flow $f$ 在 timestep $t$ 時的 action 是 $\{Prio_t(f), Rate_t(f), Path_t(f)\}$。其中 $Prio_t(f)$ 是 flow priority，$Rate_t(f)$ 是 rate limit，$Path_t(f)$ 是 path to take for flow $f$。假設 path 是跟前作 XPath [32] 一樣的方式定義。</p>
<p><em>State space</em>: 同 Sec 2.2</p>
<p><em>Rewards</em>: Reward 由 finished flows 定義，可以是：difference or ratios of sending rate, link utilization, throughput in consecutive timesteps。在 $10Gbps$ 的 link 下很難測量 active flows 的時間性的 flow-level information，因此只使用 finished flows，並使用 average throughputs 的 ratio。</p>
<script type="math/tex; mode=display">
r_t = \frac{ \sum_{f^t\in F^t_d} {Tput}^t_f }{ \sum_{f^{t-1}\in F^{t-1}_d} {Tput}^{t-1}_f }</script><h2 id="5-Implementation" class="heading-control"><a href="#5-Implementation" class="headerlink" title="5. Implementation"></a>5. Implementation<a class="heading-anchor" href="#5-Implementation" aria-hidden="true"></a></h2><p>使用 Python 2.7、Keras</p>
<h3 id="5-1-Peripheral-System" class="heading-control"><a href="#5-1-Peripheral-System" class="headerlink" title="5.1 Peripheral System"></a>5.1 Peripheral System<a class="heading-anchor" href="#5-1-Peripheral-System" aria-hidden="true"></a></h3><p>PS 有 Monitoring Module (MM) 跟 Enforcement Module (EM)。MM 負責蒐集 information，包括 recently finished flows 以及 presently active long flows。MM 會固定時間回傳資訊給 CS。EM 則負責 tagging active flows，以及 routing、rate limiting、priority tagging for long flows。本篇使用 Remote Procedure Call (RPC) 來實作傳輸界面。</p>
<h4 id="5-1-1-Monitoring-Module" class="heading-control"><a href="#5-1-1-Monitoring-Module" class="headerlink" title="5.1.1 Monitoring Module"></a>5.1.1 Monitoring Module<a class="heading-anchor" href="#5-1-1-Monitoring-Module" aria-hidden="true"></a></h4><p>MM 雖然可以實做成 Linux kernel module，如 PIAS [8]，但因為這次實驗是用 flow generator 來產生 flow，所以 Monitoring Module 直接實做在 generator 裡面。</p>
<p>每 $T$ 秒，MM 會將 $n_l$ 個 active long flows (6 個 attributes) 跟 $m_l$ 個 finished long flows (7 個 attributes) 以及 $m_s$ 個 active short flows (7個 attributes) 整合起來回傳給 CS。</p>
<p>$\{n_l,m_l,m_s\}$ 根據 traffic load 跟 $T$ 來決定，這幾個變數會是 upper-bound，若每 $T$ 秒無法蒐集齊的話，會 zero-padding，會這樣做是因為 DNN 的 Input 大小是固定的。而因實驗使用 flow generator，所以這部份可以控制。本篇使用 $\{n_l=11,m_l=10.m_s=100\}$<br></p><div class="note success">
            <p>Future work: Dynamic DNN、RNN (Dynamic input size)</p>
          </div><p></p>
<h4 id="5-1-2-Enforcement-Module" class="heading-control"><a href="#5-1-2-Enforcement-Module" class="headerlink" title="5.1.2 Enforcement Module"></a>5.1.2 Enforcement Module<a class="heading-anchor" href="#5-1-2-Enforcement-Module" aria-hidden="true"></a></h4><p>EM 會定期接收 CS 的 action。包括 MLDQ thresholds 跟 local long flows 的 TO decisions。EM 是建立在 PIAS [8] kernel module，並加上可以 dynamic configuration threshold。</p>
<p>short flow 使用 ECMP 處理 routing 跟 load-balancing，因為 short flow 不需要 centralized per-flow control 跟 DCTCP for congestion control。</p>
<p>long flows 則使用 TO actions，包括 priority、rate limiting、routing。使用相同的 kernel module 還做 priority tagging。Rate limiting 使用 Hierarchical token bucket (HTB) queueing discipline in Linux traffic control (TC)。使用 parent class in HTB 來控制 total outbound bandwidth，讓 CS 能夠控制每個 Node 的 outbound rate limit。每當一個 flow 被判定為 long flow (掉到 MLFQ 最後一個 queue) EM 就會 create 一個 HTB filter，利用 5-tuple 來 filter 掉 long flow。當 EM 收到 rate allocation decisions from the CS，EM 就會發送 Netlink messages 到 Linux kernel 來更新 the child class of the particular flow：TC class 的 rate 設定成 CS 指定的 rate，而 TC class 的 rate 的 ceiling 則是設定成 CS 指定的兩倍但不超過 original rate。</p>
<h3 id="5-2-Central-System" class="heading-control"><a href="#5-2-Central-System" class="headerlink" title="5.2 Central System"></a>5.2 Central System<a class="heading-anchor" href="#5-2-Central-System" aria-hidden="true"></a></h3><p>CS run RL agents (sRLA & lRLA) 來 optimize TO decisions。使用 SEDA-link architecture [58] 來處理 incoming updates 並 sending actions to PS。architecture 分成幾個 stages：http request handling，Deep network learning/processing，response sending。每個 stage 都會有各自的 processes，之間使用 queue 來傳輸訊息。確保 CS 的 cores 都能夠被用上、處理所有 PS 的 requests、分攤 load。</p>
<h4 id="5-2-1-sRLA" class="heading-control"><a href="#5-2-1-sRLA" class="headerlink" title="5.2.1 sRLA"></a>5.2.1 sRLA<a class="heading-anchor" href="#5-2-1-sRLA" aria-hidden="true"></a></h4><p>使用 Keras 實做 sRLA 並使用 DDPG algorithm。<br><em>Actors</em>: two fully-connected hidden layers with 600 and 600 neurons。output layer with $K-1$ output units (one for each threshold)。input layer 吃 states (700 features per-sever ($m_s=100$)) and outputs MLFQ thresholds for each host for timestep $t$</p>
<p><em>Critics</em>: three hidden layers。輸入 states，最後一層 hidden layer 前會 concat output from actors。</p>
<p>從 replay buffer 裡面 sample mini-batches of experiences: $\{s_t,a_t,r_t,s_{t+1}\}$。</p>
<h4 id="5-2-2-lRLA" class="heading-control"><a href="#5-2-2-lRLA" class="headerlink" title="5.2.2 lRLA"></a>5.2.2 lRLA<a class="heading-anchor" href="#5-2-2-lRLA" aria-hidden="true"></a></h4><p>10 hidden layers 全部都是 fully-connected layer with 300 neurons。input states (136 features per-server ($n_l=11,m_l=10$)) output probabilities for the actions for all the active flows。</p>
<p><strong>Summary</strong><br>hyper-parameters 是經過實驗挑選的，發現更深的 network 並沒有表現的多好反而花更長時間 training。綜合所有考量 (準確度、training 時間、時間延遲)，認為這樣的參數設定是最好的。</p>
<h2 id="6-Evaluation" class="heading-control"><a href="#6-Evaluation" class="headerlink" title="6. Evaluation"></a>6. Evaluation<a class="heading-anchor" href="#6-Evaluation" aria-hidden="true"></a></h2><p>主要 evaluate</p>
<ol>
<li>在 Stable traffic (固定 flow size distribution and traffic load) 情況下 AuTO 與 standard heuristics 的差</li>
<li>在 varying traffic characteristics 情況下 AuTO 適不適用</li>
<li>AuTO 的反應 traffic dynamics 的速度</li>
<li>performance overheads and overall scalability</li>
</ol>
<p><strong>Summary of results (grouped by scenarios):</strong></p>
<ul>
<li><strong>Homogeneous</strong>: fixed flow size distribution 跟 load 情況下，AuTO 的 threshold 能 converge 且 average FCT 表現的比 standard heuristics 快 48.14% </li>
<li><strong>Spatially Heterogeneous</strong>: 將 server 分成 4 個 clusters，各自產生不同 flow size distribution 跟 load；AuTO 的 threshold 能 converge，且 average FCT 表現的比 standard heuristics 快 37.20%</li>
<li><strong>Spatially & Temporally Heterogeneous</strong>: 同上述情景，但 flow size distribution 跟 load 會回時間改變，AuTO 能夠 learning 且展現很好的 adaptation。跟 fixed heuristics 比較，heuristics 只有在某些特定的 traffic settings combination 下能夠險勝 AuTO。</li>
<li><strong>System Overhead</strong>: AuTO implementation 能夠在 10ms 內給予立即的 state 更新。AuTO 同時也展現在 CPU urilization 和 throughput degradation 方面的 minimal end-host overhead。</li>
</ul>
<p><strong>Setting</strong> AuTO 實驗使用 32 台 server。Switch 支援 ECN 跟 struct priority queueing 最多有 8 個 queues。Server 則是 Dell PowerEdge R320 + 4-core Intel E5-1410 2.8GHz CPU，8G memory，and Broadcom BCM5719 NetXtreme Gigabit Ethernet NIC with 4x1Gbps ports。系統則是 64-bit Debian 8.7 (3.16.39-1 Kernel)。By default, advanced NIC offload mechanisms are enabled to reduce the CPU overhead. The base round-trip time (RTT) of our testbed is 100us。</p>
<p><img src="https://i.imgur.com/pzANneq.png" alt></p>
<p>使用前作 [2, 7, 9, 15] 使用的 traffic generator [20]，給定 flow size distribution 跟 traffic load 來自動產生 traffic flows。</p>
<p>使用兩種 realistic workloads：web search workload 跟 data mining workload。其中 15 台負責產生 flow 的叫做 application server，剩下一台就是 CS。每台 application server 會有 3 個 ports 連到 data plane switch，剩下的 port (1個) 連到 control plane switch 跟 CS 溝通。3 個 port 設定在不同的 subnet。兩台 Switch 都是 Pronto-3297 48-port Gigabit Ethernet switch。<br><img src="https://i.imgur.com/v6AZRML.png" alt></p>
<p><strong>Comparison Targets</strong> 跟兩種 heuristics 比較：Shortest-Job-First (SJF) 與 Least-Attained-Service-First (LAS)。差異在於，SJF 需要在一開始就給定 flow size。這兩個算法都需要蒐集一段時間才能決定 threshold，通常會蒐集幾周 (表示每更新一次 threshold 都需要隔幾周)。<br>實驗使用 quantized SJF 跟 LAS with 4 priority levels。</p>
<ul>
<li>Quantized SJF (QSJF): 三個 thresholds $\alpha_0<\alpha_1<\alpha_2$。直接從 flow generator 得到 flow size。最小 size 的 flow 有最高 priority。</li>
<li>Quantized LAS (QLAS): 三個 thresholds $\beta_0<\beta_1<\beta_2$。每個 flow 一開始都是最高 priority 直到送超過 $\beta_i$ 時就會降低 priority。</li>
</ul>
<p>Thresholds 則是根據 flow size distribution and traffic load 按照 [14] 方式計算。除非有指定，否則都是使用 DCTCP distribution at 80% load 情景計算出來的 thresholds。</p>
<h3 id="6-1-Experiments" class="heading-control"><a href="#6-1-Experiments" class="headerlink" title="6.1 Experiments"></a>6.1 Experiments<a class="heading-anchor" href="#6-1-Experiments" aria-hidden="true"></a></h3><h4 id="6-1-1-Homogeneous-traffic" class="heading-control"><a href="#6-1-1-Homogeneous-traffic" class="headerlink" title="6.1.1 Homogeneous traffic"></a>6.1.1 Homogeneous traffic<a class="heading-anchor" href="#6-1-1-Homogeneous-traffic" aria-hidden="true"></a></h4><p>flow size distribution 跟 load 都固定。Web Search (WS) 跟 Data Mining (DM) distribution 設定 80% load。這兩個 distribution 分別代表不同 group 的 flows: a mixture of short and long flows (WS), a set of short flows (DM)。average 跟 99th percentile FCT 如下：<br><img src="https://i.imgur.com/H1BypuL.png" alt></p>
<p>Train AuTO 8小時，使用 train 完後的 model schedule flow for one hour。</p>
<ul>
<li>mixture of short and long flow (WS)，AuTO 比 standard heuristics 快 48.14% average FCT。因為 AuTO 可以動態調整 long flow 的 priority 避免 starvation problem。</li>
<li>short flows (DM)，AuTO 表現跟 heuristics 差不多，因為 AuTO 在一開始也是給所有 flow 最高的 priority，因此表現結果跟 QLAS 差不多。</li>
<li>RL Training 期間能夠讓 average FCT 減少 18.61% 跟 4.12% for WS&DM。</li>
<li>將 incast traffic 獨立出來看，發現 QLAS 跟 QSJF 表現差不多，因為這在 congestion control 就已經處理好。DCTCP 已經把 incast 處理的很好。<div class="note info">
            <p>incast traffic: 多對一傳輸</p>
          </div>
</li>
</ul>
<h4 id="6-1-2-Spatially-heterogeneous-traffic" class="heading-control"><a href="#6-1-2-Spatially-heterogeneous-traffic" class="headerlink" title="6.1.2 Spatially heterogeneous traffic"></a>6.1.2 Spatially heterogeneous traffic<a class="heading-anchor" href="#6-1-2-Spatially-heterogeneous-traffic" aria-hidden="true"></a></h4><p>把 server 分成 4 個 clusters，create spatially hetrogeneous traffic。分別分成四種不同的 load 跟 distribution：<ws 60%>，<ws 80%>，<dm 60%>，<dm 60%></dm></dm></ws></ws></p>
<p><img src="https://i.imgur.com/fMKDPEf.png" alt></p>
<p>heuristics 的 thresholds 4 個 clusters 分別根據 distribution 與 load 計算。比較結果後發現跟 homogeneous traffic 的情況很像。 AuTO 跟 QLAS 比，average FCT 快 37.20%，99th percentile FCT 快 19.78%；跟 QSJF 比，average FCT 快 27.95%，99th percentile FCT 快 11.98%。</p>
<h4 id="6-1-3-Temporally-amp-spatially-heterogeneous-traffic" class="heading-control"><a href="#6-1-3-Temporally-amp-spatially-heterogeneous-traffic" class="headerlink" title="6.1.3 Temporally & spatially heterogeneous traffic"></a>6.1.3 Temporally & spatially heterogeneous traffic<a class="heading-anchor" href="#6-1-3-Temporally-amp-spatially-heterogeneous-traffic" aria-hidden="true"></a></h4><p>每小時改變 flow size distribution 與 network load：load value 為 {60%, 70%, 80%}，distribution 則是隨機挑選，並確保每個小時的 load 與 distribution 會不同。實驗 run 8 小時。</p>
<p><img src="https://i.imgur.com/yEUC2Xx.png" alt><br><img src="https://i.imgur.com/G5axuHf.png" alt></p>
<ul>
<li>heuristics with fixed parameters，當 traffic 特性 match 到 parameters 的時候 average 99th percentile FCT 都表現的比其他方法好。但當 mismatch 時，FCT 瞬間變差，顯示他的適應 dynamic traffic 的能力差。</li>
<li>AuTO 能夠學習適應 dynamiccally changed traffic，最後一個 hour，AuTO achieves 8.71%(9.18%) average (99th percentile) FCT compared to QSJF。這是因為 AuTO 使用 2 個 agent 來學習並動態調整 priorities of flows。不需人為干涉，能夠自動調整。</li>
</ul>
<p>觀察 AuTO 可以發現 a constant decline in FCTs 表示他能夠在 dynamic traffic 下學習，最後收斂到 local optimum。表示 datacenter traffic scheduling 可以轉換為 RL problem and DRL techniques can be applied to solve it。</p>
<h3 id="6-2-Deep-Dive" class="heading-control"><a href="#6-2-Deep-Dive" class="headerlink" title="6.2 Deep Dive"></a>6.2 Deep Dive<a class="heading-anchor" href="#6-2-Deep-Dive" aria-hidden="true"></a></h3><h4 id="6-2-1-Optimizing-MLFQ-thresholds-using-DRL" class="heading-control"><a href="#6-2-1-Optimizing-MLFQ-thresholds-using-DRL" class="headerlink" title="6.2.1 Optimizing MLFQ thresholds using DRL"></a>6.2.1 Optimizing MLFQ thresholds using DRL<a class="heading-anchor" href="#6-2-1-Optimizing-MLFQ-thresholds-using-DRL" aria-hidden="true"></a></h4><p>在 60% load 的環境下 train sRLA for 8 小時之後，跟 PIAS 方法算出來的 threshold 做比較，發現除了最後一個 threshold 有差之外其他都差不多。</p>
<p><img src="https://i.imgur.com/HVzSsc3.png" alt></p>
<p>把 average FCT 跟 99t percentile FCT 畫出來看發現，兩個的 performance 差不多。因此下結論，AuTO train 8 個小時後 performance 跟 PIAS 差不多。</p>
<p><img src="https://i.imgur.com/nWnlRe2.png" alt><br><img src="https://i.imgur.com/ocnTpii.png" alt></p>
<h4 id="6-2-2-Optimizing-Long-Flows-using-DRL" class="heading-control"><a href="#6-2-2-Optimizing-Long-Flows-using-DRL" class="headerlink" title="6.2.2 Optimizing Long Flows using DRL"></a>6.2.2 Optimizing Long Flows using DRL<a class="heading-anchor" href="#6-2-2-Optimizing-Long-Flows-using-DRL" aria-hidden="true"></a></h4><p>在 experiment (Sec 6.1.3) 時持續 5 分鐘紀錄 long flow 的數量。設 $L$ 表示 the set of all links，$N_l(t)$ 表示 the number of long flows on link $l\in L$。$N(t)=\{N_l(t),\forall l\}$，下圖表示 $\max(N(t))-min(N(t))m \forall t$，用來說明 load imbalance。</p>
<p><img src="https://i.imgur.com/aaAxoY6.png" alt></p>
<p>發現通常 imbalance 不超過 10 個 flow。即使偶爾發生很不 imbalance 的情況，lRLA 會自動調整將過多的 flow 送到比較少的 link 上。這是因為如 Sec 2.2 所說，使用 Throughput 當作 reward，當多個 flows 集中在同一條 link 上，他的 throughput 會比分散在各個 link 上還低。</p>
<h4 id="6-2-3-System-Overhead" class="heading-control"><a href="#6-2-3-System-Overhead" class="headerlink" title="6.2.3 System Overhead"></a>6.2.3 System Overhead<a class="heading-anchor" href="#6-2-3-System-Overhead" aria-hidden="true"></a></h4><p>探討 AuTO performance 跟 System overhead。首先先探討 CS 的 response latency，以及 scalability。接著探討 end-host PS 的 overhead</p>
<p><strong>CS Response Latency</strong> 測量方式如下：$t_u$ 代表 CS 收到 update 的時間點，$t_s$ 代表 CS 回饋 action 的時間點。response time 是 $t_s-t_u$。發現 CS 平均可以在 10ms 內回應所有 server。這個時間是由 computation overhead of DNN 跟 update queueing delay 造成。AuTO 目前只用 CPU。<br></p><div class="note success">
            <p>能夠保證降低 latency 的方式是 CPU-GPU hybrid training and serving，CPU 負責 interfact with the environment，GPU 負責在背景 training model。</p>
          </div><br><img src="https://i.imgur.com/UBxN26v.png" alt><p></p>
<p>Response latency 也會隨 DNN computation complexity 增加。AuTO 的 network size 是由 $\{n_l, m_l, m_s\}$ 決定。若把 $\{n_l, m_l\}$ 從 ${11, 10}$ 增加到 ${1000, 1000}$，average response time 會變成 81.82ms。下圖為增加 $m_s$ 數量的走勢圖。<br><img src="https://i.imgur.com/NUGOXTW.png" alt></p>
<p>發現 $m_s$ 越大， response latency 增加愈慢，因為 $m_s$ 數量會決定 input layer size。只會影響 input layer 到第一層 hidden layer 的 matrix size。<br></p><div class="note success">
            <p>未來如果 AuTO 使用更複雜的 DNN，可以利用 parallelization techniques for DRL [6, 25, 27, 39] 來 reduce the response latency。</p>
          </div><p></p>
<p><strong>CS Scalability</strong> 實驗環境比較小，CS 的 NIC capacity 並沒有用滿，monitoring flow 所使用的 bandwidth 只有 12.40Kbps per server。假設 1Gbps network interface，CS 最多可以 monitor 80.64K servers。或是利用下列方法提昇 CS scalability<br></p><div class="note success">
            <ol><li>現行的 datacenter 使用 10Gbps 或更高的 network interface</li><li>CS 使用 GPUs 或其他加速計算的裝置</li><li>reduce the bandwidth of monitoring flows by compression</li></ol>
          </div><p></p>
<p><strong>PS Overhead</strong> 測量 CPU utilization，跟 throughput reduction。嘗試測量沒有使用 MM 跟 EM 發現 overhead 差異極小 (CPU utilization 差異少於 1%)，表示 AuTO 的 throughput 跟 GPU overhead 極小，跟 PIAS 差不多。</p>
<h2 id="7-Related-Works" class="heading-control"><a href="#7-Related-Works" class="headerlink" title="7. Related Works"></a>7. Related Works<a class="heading-anchor" href="#7-Related-Works" aria-hidden="true"></a></h2><h2 id="8-Conclution-Remarks" class="heading-control"><a href="#8-Conclution-Remarks" class="headerlink" title="8.Conclution Remarks"></a>8.Conclution Remarks<a class="heading-anchor" href="#8-Conclution-Remarks" aria-hidden="true"></a></h2><div class="note success">
            <p>RL algorithm for congestion control and task scheduling<br>WAN bandwidth management</p>
          </div>
</body></html>]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Reinforcement Learning</category>
        <category>Applications</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Deep Learning</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Paper Note</tag>
        <tag>Datacenter Management</tag>
        <tag>Traffic Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>[Note] Quantum Computation and Quantum Information - Chapter 4: Quantum Circuits</title>
    <url>/Ending2015a/56696/</url>
    <content><![CDATA[<html><head></head><body><p>上課筆記，原文書:</p>
<blockquote>
<p>Quantum Computation and Quantum Information, Michael A. Nielsen & Isaac L. Chuang</p>
</blockquote>
<h2 id="4-1-Quantum-algorithm" class="heading-control"><a href="#4-1-Quantum-algorithm" class="headerlink" title="4.1 Quantum algorithm"></a>4.1 Quantum algorithm<a class="heading-anchor" href="#4-1-Quantum-algorithm" aria-hidden="true"></a></h2><p><img src="https://i.imgur.com/nACul9I.png" alt></p>
<a id="more"></a>
<p>There are mainly two classes of algorithms:</p>
<ol>
<li>Based on Shor’s quantum Fourier transform, e.g. shor’s algorithm (factoring)<ul>
<li>exponential speedup</li>
</ul>
</li>
<li>Based upon Grover’s algorithm (search algorithm)<ul>
<li>quadratic speedup</li>
</ul>
</li>
</ol>
<h2 id="4-2-Single-qubit-operations" class="heading-control"><a href="#4-2-Single-qubit-operations" class="headerlink" title="4.2 Single qubit operations"></a>4.2 Single qubit operations<a class="heading-anchor" href="#4-2-Single-qubit-operations" aria-hidden="true"></a></h2><p>A single qubit: $\vert \psi\rangle=a\vert0\rangle+b\vert1\rangle$, where $a,b \in \mathbb{C}$ and $\vert a\vert^2+\vert b\vert^2=1$.</p>
<p>Quantum gates (operations):</p>
<ul>
<li>Pauli matrices:<script type="math/tex; mode=display">
X\equiv\left[\begin{matrix}
0 & 1\\
1 & 0
\end{matrix}\right];\quad
Y\equiv\left[\begin{matrix}
0 & -i\\
i & 0
\end{matrix}\right];\quad
Z\equiv\left[\begin{matrix}
1 & 0\\
0 & -1
\end{matrix}\right]</script></li>
<li>Hadamard gate:<script type="math/tex; mode=display">
H\equiv\frac{1}{\sqrt{2}}\left[\begin{matrix}
1 & 1\\
1 & -1
\end{matrix}\right]=(X+Z)/\sqrt{2}</script></li>
<li>Phase gate:<script type="math/tex; mode=display">
S\equiv\left[\begin{matrix}
1 & 0\\
0 & i
\end{matrix}\right]=T^2</script></li>
<li>$\pi/8$ gate:<script type="math/tex; mode=display">
T\equiv\left[\begin{matrix}
1 & 0\\
0 & \exp(i\pi/4)
\end{matrix}\right]=
\exp(i\pi/8)\left[\begin{matrix}
\exp(-i\pi/8) & 0\\
0 & \exp(i\pi/8)
\end{matrix}\right]</script></li>
</ul>
<p><img src="https://i.imgur.com/odEIVnM.png" alt></p>
<p>Bloch sphere representation: </p>
<ul>
<li>A single qubit can be visualized as a point $(\theta, \varphi)$ on the unit sphere.<script type="math/tex; mode=display">
\vert \psi\rangle = \cos(\theta/2)\vert 0\rangle + e^{i\varphi}\sin(\theta/2)\vert 1\rangle\</script></li>
<li>Bloch vector: $(\cos\varphi\sin\theta, \sin\varphi\sin\theta, \cos\theta)$</li>
</ul>
<p>Exponentiating Pauli matrices gives <em>rotation operators</em> about the $\hat{x}$, $\hat{y}$ and $\hat{z}$ axes:</p>
<script type="math/tex; mode=display">
\begin{align*}
R_x(\theta) &\equiv e^{-i\theta X/2}=\cos\frac{\theta}{2}I-i\sin\frac{\theta}{2}X=\left[\begin{matrix}
\cos\frac{\theta}{2} & -i\sin\frac{\theta}{2}\\
-i\sin\frac{\theta}{2} & \cos\frac{\theta}{2}
\end{matrix}\right]\\
R_y(\theta) &\equiv e^{-i\theta Y/2}=\cos\frac{\theta}{2}I-i\sin\frac{\theta}{2}Y=\left[\begin{matrix}
\cos\frac{\theta}{2} & -\sin\frac{\theta}{2}\\
\sin\frac{\theta}{2} & \cos\frac{\theta}{2}
\end{matrix}\right]\\
R_z(\theta) &\equiv e^{-i\theta Z/2}=\cos\frac{\theta}{2}I-i\sin\frac{\theta}{2}Z=\left[\begin{matrix}
e^{-i\theta/2} & 0\\
0 & e^{i\theta/2}
\end{matrix}\right]
\end{align*}</script><div class="note info">
            <p><strong>Exercise 4.2</strong>: Let $x$ be a real number and $A$ a matrix such that $A^2=I$. Show that</p><script type="math/tex; mode=display">e^{iAx}=\cos(x)I +i\sin(x)A.</script><p><em>Proof</em>: Recall that the taylor series</p><script type="math/tex; mode=display">e^x=\sum^\infty_{n=0}\frac{x^n}{n!}, \quad\sin(x)=\sum^\infty_{n=0}\frac{(-1)^n}{(2n+1)!}x^{2n+1}, \quad\cos(x)=\sum^\infty_{n=0}\frac{(-1)^n}{(2n)!}x^{2n}</script><p>For $A^2=I$, </p><script type="math/tex; mode=display">\begin{align*}e^{iAx}&=\sum_n\frac{(iAx)^n}{n!}=\sum_{n\in \text{even}}\frac{(ix)^n}{n!}I+\sum_{n\in\text{odd}}\frac{(ix)^n}{n!}A\\    &= \cos(x)I+i\sin(x)A\end{align*}</script>
          </div>
<p>If $\hat{n}=(n_x,n_y, n_z)$ is a real unit vector in three dimensions then we generalize the previous definitions by defining a rotation by $\theta$ about the $\hat{n}$ axis by the equation</p>
<script type="math/tex; mode=display">
R_{\hat{n}}(\theta)\equiv e^{-i\theta\hat{n}\cdot\vec{\sigma}/2}=\cos\left(\frac{\theta}{2}\right)I-i\sin\left(\frac{\theta}{2}\right)(n_xX+n_yY+n_zZ),</script><p>where $\vec{\sigma}$ denotes the three component vector $(X,Y,Z)$ of Pauli matrices.</p>
<p>Operation function for diagonalizable matrix:<br>Let $A$ be an diagonalizable matrix $A=\sum_\lambda \lambda\vert \psi_\lambda\rangle\langle\psi_\lambda\vert$, an operation function $f: \mathbb{C}\to\mathbb{C}$. Then</p>
<script type="math/tex; mode=display">
f(A)=\sum_\lambda f(\lambda)\vert \psi_\lambda\rangle\langle\psi_\lambda\vert</script><p>For example, a diagonalizable matrix $Z=\vert 0\rangle\langle 0\vert - \vert 1\rangle\langle 1\vert=\left[\begin{matrix}1&0\\0&-1\end{matrix}\right]$, an operation function $f(x)=e^{\theta x}$. Then</p>
<script type="math/tex; mode=display">
f(Z)=e^{\theta Z}= e^\theta\vert 0\rangle\langle 0\vert + e^{-\theta}\vert 1\rangle\langle 1\vert=\left[\begin{matrix}e^\theta&0\\0&e^{-\theta}\end{matrix}\right].</script><p>For example, $f(x)=\sum_{n\ge0}a_nx^n$ (Taylor expantion).</p>
<script type="math/tex; mode=display">
\begin{align*}
f(A) &= \sum_\lambda f(\lambda)\vert \psi_\lambda\rangle\langle \psi_\lambda\vert = \sum_\lambda\sum_n a_n\lambda^n\vert \psi_\lambda\rangle\langle\psi_\lambda\vert\\
    &= \sum_n a_n\sum_\lambda \lambda^n\vert\psi_\lambda\rangle\langle\psi_\lambda\vert = \sum_n a_n A^n
\end{align*}</script><div class="note success">
            <p><strong>Theorem 4.1</strong>: ($Z$-$Y$ <strong>decomposition for a single qubit</strong>) Suppose $U$ is a unitary operation on a single qubit. Then there exist real numbers $\alpha, \beta, \gamma$ and $\delta$ such that</p><script type="math/tex; mode=display">U=e^{i\alpha}R_z(\beta)R_y(\gamma)R_z(\delta)</script><p><em>Proof</em>: Let a unitary matrix $U=\left[\begin{matrix}a&c\\b&d\end{matrix}\right]$</p><script type="math/tex; mode=display">\begin{align*}\vert a\vert^2+\vert b\vert^2=1 &\Rightarrow a\equiv e^{i\phi_1}\cos\frac{\gamma}{2},\quad b\equiv e^{i\phi_2}\sin\frac{\gamma}{2}\\\vert a\vert^2+\vert c\vert^2=1 &\Rightarrow c\equiv -e^{i\phi_3}\sin\frac{\gamma}{2}\\\vert c\vert^2+\vert d\vert^2=1 &\Rightarrow d\equiv e^{i\phi_4}\cos\frac{\gamma}{2}\\a^*c+b^*d=0 &\Rightarrow -e^{i(\phi_3-\phi_1)}+e^{i(\phi_4-\phi_2)}=0\\&\Rightarrow\phi_3-\phi_1=\phi_4-\phi_2\equiv \beta\end{align*}</script><p>Another independent variables:</p><script type="math/tex; mode=display">\phi_1+\phi_3\equiv2\alpha-\delta,\quad \phi_2+\phi_4\equiv 2\alpha+\delta</script><script type="math/tex; mode=display">\Rightarrow U=\left[ \begin{matrix}e^{i(\alpha-\beta/2-\delta/2)}\cos\frac{\gamma}{2} & -e^{i(\alpha-\beta/2+\delta/2)}\sin\frac{\gamma}{2}\\e^{i(\alpha+\beta/2-\delta/2)}\sin\frac{\gamma}{2} & e^{i(\alpha+\beta/2+\delta/2)}\cos\frac{\gamma}{2}\end{matrix}\right]</script>
          </div>
<div class="note success">
            <p>Corollery 4.2: Suppose $U$ is a unitary gate on a single qubit. Then there exist unitary operators $A$, $B$, $C$ on a single qubit such that $ABC=I$ and $U=e^{i\alpha}AXBXC$, where $\alpha$ is some overall phase factor.</p><p><em>Proof</em>: For $Z$-$Y$ decomposition of $U$, $U=e^{i\alpha}R_z(\beta)R_y(\gamma)R_z(\delta)$. Let</p><script type="math/tex; mode=display">\begin{align*}A &\equiv R_z(\beta)R_y(\frac{\gamma}{2})\\B &\equiv R_y(-\frac{\gamma}{2})R_z(\frac{-(\delta+\beta)}{2})\\C &\equiv R_z(\frac{\delta-\beta}{2})\end{align*}</script><p>Check that</p><script type="math/tex; mode=display">\begin{align*}ABC &= R_z(\beta)R_y(\frac{\gamma}{2})R_y(-\frac{\gamma}{2})R_z(\frac{-(\delta+\beta)}{2})R_z(\frac{\delta-\beta}{2})\\    &= R_z(\beta)R_y(0)R_z(-\beta)\\    &= I\end{align*}</script><p>Let $f(x)=\sum_na_nx^n$, we show that $Xf(Y)X=f(-Y)$:</p><ol><li>$XY=-YX~\Rightarrow XYX=-Y$</li><li>$Xf(Y)X=X\sum_na_nY^nX=\sum_n a_n(XYX)^n=\sum_na_n(-Y)^n=f(-Y)$</li></ol><p>Similar for $Xf(Z)X=f(-Z)$. Then we have </p><script type="math/tex; mode=display">XBX=XR_y(-\frac{\gamma}{2})XXR_z(-\frac{\delta+\beta}{2})X=R_y(\frac{\gamma}{2})R_z(\frac{\delta+\beta}{2})</script><p>Hence,</p><script type="math/tex; mode=display">\begin{align*}e^{i\alpha} AXBXC &= e^{i\alpha}R_z(\beta)R_y(\frac{\gamma}{2})R_y(\frac{\gamma}{2})R_z(\frac{\delta+\beta}{2})R_z(\frac{\delta-\beta}{2})\\    &= e^{i\alpha}R_z(\beta)R_y(\gamma)R_z(\delta)\\    &= U\end{align*}</script>
          </div>
<h2 id="4-3-Controlled-operations" class="heading-control"><a href="#4-3-Controlled-operations" class="headerlink" title="4.3 Controlled operations"></a>4.3 Controlled operations<a class="heading-anchor" href="#4-3-Controlled-operations" aria-hidden="true"></a></h2><ul>
<li>controlled operation: If $A$ is true, then do $B$.</li>
<li>controlled-NOT (CNOT): If $c=1$, the target $\vert t\rangle$ flipped, $\vert c\rangle\vert t\rangle\to\vert c\rangle\vert t\oplus c\rangle$<script type="math/tex; mode=display">
\left[\begin{matrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 1 & 0
\end{matrix}\right]=
\left[\begin{matrix}
I & 0\\
0 & X 
\end{matrix}\right]</script><img src="https://i.imgur.com/MOT5gF0.png" width="300px"></li>
</ul>
<ul>
<li>controlled-$U$: If $c=1$, apply unitary $U$ to the target $\vert t\rangle$. $\vert c\rangle\vert t\rangle\to\vert c\rangle U^c\vert t\rangle$.<script type="math/tex; mode=display">
\left[\begin{matrix}
I & 0\\
0 & U
\end{matrix}\right]</script><img src="https://i.imgur.com/xs4UlXV.png" width="300px"></li>
</ul>
<ul>
<li>controlled-$Z$:<script type="math/tex; mode=display">
\left[\begin{matrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & -1
\end{matrix}\right]</script><img src="https://i.imgur.com/TFbXW5C.png" width="600px"></li>
</ul>
<ul>
<li>SWAP gate: $\vert a\rangle\vert b\rangle \to \vert b\rangle\vert a\rangle$<script type="math/tex; mode=display">
\left[\begin{matrix}
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 0 & 1
\end{matrix}\right]</script><img src="https://i.imgur.com/IjSI2rK.png" width="300px"></li>
</ul>
<ul>
<li>Toffoli gate (CCNOT):<ul>
<li>Universal for classical circuit (but not for quantum circuit)</li>
<li>reversible<br><img src="https://i.imgur.com/OensqGf.png" width="300px"></li>
</ul>
</li>
</ul>
<p>How to construct controlled-$U$ operation for arbitrary single qubit? By Corollary 4.2, $U=e^{i\alpha}AXBXC$:</p>
<ol>
<li><p>Apply phase shift $e^{i\alpha}$:</p>
<script type="math/tex; mode=display">
\vert 00\rangle \to \vert 00\rangle, \quad\vert 01\rangle\to\vert 01\rangle, \quad \vert 10\rangle\to e^{i\alpha}\vert 10\rangle, \quad \vert 11\rangle\to e^{i\alpha}\vert 11\rangle</script><p><img src="https://i.imgur.com/4aU6v9S.png" width="600px"></p>
</li>
<li><p>(CNOT$=$controlled-$X$)</p>
<ul>
<li>If $c=0$, $ABC=I$</li>
<li>If $c=1$, $e^{i\alpha}AXBXCX=U$<br><img src="https://i.imgur.com/5lqZXsq.png" width="600px"></li>
</ul>
</li>
</ol>
<p>Conditioning multiple qubits: Suppose we have $n+k$ qubits, and $U$ is a $k$ qubit unitary operator. Then we define the controlled operation $C^n(U)$ by the eqution</p>
<script type="math/tex; mode=display">
C^n(U)\vert x_1x_2\dots x_n\rangle\vert \psi\rangle=\vert x_1x_2\dots x_n\rangle U^{x_1x_2\dots x_n}\vert\psi\rangle</script><p><img src="https://i.imgur.com/UnxIZxq.png" alt></p>
<h2 id="4-4-Measurement" class="heading-control"><a href="#4-4-Measurement" class="headerlink" title="4.4 Measurement"></a>4.4 Measurement<a class="heading-anchor" href="#4-4-Measurement" aria-hidden="true"></a></h2><ul>
<li>Projective measurement in the computational basis<ul>
<li>Any measurement can be constructed from adding ancilla qubits, performing a unitary, and followed by computational basis measurement.</li>
</ul>
</li>
</ul>
<div class="note success">
            <p><strong>Principle 1: Principle of deferred measurement</strong><br>We can assume all measurements happens at the end of the circuit because you can always move a measurements in the middle of the circuit to the end, and replace classical controls of the measurement outcome with quantum control:<br><img src="https://i.imgur.com/fJruwPW.png" alt></p>
          </div>
<div class="note success">
            <p><strong>Principle 2: Principle of implicit measurement</strong><br>We can assume that every quantum wire is measured in the end, because measuring “leftover” wires does not change thet reduced density matrix of the wires that give result.<br><img src="https://i.imgur.com/GG60pDC.png" alt></p>
          </div>
<div class="note info">
            <p><em>Exercise 4.33</em>: (<strong>Measurement in the Bell basis</strong>) Show that<br><img src="https://i.imgur.com/fwOGrrK.png" width="200px"><br>Performs a measurement in the bell basis.</p><script type="math/tex; mode=display">\begin{align*}(H\otimes I)CNOT &= (H\otimes I)(\vert 0\rangle\langle 0\vert \otimes I + \vert 1\rangle\langle 1 \vert\otimes X)\\    &= (\vert +\rangle\langle 0\vert \otimes I + \vert -\rangle\langle 1\vert X)\\    &= \frac{1}{\sqrt2}(\vert 0\rangle\langle 0\vert\otimes I + \vert 1\rangle\langle 0\vert\otimes I + \vert 0\rangle\langle 1\vert\otimes X - \vert 1\rangle\langle 1\vert\otimes X)\\    &= \frac{1}{\sqrt2}(\vert00\rangle(\langle00\vert+\langle11\vert)+\vert10\rangle(\langle00\vert-\langle11\vert)\\    &    \quad\quad\quad\quad+\vert01\rangle(\langle01\vert+\langle10\vert)+\vert11\rangle(\langle01\vert-\langle10\vert))\end{align*}</script><p>Projective measurements:</p><script type="math/tex; mode=display">\begin{align*}\vert 00\rangle\langle00\vert &\Rightarrow ((H\otimes I)CNOT)^\dagger \vert 00\rangle\langle00\vert (H\otimes I)CNOT \\    &= \frac{1}{2}(\vert00\rangle + \vert11\rangle)(\langle00\vert+\langle11\vert)\\    &= \vert\beta_{00}\rangle\langle\beta_{00}\vert\end{align*}</script><script type="math/tex; mode=display">\begin{align*}\vert 01\rangle\langle01\vert &\Rightarrow ((H\otimes I)CNOT)^\dagger \vert 01\rangle\langle01\vert (H\otimes I)CNOT \\    &= \frac{1}{2}(\vert01\rangle + \vert10\rangle)(\langle01\vert+\langle10\vert)\\    &= \vert\beta_{10}\rangle\langle\beta_{10}\vert\end{align*}</script><script type="math/tex; mode=display">\begin{align*}\vert 10\rangle\langle10\vert &\Rightarrow ((H\otimes I)CNOT)^\dagger \vert 10\rangle\langle10\vert (H\otimes I)CNOT \\    &= \frac{1}{2}(\vert00\rangle - \vert11\rangle)(\langle00\vert-\langle11\vert)\\    &= \vert\beta_{01}\rangle\langle\beta_{01}\vert\end{align*}</script><script type="math/tex; mode=display">\begin{align*}\vert 11\rangle\langle11\vert &\Rightarrow ((H\otimes I)CNOT)^\dagger \vert 11\rangle\langle11\vert (H\otimes I)CNOT \\    &= \frac{1}{2}(\vert01\rangle - \vert10\rangle)(\langle01\vert-\langle10\vert)\\    &= \vert\beta_{11}\rangle\langle\beta_{11}\vert\end{align*}</script>
          </div>
<h2 id="4-5-Universal-quantum-gates" class="heading-control"><a href="#4-5-Universal-quantum-gates" class="headerlink" title="4.5 Universal quantum gates"></a>4.5 Universal quantum gates<a class="heading-anchor" href="#4-5-Universal-quantum-gates" aria-hidden="true"></a></h2><p>A set of gates is said to be <em>universal for quantum computation</em> if any unitary operation may be <strong>approximated</strong> to <strong>arbitary accuracy</strong> by a quantum circuit involving only those gates.</p>
<p>Three step constructions:</p>
<ol>
<li>(exact) $\{\text{2-level unitaries}\}$ universal: an arbitrary unitary may be expressed as a product of unitary operators that each acts non-trivially only on a subspace spanned by two computational basis states.</li>
<li>(exact) $\{\text{all single qubit } U, CNOT\}$ universal: an arbitrary unitary from the first step may be expressed using single qubit and <code>CNOT</code> gates.</li>
<li>(approx) $\{H, T, CNOT\}$: single qubit operation from the second step may be approximated to arbitray accuracy using the Hadamard, phase, and $\pi/8$ gates ($T$). This implies that any unitary operation can be approximated to arbitrary accuracy using Hadamard, phase, <code>CNOT</code>, and $\pi/8$ gates.</li>
</ol>
<h3 id="4-5-1-Two-level-unitary-gates-are-universal" class="heading-control"><a href="#4-5-1-Two-level-unitary-gates-are-universal" class="headerlink" title="4.5.1 Two-level unitary gates are universal"></a>4.5.1 Two-level unitary gates are universal<a class="heading-anchor" href="#4-5-1-Two-level-unitary-gates-are-universal" aria-hidden="true"></a></h3><ul>
<li>Two-level unitary: unitary that only acts non-trivially on two computational basis.<script type="math/tex; mode=display">
\left[\begin{matrix}
\begin{matrix} a & c \\ b & d \end{matrix} & 0\\
0 & I
\end{matrix}\right]
\quad ~\text{ or}~ \quad
\left[\begin{matrix}
\ddots&&&&0\\
&a&&c&\\
&&\ddots&&\\
&b&&d&\\
0&&&&\ddots
\end{matrix}\right]</script></li>
</ul>
<div class="note success">
            <p>Any unitary $U$ can be decomposed into two-level unitaries</p><p><em>Example</em>: $U\in C^{3\times 3}$, $U=\left[\begin{matrix} a & d & g\\ b & e & h\\ c & f & j\end{matrix}\right]$, find two-level unitaries $U_1, U_2, U_3$ s.t. $U_3U_2U_1U=I\Rightarrow U=U_1^\dagger U_2^\dagger U_3^\dagger$.</p><p>Find $U_1$: </p><ul><li>If $b=0$, $U_1=I$, </li><li>If $b\ne0$, $U_1=\left[\begin{matrix}\frac{a^*}{\sqrt{\vert a\vert^2+\vert b\vert^2}} & \frac{b^*}{\sqrt{\vert a\vert^2+\vert b\vert^2}} & 0\\\frac{b}{\sqrt{\vert a\vert^2+\vert b\vert^2}} & \frac{-a}{\sqrt{\vert a\vert^2+\vert b\vert^2}} & 0\\0&0&I\end{matrix}\right]$</li></ul><p>$\Rightarrow U_1U=\left[\begin{matrix}a’&d’&g’\\0&e’&h’\\c’&f’&j’\end{matrix}\right]$</p><p>Find $U_2$:</p><ul><li>If $c’=0$, $U_2=\left[\begin{matrix}a’^\dagger & 0&0\\0&1&0\\0&0&1\end{matrix}\right]$</li><li>If $c’\ne 0$, $U_2=\left[\begin{matrix} \frac{a’^*}{\sqrt{\vert a’\vert^2+\vert c’\vert^2}}&0&\frac{c’^*}{\sqrt{\vert a’\vert^2+\vert c’\vert^2}} \\ 0&1&0\\ \frac{c’}{\sqrt{\vert a’\vert^2+\vert c’\vert^2}}&0&\frac{-a’}{\sqrt{\vert a’\vert^2+\vert c’\vert^2}}\end{matrix}\right]$</li></ul><p>$\Rightarrow U_2U_1U=\left[\begin{matrix}1&d’’&g’’\\0&e’’&h’’\\0&f’’&j’’\end{matrix}\right]=\left[\begin{matrix}1&0&0\\0&e’’&h’’\\0&f’’&j’’\end{matrix}\right]$, because $U_2U_1U$ is a unitary.</p><p>Find $U_3$:<br>$U_3=\left[\begin{matrix}1&0&0\\0&e’’^\dagger&f’’^\dagger\\0&h’’^\dagger&j’’^\dagger\end{matrix}\right]$</p><p>$\Rightarrow U_3U_2U_1U=I$<br>\<br><em>Proof</em>. For general case, prove by induction.</p><ul><li>We can decompose $2\times 2$ U</li><li>If we can decompose $(d-1)\times(d-1)$ unitaries, then given any $U\in C^{d\times d}$, we can find $U_1, \dots, U_{d-1}$ s.t. $U_{d-1}U_{d-2}\dots U_1U=\left[\begin{matrix}1&\begin{matrix}0&0&\cdots&0\end{matrix}\\\begin{matrix}0\\0\\\vdots\\0\end{matrix} & U’\end{matrix}\right]$. By assumption, we can decompose $U’$ because it is a $(d-1)\times(d-1)$ unitary $\Rightarrow$ We can decompose any $U\in C^{d\times d}$ i.e. $U=U_1U_2\cdots U_k$ where $\{U_i\}$ are two level unitaries (and $k\le \frac{d(d-1)}{2}$)</li></ul>
          </div>
<div class="note success">
            <p><strong>Corollary</strong>: Unitaries on $n$-qubit can be decomposed as $2^{n-1}(2^n-1)$ two-level unitaries.</p>
          </div>
<h3 id="4-5-2-Single-qubit-and-CNOT-gates-are-universal" class="heading-control"><a href="#4-5-2-Single-qubit-and-CNOT-gates-are-universal" class="headerlink" title="4.5.2 Single qubit and CNOT gates are universal"></a>4.5.2 Single qubit and <code>CNOT</code> gates are universal<a class="heading-anchor" href="#4-5-2-Single-qubit-and-CNOT-gates-are-universal" aria-hidden="true"></a></h3><p>Recall any unitary $U$ can be decomposed into two-level unitaries. Now we decompose two-level unitaries into multi-controlled unitaries where the target is a single qubit.</p>
<ol>
<li>Suppose unitary $U$ acts non-trivially on the two computational basis $\vert s\rangle=\vert s_1s_2\dots s_n\rangle$ and $\vert t\rangle=\vert t_1t_2\dots t_n\rangle$. Let $\tilde{U}$ be the non-trivial $2\times 2$ unitary submatrix of $U$:<script type="math/tex; mode=display">
U=
\left[\begin{matrix}
\ddots&&&&0\\
&\tilde{U}_{11}&&\tilde{U}_{12}&\\
&&\ddots&&\\
&\tilde{U}_{21}&&\tilde{U}_{22}&\\
0&&&&\ddots
\end{matrix}\right]</script></li>
<li>Use <em>Gray code</em> to connect two computational bases $\vert s\rangle$ and $\vert t\rangle$ by flipping one bit at a time. For instance, $s=101001$ and $t=110011$ we have the Gray code:<script type="math/tex; mode=display">
\begin{align*}
s=g_1&=101001\\
g_2&=101011\\
g_3&=100011\\
t=g_4&=110011
\end{align*}</script></li>
<li>Let $g_1$ through $g_m$ be the element of a Gray code connecting $g_1=s$ and $g_m=t$. The goal is to design a circuit $F$ to change the basis from $\vert g_1\rangle \to \vert g_2\rangle \to \cdots \to\vert g_{m-1}\rangle$, then perform controlled unitary $\tilde U$ on $\vert g_{m-1}\rangle$ and $vert g_m\rangle$, with the target qubit located at the single qubit where $\vert g_{m-1}\rangle$ and $\vert g_{m}\rangle$ differ, and then undo the transformation $\vert g_{m-1}\rangle \to \vert g_{m-2}\rangle \to\cdots\to \vert g_1\rangle$ by reversing the circuit $F$, let’s say $\tilde F$.<ul>
<li>Suppose $\vert g_1 \rangle$ and $\vert g_2\rangle$ differs at $i$-th bit, do a $C^{(n-1)}-X$ on the $i$-th bit, i.e. swap $\vert g_1 \rangle$ and $\vert g_2 \rangle$.</li>
<li>Next, swap $\vert g_2 \rangle$ and $\vert g_3 \rangle$ with another Controlled-$X$.</li>
<li>Continue this fashion until we swap $\vert g_{m-2} \rangle$ and $\vert g_{m-1} \rangle$.</li>
<li>Result in the permutation (Other basis will remain the same after applying $F$):<script type="math/tex; mode=display">
\begin{align*}
\vert g_1\rangle &\xrightarrow{F} \vert g_{m-1}\rangle\\
\vert g_2\rangle &\xrightarrow{F} \vert g_1\rangle\\
\vert g_3\rangle &\xrightarrow{F} \vert g_2\rangle\\
&\vdots\\
\vert g_{m-1}\rangle &\xrightarrow{F} \vert g_{m-2}\rangle\\
\end{align*}</script></li>
</ul>
</li>
</ol>
<p>Example: Implement the two-level unitary transformation</p>
<script type="math/tex; mode=display">
U=\left[\begin{matrix} a&0&\cdots&0&c \\ 0&1&&0&0 \\ \vdots&&\ddots&&\vdots \\0&0&&1&0\\b&0&\cdots&0&d\end{matrix}\right], \quad \tilde U\equiv\left[\begin{matrix}a&c\\b&d\end{matrix}\right]</script><p>which acts non-trivially only on the states $\vert 000\rangle$ and $\vert 111\rangle$. Write down the Gray code:</p>
<script type="math/tex; mode=display">
\begin{matrix}
& A & B & C \\
g_1&0&0&0\\
g_2&0&0&1\\
g_3&0&1&1\\
g_4&1&1&1
\end{matrix}</script><p>Construct the circuit:<br><img src="https://i.imgur.com/9OIt2Sw.png" alt></p>
<p>$\Rightarrow$ we can decompose any two-level unitaries into multi-controlled single qubit unitaries.</p>
<p>$\Rightarrow$ $\{\text{single qubit } U, CNOT\}$ universal.</p>
<p>Complexity:</p>
<ul>
<li>Each $C^{n-1}-\tilde U$: $O(n)$ gates</li>
<li>Each two-level $U$: $O(n^2)$ gates</li>
<li>Each unitary: $O((2^n)^2)=O(2^{2n})$ two-level unitaries $=O(n^22^{2n})$ gates.</li>
</ul>
<h3 id="4-5-3-A-discrete-set-of-universal-operations" class="heading-control"><a href="#4-5-3-A-discrete-set-of-universal-operations" class="headerlink" title="4.5.3 A discrete set of universal operations"></a>4.5.3 A discrete set of universal operations<a class="heading-anchor" href="#4-5-3-A-discrete-set-of-universal-operations" aria-hidden="true"></a></h3><p>$\{H, T\}$ approximate single qubit unitaries.</p>
<ul>
<li>Solovay-Kitaev theorem<ul>
<li>$O(m\log^c(m/\varepsilon))$ gates</li>
</ul>
</li>
<li>$\Omega(1/\varepsilon)$ gates</li>
</ul>
<p>Error:</p>
<ul>
<li>Distance on matrices: $|U-V| = \max_{\vert \psi\rangle,\langle\psi\vert\psi\rangle=1}|(U-V)\vert\psi\rangle|$</li>
<li>unitary invariant:<ul>
<li>$|AU|=|A|$, $|UA|=|A|$</li>
</ul>
</li>
<li>triangle inequality:<ul>
<li>$|A-C| \le |A-B|+|B-C|$, where $|A|\ge0$, $|A|=0, ~\text{iff}~A=0$ i.e. $|U-V| ~\text{iff}~U=V$</li>
</ul>
</li>
</ul>
<blockquote>
<p>待補</p>
</blockquote>
<h2 id="4-6-Summary-of-the-quantum-circuit-model-of-computation" class="heading-control"><a href="#4-6-Summary-of-the-quantum-circuit-model-of-computation" class="headerlink" title="4.6 Summary of the quantum circuit model of computation"></a>4.6 Summary of the quantum circuit model of computation<a class="heading-anchor" href="#4-6-Summary-of-the-quantum-circuit-model-of-computation" aria-hidden="true"></a></h2><blockquote>
<p>待補</p>
</blockquote>
<h2 id="4-7-Simulation-of-quantum-systems" class="heading-control"><a href="#4-7-Simulation-of-quantum-systems" class="headerlink" title="4.7 Simulation of quantum systems"></a>4.7 Simulation of quantum systems<a class="heading-anchor" href="#4-7-Simulation-of-quantum-systems" aria-hidden="true"></a></h2><blockquote>
<p>待補</p>
</blockquote>
<h2 id="Bonus" class="heading-control"><a href="#Bonus" class="headerlink" title="Bonus"></a>Bonus<a class="heading-anchor" href="#Bonus" aria-hidden="true"></a></h2><h3 id="Quantum-oracle" class="heading-control"><a href="#Quantum-oracle" class="headerlink" title="Quantum oracle"></a>Quantum oracle<a class="heading-anchor" href="#Quantum-oracle" aria-hidden="true"></a></h3><blockquote>
<p>Please refer to Section 6.1.1 The oracle</p>
</blockquote>
<p>Oracle: Blackbox for $f(x)$</p>
<p>Classical oracle: used in a classical circuit<br>Quantum oracle: quantum superposition</p>
<ul>
<li>Example: $U_f(\vert x_1\rangle\vert y_1\rangle + \vert x_2\rangle \vert y_2\rangle)=\vert x_1\rangle \vert y_1\oplus f(x_1)\rangle + \vert x_2\rangle\vert y_2\oplus f(x_2)\rangle$</li>
</ul>
<p><img src="https://i.imgur.com/aV2BnxT.png" alt></p>
<ul>
<li>Classical query complexity is also a big research area</li>
<li>Gap between quantum query complexity v.s. classical query complexity?</li>
</ul>
<h3 id="Deutsch’s-algorithm" class="heading-control"><a href="#Deutsch’s-algorithm" class="headerlink" title="Deutsch’s algorithm"></a>Deutsch’s algorithm<a class="heading-anchor" href="#Deutsch’s-algorithm" aria-hidden="true"></a></h3><blockquote>
<p>Please refer to Section 1.4.3 Deutsch’s algorithm</p>
</blockquote>
<p><strong>Problem</strong>: Compute $f(0)+f(1)$ for some $f:\{0, 1\}\to\{0, 1\}$ (with $f$ given as a blackbox)</p>
<p>Classical: need two quries, find $f(0)$ and $f(1)$, then compute $f(0)+f(1)$</p>
<p>Quantum: Only one query:</p>
<p><img src="https://i.imgur.com/DLBZoOn.png" alt></p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi_1\rangle &= (H\vert 0\rangle) \otimes (HX\vert 0\rangle)\\
    &= \vert+\rangle \otimes \vert -\rangle\\
    &= \frac{1}{\sqrt 2}\left(\vert 0\rangle \otimes \vert -\rangle + \vert 1\rangle \otimes \vert -\rangle\right)\\
\end{align*}</script><p>Note that:</p>
<script type="math/tex; mode=display">
\begin{align*}
U_f \vert x\rangle \otimes \vert -\rangle &= \vert x\rangle\otimes \frac{1}{\sqrt 2}(\vert 0\oplus f(x)\rangle-\vert 1\oplus f(x)\rangle)\\
    &= \vert x\rangle \otimes \begin{cases}
        \vert -\rangle, &f(x)=0\\
        -\vert-\rangle, &f(x)=1
        \end{cases}\\
    &= (-1)^{f(x)}\vert x\rangle \otimes \vert -\rangle\\
\end{align*}</script><p>Then, </p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi_2\rangle &= U_f\frac{1}{\sqrt 2}(\vert 0\rangle\otimes \vert-\rangle + \vert1\rangle\otimes\vert-\rangle)\\
    &= \frac{1}{\sqrt 2}\left( (-1)^{f(0)}\vert 0\rangle\otimes\vert -\rangle + (-1)^{f(1)}\vert 1\rangle\otimes \vert-\rangle\right)\\
    &=\frac{1}{\sqrt 2} \underbrace{(-1)^{f(0)}}_{\text{global phase}}\left(\vert 0\rangle\otimes\vert-\rangle+\overbrace{(-1)^{f(1)-f(0)}}^{\text{relative phase}}\vert 1\rangle\otimes\vert-\rangle\right)\\
    &= (-1)^{f(0)}\left( \begin{cases}
            \vert +\rangle, &\text{if }f(0)=f(1)~\\
            \vert -\rangle, &\text{if }f(0)\ne f(1)~
        \end{cases} \right)\otimes \vert -\rangle\\
        ~\\
\vert \psi_3\rangle &= (-1)^{f(0)}\left( \begin{cases}
            \vert 0\rangle, &\text{if }f(0)=f(1)~\\
            \vert 1\rangle, &\text{if }f(0)\ne f(1)~
        \end{cases}\right)\otimes \vert -\rangle
\end{align*}</script><p>Measurement:</p>
<script type="math/tex; mode=display">
\begin{cases}
0, &\text{if } f(0)=f(1)\\
1, &\text{if } f(0)\ne f(1)
\end{cases}</script><h3 id="Deutsch－Jozsa-algorithm" class="heading-control"><a href="#Deutsch－Jozsa-algorithm" class="headerlink" title="Deutsch－Jozsa algorithm"></a>Deutsch－Jozsa algorithm<a class="heading-anchor" href="#Deutsch－Jozsa-algorithm" aria-hidden="true"></a></h3><blockquote>
<p>Please refer to Section 1.4.4 The Deutsch-Jozsa algorithm</p>
</blockquote>
<p><strong>Problem</strong>: Given an black box quantum computer known as an oracle that implements some function $f:\{0,1\}^n\to\{0,1\}$. We are probmised that the function is either constant (0 on all outputs or 1 on all outputs, $f(x)=0, \forall x$, or $f(x)=1, \forall x$) or balanced (returns 1 for half of the input domain and 0 for the other half). The task then is to determine if $f$ is constant or balanced by using the oracle.</p>
<p>Classical query complexity depends on details:</p>
<ul>
<li>If we want to solve with no error, need $2^n/2+1$ queries. Even if $f(x)$ is balanced, the first $2^n/2$ queries might be all zero.</li>
<li>Using randomized algorithm only want constant (say less than $1/3$) error $\Rightarrow$ only need constant queries. Low probability that random queries or balanced $f$ all match.</li>
</ul>
<p>Quantum query, perfect with $1$ query:</p>
<p><img src="https://i.imgur.com/VXXcUdM.png" alt></p>
<p>Note that:</p>
<script type="math/tex; mode=display">
\begin{align*}
H^{\otimes n} (\vert 0\rangle)^n&=\vert +\rangle^{\otimes n}\\
    &= \frac{1}{\sqrt {2^n}}(\vert 0\rangle+\vert 1\rangle)\otimes(\vert 0\rangle+\vert 1\rangle)\otimes\cdots\otimes(\vert 0\rangle+\vert 1\rangle)\\
    &= \frac{1}{\sqrt {2^n}}(\vert00\dots0\rangle+\cdots+\vert11\dots1\rangle)\\
    &= \frac{1}{\sqrt {2^n}}\sum_{x\in\{0,1\}^n}\vert x\rangle
\end{align*}</script><p>Then,</p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi_1\rangle 
    &= (H^{\otimes n}\otimes HX)\vert 0\rangle^{\otimes n}\otimes \vert 0\rangle\\
    &= \sum_{x\in\{0,1\}^n}\frac{1}{\sqrt {2^n}}\vert x\rangle \otimes \vert -\rangle
\end{align*}</script><p>Recall from Deutsch’s algorithm $U_f \vert x\rangle \otimes \vert -\rangle = (-1)^{f(x)}\vert x\rangle \otimes \vert -\rangle$,</p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi_2\rangle &= U_f(\sum_{x\in\{0,1\}^n}\frac{1}{\sqrt {2^n}}\vert x\rangle \otimes \vert -\rangle)\\
    &= \sum_{x\in\{0,1\}^n}\frac{(-1)^{f(x)}}{\sqrt {2^n}}\vert x\rangle \otimes \vert -\rangle
\end{align*}</script><p>How about $H^{\otimes n}\vert x\rangle$? On one qubit ($n=1$):</p>
<script type="math/tex; mode=display">
\begin{align*}
&\begin{cases}
H\vert 0 \rangle = \frac{1}{\sqrt 2}(\vert 0\rangle+\vert 1\rangle)\\
H\vert 1 \rangle = \frac{1}{\sqrt 2}(\vert 0\rangle-\vert 1\rangle)
\end{cases}\\
\Rightarrow& ~H\vert x \rangle = \frac{1}{\sqrt 2}\sum_{z\in\{0,1\}}(-1)^{xz}\vert z\rangle
\end{align*}</script><p>On multiple qubits:</p>
<script type="math/tex; mode=display">
\begin{align*}
H^{\otimes n}\vert x_1x_2\dots x_n\rangle
    &=\sum_{z_1,z_2,\dots,z_n\in\{0,1\}}\frac{(-1)^{x_1z_1+x_2z_2+\dots+x_nz_n}}{\sqrt {2^n}}\vert z_1z_2\dots z_n\rangle\\
    &= \sum_{z\in\{0,1\}^n}\frac{(-1)^{x\cdot z}}{\sqrt{2^n}}\vert z\rangle
\end{align*}</script><p>Then,</p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi_3\rangle &= (H^{\otimes n}\otimes I)\vert \psi_2\rangle\\
    &=(H^{\otimes n}\otimes I)\sum_{x\in\{0,1\}^n} \frac{(-1)^{f(x)}}{\sqrt{2^n}}\vert x\rangle \otimes \vert -\rangle\\
    &= \sum_{z\in\{0,1\}^n}\left( \sum_{x\in\{0,1\}^n} \frac{(-1)^{x\cdot z+f(x)}}{2^n} \right)\vert z\rangle \otimes \vert -\rangle 
\end{align*}</script><p>Amplitude for $z=0^n$ on first register $x\cdot z=0 \Rightarrow \text{amp}=\sum_{x\in\{0,1\}^n}\frac{(-1)^{f(x)}}{2^n}$. </p>
<script type="math/tex; mode=display">
\begin{cases}
\text{amp}=1 & \text{if $f(x)$ is constant, $f(x)=0, \forall x$}\\
\text{amp}=-1 & \text{if $f(x)$ is constant, $f(x)=1, \forall x$}\\
\text{amp}=0 & \text{if $f(x)$ is balanced, $\Big\vert \{x\vert f(x)=0, \forall x\}\Big\vert=\Big\vert \{x\vert f(x)=1, \forall x\}\Big\vert$}
\end{cases}</script><p>$\vert\text{amp}\vert^2=\text{Probability}$</p>
<p>Measurement:</p>
<script type="math/tex; mode=display">
\begin{cases}
z=0^n, &\text{if $f(x)$ is constant}\\
z\ne0^n, &\text{if $f(x)$ is balanced}
\end{cases}</script><h3 id="Bernstain－Vazirani-algorithm" class="heading-control"><a href="#Bernstain－Vazirani-algorithm" class="headerlink" title="Bernstain－Vazirani algorithm"></a>Bernstain－Vazirani algorithm<a class="heading-anchor" href="#Bernstain－Vazirani-algorithm" aria-hidden="true"></a></h3><blockquote>
<p>Not on book</p>
</blockquote>
<p><strong>Problem</strong>: Given an oracle that implements a function $f:\{0,1\}^n\to\{0,1\}$ in which $f(x)$ is promised to be the dot product between $x$ and a secret string $s\in\{0,1\}^n$ module $2$, $f(x)=x\cdot s=x_1\cdot s_1+x_2\cdot s_2+\cdots +x_n\cdot s_n\mod 2$, find $s$.</p>
<p>Classical: need $n$ queries: $s=s_1s_2\dots s_n$</p>
<ul>
<li>Get $s_1$ by query $x=100\dots0$</li>
<li>Get $s_2$ by query $x=010\dots0$</li>
<li>… learn $s$ in $n$ queries</li>
<li>Claim: Can’t do better even with random algorithm because each query only gives one bit of information on $s$.</li>
</ul>
<p>Quantum: $1$ query. The algorithm is almost the same as Deutsch－Jozsn algorithm.</p>
<p><img src="https://i.imgur.com/6ple5iU.png" alt></p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi_3\rangle &= \sum_{z\in\{0,1\}^n}\sum_{x\in\{0,1\}^n} \frac{(-1)^{z\cdot x + f(x)}}{2^n} \vert z\rangle\\
    &= \sum_{z\in\{0,1\}^n}\sum_{x\in\{0,1\}^n} \frac{(-1)^{z\cdot x + s\cdot x}}{2^n} \vert z\rangle\\
    &= \sum_{z\in\{0,1\}^n}\sum_{x\in\{0,1\}^n} \frac{(-1)^{(z\oplus s)\cdot x}}{2^n} \vert z\rangle\\
\end{align*}</script><p>Claim that $\sum_{x\in\{0,1\}^n} \frac{1}{2^n}(-1)^{(z\oplus s)\cdot x} \vert z\rangle=\delta_{z\cdot s}$.</p>
<p>Proof:</p>
<ul>
<li>When $z=s$, $z\oplus s=0\Rightarrow\sum_{x\in\{0,1\}^n} \frac{1}{2^n}(-1)^{(z\oplus s)\cdot x}=\sum_{x\in\{0,1\}^n}\frac{1}{2^n}=1$.</li>
<li>When $z\ne s$, $\vert \psi_3\rangle$ normalized $\Rightarrow$ all $z\ne s$ terms $=0$ (can also check cancellation)</li>
</ul>
<script type="math/tex; mode=display">
\Rightarrow \vert\psi_3\rangle=\sum_{z\in\{0,1\}^n}\delta_{z\cdot s}\vert z\rangle=\vert s\rangle</script><p>Measurement result is $s$.</p>
<div class="note info">
            <p>Let $a=z\oplus s$, prove that if $a\ne 0^n$ (i.e. $z\ne s$), </p><script type="math/tex; mode=display">\sum_{x\in\{0,1\}^n} (-1)^{a\cdot x} = 0.</script><p><em>Proof</em>: Let $a_i$ and $x_i$ denote the $i^\text{th}$ element of $a$ and $x$, respectively. Suppose there are $k>0$ bits in $a$ are $1$, then we can separate $a$ and $x$ into $\bar{a}=(a_i|a_i=0,\forall i)$, $\hat{a}=(a_i|a_i=1,\forall i)$, $\bar{x}=(x_i|a_i=0,\forall i)$ and $\hat{x}=(x_i|a_i=1,\forall i)$. Since $\bar{a}=0^{n-k}$ and $\hat{a}=1^k$, </p><script type="math/tex; mode=display">\sum_{x\in\{0,1\}^n}(-1)^{a\cdot x}=\sum_{\bar{x}\in\{0, 1\}^{n-k}}\sum_{\hat{x}\in\{0,1\}^k}(-1)^{\bar{a}\cdot\bar{x}+\hat{a}\cdot\hat{x}}=2^{n-k}\sum_{\hat{x}\in\{0,1\}^k}(-1)^{\hat{a}\cdot\hat{x}}</script><p>If the number of $1$-bits in $\hat{x}$ is odd, $(-1)^{\hat{a}\cdot\hat{x}}=-1$. Otherwise, $(-1)^{\hat{a}\cdot\hat{x}}=1$ if it is even. Thus, we can prove $\sum_{\hat{x}\in\{0,1\}^k}(-1)^{\hat{a}\cdot\hat{x}}=0$ if we show the number of $\hat{x}$ in which the number of $1$-bits is odd equals to that of the number of $1$-bits is even. We prove by induction:</p><ol><li><p><strong>Hypothesis</strong>: for $k>0$, $\text{Odd}(k)=\text{Even}(k)$, where $\text{Odd}(k)$ denotes the number of $\hat{x}\in\{0,1\}^k$ in which the number of $1$ is odd, $\text{Even}(k)$ denotes the number of $\hat{x}\in\{0,1\}^k$ in which the number of $1$ is even.</p></li><li><p><strong>Base case</strong>: for $k=1$, the outcomes of $\hat{x}$ are $\{0, 1\}$ $\Rightarrow \text{Odd}(1)=\text{Even}(1)=1$.</p></li><li><p><strong>Induction step</strong>: suppose for $k>1$, $\text{Odd}(k)=\text{Even}(k)$ is true. Then for $k+1$, $\text{Odd}(k+1)=\text{Odd}(k)+\text{Even}(k)$ as we add a $0$ for odd numbers they still remain odd, and add a $1$ for even numbers they become odd. Similar for $\text{Even}(k+1)=\text{Even}(k)+\text{Odd}(k)$. Thus, $\text{Odd}(k+1)=\text{Even}(k+1)$, the statement holds.</p></li></ol><p>Hence, $\sum_{\hat{x}\in\{0,1\}^k}(-1)^{\hat{a}\cdot\hat{x}}=0, \forall k>0 \Longrightarrow \sum_{x\in\{0,1\}^n}(-1)^{a\cdot x}=0$, if $a\ne 0^n$.</p>
          </div>
<h3 id="Simon’s-problem" class="heading-control"><a href="#Simon’s-problem" class="headerlink" title="Simon’s problem"></a>Simon’s problem<a class="heading-anchor" href="#Simon’s-problem" aria-hidden="true"></a></h3><blockquote>
<p>Please refer to Section 5.4.3 The hidden subgroup problem</p>
</blockquote>
<p><strong>Problem</strong>: Given an oracle $f:\{0,1\}^n\to\{0,1\}^n$, promised to satisfy the property that, for some $s\in\{0,1\}^n, s\ne 0^n$ and all $x,y\in\{0, 1\}^n$, $f(x)=f(x\oplus s)$. Find $s$.</p>
<p>Example: if $n=3$, the following is the function $f(x)$:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>$x$</th>
<th>000</th>
<th>001</th>
<th>010</th>
<th>011</th>
<th>100</th>
<th>101</th>
<th>110</th>
<th>111</th>
</tr>
</thead>
<tbody>
<tr>
<td>$f(x)$</td>
<td>101</td>
<td>010</td>
<td>000</td>
<td>110</td>
<td>000</td>
<td>110</td>
<td>101</td>
<td>010</td>
</tr>
</tbody>
</table>
</div>
<p>The answer is $s=110$.</p>
<p>Classical: How many random classical queries needed to find a “collision”? $2^{n-1}$ possible outputs. $\sim\frac{1}{2^{n-1}}$ probability a pair is a collision.</p>
<ul>
<li>$O(2^n)$ probabilities $\Rightarrow$ $O(\sqrt{2^n})=O(2^{\frac{n}{2}})$ queries to find a collision with constant probability.</li>
<li>Lowerbound also $\Omega(2^\frac{n}{2})$</li>
<li>$\Rightarrow$ classical complexity $\Theta(2^\frac{n}{2})$</li>
</ul>
<p>Quantum complexity: $O(n)$ queries</p>
<p><img src="https://i.imgur.com/KuPGt93.png" alt></p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi_1\rangle &= \frac{1}{\sqrt{2^n}}\sum_{x\in\{0,1\}^n}\vert x\rangle \otimes \vert 0\rangle^{\otimes n}\\
\vert \psi_2\rangle &= \frac{1}{\sqrt{2^n}}\sum_{x\in\{0,1\}^n}\vert x\rangle \otimes \vert f(x)\rangle
\end{align*}</script><p>On measuring second register: uniformly random $f(x)$ collapse the first register into corresponding preimages:</p>
<script type="math/tex; mode=display">
\vert \psi_3\rangle = \frac{1}{\sqrt 2}(\vert x\rangle + \vert x\oplus s\rangle)</script><script type="math/tex; mode=display">
\vert \psi_4\rangle = \frac{1}{\sqrt 2}\frac{1}{\sqrt{2^n}}\sum_{z\in\{0,1\}^n}\left( (-1)^{x\cdot z}+(-1)^{(x\oplus s)\cdot z} \right)\vert z\rangle</script><p>Measurement:</p>
<ol>
<li>non-zero amplitude $\Rightarrow$ $x\cdot z=(x\oplus s)\cdot z \Rightarrow s\cdot z=0$</li>
<li>Will get a “uniformly” random $z$ such that $s\cdot z=0$ (there are $2^{n-1}$ such z)</li>
<li>Do this $k$ times, get $z_1, z_2, \dots, z_k$.<br>(informal) each $z_i$ gives one bit of information on $s$, need $O(n)$ $z_i$ to solve:<script type="math/tex; mode=display">
\begin{eqnarray*}
s\cdot &z_1=0 &~~\Rightarrow~ &s_1z_{11}+s_2z_{12}+\cdots+s_nz_{1n}&=0\\
s\cdot &z_2=0 &~~\Rightarrow~ &s_1z_{21}+s_2z_{22}+\cdots+s_nz_{2n}&=0\\
&\vdots\\
s\cdot &z_k=0 &~~\Rightarrow~ &s_1z_{n1}+s_2z_{n2}+\cdots+s_nz_{kn}&=0
\end{eqnarray*}</script></li>
<li>$k$ linear equations on $(s_1,s_2,\dots,s_n)$. $n$ Linearly independent equations $\Rightarrow$ solve $s$</li>
<li>Good probability to determine $s$ with $k=O(n)$ random equations.</li>
</ol>
<p>Birthday paradox: How many people in a room such that the probability of “a pair of people has same birthday” $\sim 50\%$ ?</p>
<ul>
<li>$365$ possible birthday $\Rightarrow$ $23$ people suffice.</li>
<li>Number of pairs $\sim N^2$ $\Rightarrow$ $\sqrt{365}$ to find collision</li>
</ul>
</body></html>]]></content>
      <categories>
        <category>Quantum Computing</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Course Note</tag>
        <tag>Quantum Computing</tag>
        <tag>Quantum Algorithm</tag>
        <tag>Quantum Circuits</tag>
      </tags>
  </entry>
  <entry>
    <title>[Note] Quantum Computation and Quantum Information - Chapter 5: The quantum Fourier transform and its applications</title>
    <url>/Ending2015a/16720/</url>
    <content><![CDATA[<html><head></head><body><p>上課筆記，原文書:</p>
<blockquote>
<p>Quantum Computation and Quantum Information, Michael A. Nielsen & Isaac L. Chuang</p>
</blockquote>
<p><img src="https://i.imgur.com/kaLCQJW.png" alt></p>
<a id="more"></a>
<h2 id="5-1-The-quantum-Fourier-transform" class="heading-control"><a href="#5-1-The-quantum-Fourier-transform" class="headerlink" title="5.1 The quantum Fourier transform"></a>5.1 The quantum Fourier transform<a class="heading-anchor" href="#5-1-The-quantum-Fourier-transform" aria-hidden="true"></a></h2><ul>
<li>Discrete Fourier transform<ul>
<li>$F_N: \mathbb{C}^N\to\mathbb{C}^N$<script type="math/tex; mode=display">
\left[\begin{matrix}x_0\\x_1\\\vdots\\x_{N-1}\end{matrix}\right]
\xrightarrow{F_N}
\left[\begin{matrix}y_0\\y_1\\\vdots\\y_{N-1}\end{matrix}\right],\quad
y_k\equiv\frac{1}{\sqrt N}\sum^{N-1}_{j=0}x_je^{2\pi~ijk/N}</script></li>
</ul>
</li>
<li>Quantum Fourier transform<script type="math/tex; mode=display">
\sum^{N-1}_{j=0}x_j\vert j\rangle \xrightarrow{QFT_N}\sum^{N-1}_{k=0}y_k\vert k\rangle, \quad y_k\equiv\frac{1}{\sqrt N}\sum^{N-1}_{j=0}x_je^{2\pi~ijk/N}</script></li>
</ul>
<div class="note info">
            <p>Prove this transformation is unitary</p>
          </div>
<ul>
<li>Some notations $\vert j\rangle$<ul>
<li>Binary representation: $j=j_1j_2\dots j_n$<ul>
<li>$j=j_12^{n-1}+j_22^{n-2}+\cdots+j_n2^0$</li>
<li>e.g. $110_2\Rightarrow4+2+0=6$</li>
</ul>
</li>
<li>Binary fraction: $0.j_lj_{l+1}\dots j_m$<ul>
<li>$j_l/2+j_{l+1}/4+\cdots+j_m/2^{m-l+1}$</li>
<li>$0.11_2=1/2+1/4=3/4$</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="note success">
            <p>Take $N=2^n$, prove the <em>product representation</em> of quantum Fourier transform</p><script type="math/tex; mode=display">\vert j_1,\dots,j_n\rangle\to\frac{1}{2^{n/2}}\left(\vert 0\rangle+e^{2\pi i0.j_n}\vert 1\rangle\right)\left(\vert 0\rangle+e^{2\pi i0.j_{n-1}j_n}\vert 1\rangle\right)\cdots\left(\vert 0\rangle+e^{2\pi i0.j_1j_2\dots j_n}\vert 1\rangle\right)</script><p><em>Proof</em> By definition:</p><script type="math/tex; mode=display">\begin{align*}\vert j\rangle &\to \frac{1}{2^{N/2}}\sum^{2^n-1}_{k=0}e^{2\pi ijk/2^n}\vert k\rangle\\    &= \frac{1}{2^{n/2}}\sum^1_{k_1=0}\sum^1_{k_2=0}\cdots\sum^1_{k_n=0}e^{2\pi ij(\sum^n_{l=1}k_l2^{-l})}\vert k_1\dots k_n\rangle\\    &= \frac{1}{2^{n/2}}\bigotimes^n_{l=1}\left[\sum^1_{k_l=0}e^{2\pi ijk_j2^{-l}}\vert k_l\rangle\right]\\    &= \frac{1}{2^{n/2}}\bigotimes^n_{l=1}\left[ \vert 0\rangle+e^{2\pi ij2^{-l}}\vert 1\rangle \right]\\    &= \frac{1}{2^{n/2}}\left( \vert 0\rangle + e^{2\pi i0.j_n}\vert 1\rangle \right)\otimes\left( \vert 0\rangle + e^{2\pi i0.j_{n-1}j_n}\vert 1\rangle \right)\otimes\cdots\\    &\qquad\qquad\qquad\qquad\qquad\otimes\left( \vert 0\rangle + e^{2\pi i0.j_1j_2\dots j_n}\vert 1\rangle \right)\end{align*}</script>
          </div>
<ul>
<li>Easy to derive an efficient circuit for the product representaiton<ul>
<li>$R_k$ gate<script type="math/tex; mode=display">
R_k\equiv\left[\begin{matrix} 1&0\\0&e^{2\pi i/2^k} \end{matrix}\right]</script></li>
</ul>
</li>
</ul>
<p>QFT circuit<br><img src="https://i.imgur.com/kaLCQJW.png" alt></p>
<ol>
<li>Input: $\vert j_1j_2\dots j_n\rangle$</li>
<li>Hadamard on first qubit:<script type="math/tex; mode=display">
\frac{1}{2^{n/2}}(\vert 0\rangle+e^{2\pi i0.j_1}\vert 1\rangle)\otimes \vert j_2\dots j_n\rangle,\quad\because e^{2\pi i0.j_1}=(-1)^{j_1}</script></li>
<li>Applying controlled-$R_2$ on first qubit:<script type="math/tex; mode=display">
\frac{1}{2^{n/2}}(\vert 0\rangle + e^{2\pi i0.j_1j_2}\vert 1\rangle)\otimes\vert j_2\dots j_n\rangle</script></li>
<li>Similarly, controlled-$R_3$, $\dots$, $R_n$ on first qubit:<script type="math/tex; mode=display">
\frac{1}{2^{n/2}}(\vert 0\rangle + e^{2\pi i0.j_1j_2\dots j_n}\vert 1\rangle)\otimes \vert j_2\dots j_n\rangle</script></li>
<li>On second qubit, Hadamard:<script type="math/tex; mode=display">
\frac{1}{2^{n/2}}(\vert 0\rangle + e^{2\pi i0.j_1j_2\dots j_n}\vert 1\rangle)\otimes (\vert 0\rangle + e^{2\pi i0.j_2}\vert 1\rangle)\otimes \vert j_3\dots j_n\rangle</script></li>
<li>Controlled-$R_2$, $\dots$, $R_{n-1}$ on second qubit:<script type="math/tex; mode=display">
\frac{1}{2^{n/2}}(\vert 0\rangle + e^{2\pi i0.j_1j_2\dots j_n}\vert 1\rangle)\otimes (\vert 0\rangle + e^{2\pi i0.j_2\dots j_n}\vert 1\rangle)\otimes \vert j_3\dots j_n\rangle</script></li>
<li>~$n$-th qubit:<script type="math/tex; mode=display">
\frac{1}{2^{n/2}}(\vert 0\rangle + e^{2\pi i0.j_1j_2\dots j_n}\vert 1\rangle)\otimes (\vert 0\rangle + e^{2\pi i0.j_2\dots j_n}\vert 1\rangle)\otimes \cdots\otimes (\vert 0\rangle+e^{2\pi i0.j_n}\vert1\rangle)</script></li>
<li>Reverse order of qubits with $SWAP$ gates $\Rightarrow$ get QFT.</li>
</ol>
<h2 id="5-2-Phase-estimation" class="heading-control"><a href="#5-2-Phase-estimation" class="headerlink" title="5.2 Phase estimation"></a>5.2 Phase estimation<a class="heading-anchor" href="#5-2-Phase-estimation" aria-hidden="true"></a></h2><p>Suppose a unitary $U$ (<em>oracle</em>) has an eigenvector $\vert u\rangle$ with eigenvalue $e^{2\pi i\varphi}$, estimate phase $\varphi$.</p>
<ul>
<li>Assume we can do controlled-$U$. Also for efficient implementation, assume we have controlled-$U^{2^j}$.</li>
<li>$U\vert u\rangle=e^{2\pi i\varphi}\vert u\rangle$</li>
</ul>
<p>Small case: suppose $\varphi=\frac{1}{2} \text{ or } 0\Rightarrow U\vert u\rangle=\vert u\rangle \text{ or } U\vert u\rangle=-\vert u\rangle$</p>
<p><img src="https://i.imgur.com/kUnv4ei.png" width="300px"></p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi \rangle &= \text{C-}U\vert+\rangle\vert u\rangle\\
    &= \frac{1}{\sqrt 2}\text{C-}U(\vert 0\rangle\vert u\rangle+\vert 1\rangle\vert u\rangle)\\
    &= \frac{1}{\sqrt 2}(\vert 0\rangle\vert u\rangle+\vert 1\rangle e^{2\pi i\varphi}\vert u\rangle)\\
    &= \frac{1}{\sqrt 2}(\vert 0\rangle+2^{2\pi i\varphi}\vert 1\rangle)\otimes \vert u\rangle\\
    &= \begin{cases}
        \vert +\rangle\otimes \vert u\rangle, &\text{if } \varphi=0,\\
        \vert -\rangle\otimes \vert u\rangle, &\text{if } \varphi=\frac{1}{2}
    \end{cases}
\end{align*}</script><p>$\Rightarrow$<br><img src="https://i.imgur.com/084U6xI.png" width="300px"></p>
<p>$\Rightarrow$ Only works for $\varphi=0.j_1$</p>
<p>General case: $\varphi=0.j_1j_2\dots j_n$<br><img src="https://i.imgur.com/fBVLv78.png" alt></p>
<ul>
<li>Final state:<script type="math/tex; mode=display">
\begin{align*}
&\frac{1}{2^{t/2}}(\vert 0\rangle+e^{2\pi i2^{t-1}\varphi}\vert 1\rangle)(\vert 0\rangle+e^{2\pi i2^{t-2}\varphi}\vert 1\rangle)\cdots(\vert 0\rangle+e^{2\pi i\varphi}\vert 1\rangle)\\
&\quad\because 2^{t-1}\varphi=j_1j_2\dots j_n\\
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}
\Rightarrow ~&\frac{1}{2^{t/2}}(\vert 0\rangle+e^{2\pi i0.j_t}\vert 1\rangle)(\vert 0\rangle+e^{2\pi i0.j_{t-1}j_t}\vert 1\rangle)\cdots(\vert 0\rangle+e^{2\pi i0.j_1j_2\dots j_t}\vert 1\rangle)\\
\Rightarrow ~&QFT_t\vert 2^t\varphi\rangle = \frac{1}{2^{t/2}}\sum^{2^t-1}_{k=0} e^{2\pi ik\varphi}\vert k\rangle
\end{align*}</script></li>
<li>Apply $(QFT_t)^{-1}$ to get $\vert \varphi\rangle$<br><img src="https://i.imgur.com/fyKx3qp.png" width="500px"></li>
</ul>
<h3 id="5-2-1-Performance-and-requirements" class="heading-control"><a href="#5-2-1-Performance-and-requirements" class="headerlink" title="5.2.1 Performance and requirements"></a>5.2.1 Performance and requirements<a class="heading-anchor" href="#5-2-1-Performance-and-requirements" aria-hidden="true"></a></h3><p>The algorithm has “good” accuracy even if $\varphi$ isn’t exactly $t$ bit binary expansion, i.e. $\varphi$ can be irrational.</p>
<p>Let $b=\text{first } t \text{ bits of } \varphi$, </p>
<script type="math/tex; mode=display">
\varphi=0.\underbrace{b_1b_2\dots b_t}_bb_{t+1}b_{t+2}\dots</script><p>define $\delta\equiv \varphi - b/2^t$</p>
<script type="math/tex; mode=display">
\frac{1}{2^{t/2}}\sum^{2^t-1}_{k=0}e^{2\pi i\varphi k}\vert k\rangle \xrightarrow{QFT^{-1}}\frac{1}{2^t}\sum^{2^t-1}_{l=0}\sum^{2^t-1}_{k=0} e^{-2\pi i\frac{kl}{2^t}}e^{2\pi i\varphi k}\vert l\rangle</script><p>Let $\alpha_l$ be the amplitude of $\vert b+l~(\text{mod }2^t)\rangle$,</p>
<script type="math/tex; mode=display">
\begin{align*}
\alpha_l
    &\equiv\frac{1}{2^t}\sum^{2^t-1}_{k=0} e^{2\pi ik(\varphi-\frac{b+l}{2^t})}\\
    &=\frac{1}{2^t}\sum^{2^t-1}_{k=0}\left[e^{2\pi i(\varphi-\frac{b+l}{2^t})}\right]^k\quad(\because\alpha_l \text{ is a geometric series } \frac{1-r^n}{1-r})\\
    &= \frac{1}{2^t}\frac{1-e^{2\pi i(2^t\varphi-(b+l))}}{1-e^{2\pi i(\varphi-(b+l)/2^t)}}\\
    &= \frac{1}{2^t}\frac{1-e^{2\pi i(2^t\delta-l)}}{1-e^{2\pi i(\delta-l/2^t)}}
\end{align*}</script><p>Suppose measurement outcome is $m$, the probability of getting error results is</p>
<script type="math/tex; mode=display">
P(\vert m-b\vert>e)=\sum_{-2^{t-1}<l\le-(e+1)}\vert\alpha_l\vert^2+\sum_{e+1\le l\le2^{t-1}}\vert\alpha_l\vert^2</script><p>where $e$ is a positive integer characterizing desired tolerance to error.</p>
<p>For any real $\theta$, $\vert1-e^{i\theta}\vert\le 2$,</p>
<script type="math/tex; mode=display">
\vert \alpha_l\vert = \left\vert\frac{1}{2^t}\frac{1-e^{2\pi i(2^t\delta-l)}}{1-e^{2\pi i(\delta-l/2^t)}}\right\vert\le\frac{1}{2^t}\frac{2}{\vert1-e^{2\pi i(\delta-l/2^t)}\vert}</script><p>By geometry or calculus $\vert 1-e^{i\theta}\vert\ge\frac{2\vert \theta\vert}{\pi}$ whenever $-\pi\le\theta\le\pi$. But when $-2^{t-1}<l\le 2^{t-1}$ we have $-\pi\le 2\pi(\delta-l/2^t)\le \pi$. Thus</p>
<script type="math/tex; mode=display">
\vert \alpha_l\vert \le \frac{1}{2^{t+1}\vert \delta-l/2^t\vert}</script><p>Then</p>
<script type="math/tex; mode=display">
\begin{align*}
P(\vert m-b\vert > e)
    &\le \frac{1}{4}\left[ \sum^{-(e+1)}_{l=-2^{t-1}+1}\frac{1}{(l-2^t\delta)^2}+\sum^{2^{t-1}}_{l=e+1}\frac{1}{(l-2^t\delta)^2} \right]\\
    &\le \frac{1}{4}\left[ \sum^{-(e+1)}_{l=-2^{t-1}+1}\frac{1}{l^2}+\sum^{2^{t-1}}_{l=e+1}\frac{1}{(l-1)^2} \right] \quad(\because 0\le2^t\delta\le 1)\\
    &\le \frac{1}{2}\sum^{2^{t-1}-1}_{l=e}\frac{1}{l^2}\\
    &\le \frac{1}{2}\int^{2^{t-1}-1}_{e-1}dl\frac{1}{l^2}=\frac{1}{2(e-1)}
\end{align*}</script><p>To have accuracy $2^{-n}$ with probability of success at least $l-\epsilon$, where $\epsilon$ is error rate, choose $e\equiv2^{t-n}-1$ and let $t=n+p$ qubits,</p>
<script type="math/tex; mode=display">
\epsilon = \frac{1}{2(2^p-2)} \Rightarrow p=\left\lceil \log\left(2+\frac{1}{2\epsilon}\right) \right\rceil</script><script type="math/tex; mode=display">
\Rightarrow~t=n+\left\lceil \log\left(2+\frac{1}{2\epsilon}\right) \right\rceil</script><p>If we don’t have eigenstate $\vert u\rangle$, pick a “random” state $\vert \psi\rangle = \sum_u c_u\vert u\rangle$.</p>
<script type="math/tex; mode=display">
\sum_uc_u\vert u\rangle \xrightarrow{p.e.}\sum_u c_u\vert \varphi_u\rangle\vert u\rangle</script><h2 id="5-3-Applications-order-finding-and-factoring" class="heading-control"><a href="#5-3-Applications-order-finding-and-factoring" class="headerlink" title="5.3 Applications: order-finding and factoring"></a>5.3 Applications: order-finding and factoring<a class="heading-anchor" href="#5-3-Applications-order-finding-and-factoring" aria-hidden="true"></a></h2><h3 id="5-3-1-Application-order-finding" class="heading-control"><a href="#5-3-1-Application-order-finding" class="headerlink" title="5.3.1 Application: order-finding"></a>5.3.1 Application: order-finding<a class="heading-anchor" href="#5-3-1-Application-order-finding" aria-hidden="true"></a></h3><p>Given $x, N$, $x<N$, $\text{gcd}(x,N)=1$, the <em>order</em> of $x$ module $N$ is the least positive integer $r$, s.t. $x^r=1(\text{mod }N)$.</p>
<ul>
<li>Problem size $L\equiv\lceil\log(N)\rceil$</li>
<li>Classical: no poly$(L)$</li>
<li>Quantum: $O(L^3)$<ul>
<li>Can reduce factoring to order finding</li>
<li>Order-finding is the phase estimation applied to the unitary $U_x$<script type="math/tex; mode=display">
U_x\vert y\rangle\equiv \vert xy~(\text{mod } N)\rangle, \quad y\in\{x,y\}^L</script></li>
<li>$U_x$ does the cyclic permutation on $x$:<script type="math/tex; mode=display">
\begin{align*}
x&\xrightarrow{U_x}x^2,\\
x^2&\xrightarrow{U_x}x^3\\
&~~~\vdots\\
x^{r-1}&\xrightarrow{U_x}x^r\\
x^r&\xrightarrow{U_x}x
\end{align*}</script></li>
<li>$\vert x^r ~(\text{mod }N)\rangle=\vert 1 ~(\text{mod }N)\rangle$</li>
<li>$\vert u_0\rangle\equiv\vert x\rangle+\vert x^2\rangle+\cdots\vert x^r\rangle$ is an eigenstate of $U_x$. $\because U_x\vert u_0\rangle=\vert x^2\rangle+\vert x^3\rangle+\cdots\vert x\rangle=\vert u_0\rangle$.</li>
<li>Let $\omega=e^{-\frac{2\pi i}{r}}$, $\vert u_1\rangle\equiv\omega\vert x\rangle+\omega^2\vert x^2\rangle+\cdots+\omega^r\vert x^r\rangle$ is also an eigenstate of $U_x$. $\because U_x\vert u_1\rangle=\omega\vert x^2\rangle+\omega^2\vert x^3\rangle+\cdots+\underbrace{\omega^r}_{=1}\vert x\rangle=\omega^{-1}\vert u_1\rangle$</li>
</ul>
</li>
</ul>
<p>The eigenstates of $U_x$ can be defined by</p>
<script type="math/tex; mode=display">
\vert u_s\rangle = \frac{1}{\sqrt r}\sum^{r-1}_{k=0}e^{-\frac{2\pi isk}{r}} \vert x^k \text{ mod}~N\rangle</script><p>for integer $0\le s\le r-1$, since</p>
<script type="math/tex; mode=display">
\begin{align*}
U_x\vert u_s\rangle &= \frac{1}{\sqrt r}\sum^{r-1}_{k=0}e^{-\frac{2\pi isk}{r}}\vert x^{k+1} \text{ mod}~N\rangle\\
    &= e^{\frac{2\pi is}{r}}\vert u_s\rangle
\end{align*}</script><p>Two requirements to use the phase estimation:</p>
<ul>
<li>Implement a controlled-$U^{2^j}$ for any integer $j$.<ul>
<li>Can be resolved by using <em>modular exponentiation</em></li>
</ul>
</li>
<li>Efficiently prepare an eigenstate $\vert u_s\rangle$ with a non-trivial eigenvalue, or at least a superposition of such eigenstates.<ul>
<li>Observe that $\frac{1}{\sqrt r}\sum^{r-1}_{s=0}\vert u_s\rangle=\vert 1\rangle$, use $\vert 1\rangle$ as the input for second register.</li>
</ul>
</li>
</ul>
<div class="note info">
            <script type="math/tex; mode=display">\begin{align*}\frac{1}{\sqrt r}\sum^{r-1}_{s=0}\vert u_s\rangle &= \frac{1}{\sqrt r}\sum^{r-1}_{s=0}\left( \frac{1}{\sqrt r}\sum^{r-1}_{k=0}e^{-\frac{2\pi isk}{r}} \vert x^k \text{ mod } N\rangle \right)\\    &= \frac{1}{r}\sum^{r-1}_{s=0}\sum^{r-1}_{k=0}e^{-\frac{2\pi isk}{r}} \vert x^k \text{ mod } N\rangle\\    &= \frac{1}{r}\sum^{r-1}_{k=0}\left(\sum^{r-1}_{s=0}e^{-\frac{2\pi isk}{r}}\right) \vert x^k \text{ mod } N\rangle\\    &= \frac{1}{r}\sum^{r-1}_{k=0}r\delta_{k,0} \vert x^k \text{ mod } N\rangle\\    &= \vert x^0 \text{ mod } N\rangle =\vert 1\rangle\end{align*}</script>
          </div>
<p>Apply phase estimation (PE):<br><img src="https://i.imgur.com/6JOd4qd.png" alt></p>
<script type="math/tex; mode=display">
\begin{align*}
\vert 0\rangle^{\otimes t}\vert 1\rangle \xrightarrow{\text{PE of }U_x} &~\frac{1}{\sqrt r}\sum_s\vert \frac{s}{r}\rangle \vert u_s\rangle\\
\Rightarrow&~\text{get }\frac{s}{r}\text{ with uniform random }s\\
\Rightarrow&~\text{get a few of }\frac{s_1}{r_1}, \frac{s_2}{r_2}, \cdots\\
\Rightarrow&~\text{calculate }r.
\end{align*}</script><p>If we use $t=2L+1+\left\lceil \log(2+\frac{1}{2\epsilon}) \right\rceil$ qubits in phase estimation, we will obtain an estimate of the phase $\varphi\approx s/r$ accuract to $2L+1$ bits, with probability at least $(1-\epsilon)/r$. Then, we find $r$ using contined fraction expansion.</p>
<p>Modular exponential:</p>
<ul>
<li>example: compute $x^{17}=\underbrace{x\cdot x\cdot\cdots\cdot x}_\text{17 terms}$ $\Rightarrow$ $16$ multiplications.</li>
<li>$x\cdot x=x^2$, $x^2\cdot x^2=x^4$, $x^4\cdot x^4=x^8$, $x^8\cdot x^8=x^{16}$, $x^{16}\cdot x=x^{17}$ $\Rightarrow$ $5$ multiplication.</li>
<li>In general can do $x^r$ with $O(\text{poly}\log(r))$ multiplications.</li>
</ul>
<h4 id="The-continued-fraction-expansion" class="heading-control"><a href="#The-continued-fraction-expansion" class="headerlink" title="The continued fraction expansion"></a>The continued fraction expansion<a class="heading-anchor" href="#The-continued-fraction-expansion" aria-hidden="true"></a></h4><ul>
<li>Given $\varphi\approx s/r$, how to find $s$, $r$?</li>
<li>ex. $\varphi=0.25001$, $s/r=1/4$?<script type="math/tex; mode=display">
\begin{align*}
\varphi&=\frac{25001}{100000}=\frac{1}{\frac{100000}{25001}}=\frac{1}{3+\frac{24997}{25001}}\\
&=\frac{1}{3+\frac{1}{\frac{25001}{24997}}}=\frac{1}{3+\frac{1}{1+\underbrace{\frac{4}{24997}}_{\text{small}}}}
\end{align*}</script></li>
</ul>
<div class="note success">
            <p><em>Theorem 5.1</em>: Suppose $s/r$ is a rational number such that</p><script type="math/tex; mode=display">\left\vert \frac{s}{r}-\varphi \right\vert\le \frac{1}{2r^2}</script><p>Then $s/r$ is a convergent of the continued fraction for $\varphi$, and thus can be computed in $O(L^3)$ operations using the continued fractions algorithm.</p><p>Since $\varphi$ is an approximation of $s/r$ accurate to $2L+1$ bits, it follows that $\vert s/r-\varphi\vert\le 2^{-2L-1}\le 1/2r^2$, since $r\le N\le 2^L$. Thus, the theorem applies.</p>
          </div>
<h4 id="Performance" class="heading-control"><a href="#Performance" class="headerlink" title="Performance"></a>Performance<a class="heading-anchor" href="#Performance" aria-hidden="true"></a></h4><p>Some considerations:</p>
<ul>
<li>phase estimation might produce a bad estimate to $s/r$ with probability $\epsilon$.</li>
<li>$s$, $r$ have a common factor, in which case the number $r’$ returned by continued fractions algorithm be a factor of $r$, and not $r$ itself $\Rightarrow$ $s$, $r$ might not be co-prime.</li>
</ul>
<ol>
<li>Solution $1$<ul>
<li>The number of prime numbers less than $r$ is at least $r/2\log r$</li>
<li>The chance that $s$ is prime is at least $1/2\log(r)>1/2\log(N)$</li>
<li>Repeating algorithm $2\log(N)$ times ($s_1/r_1, s_2/r_2, \dots$) will have high probability to obtain co-prime $s$, $r$.</li>
</ul>
</li>
<li>Solution $2$<ul>
<li>if $r’\ne r~\Rightarrow~r’$ is a factor of $r$. The probability is $1/r\le1/2$</li>
<li><blockquote>
<p>待補</p>
</blockquote>
</li>
</ul>
</li>
<li>Solution $3$<ul>
<li>Do phase estimation and continued fractions twice, get $r’_1$, $s’_1$, $r’_2$, $s’_2$. Find $r$ from $r_1$, $r_2$.</li>
<li>If $LCM(r_1, r_2)\ne r$, exist $q$ s.t. $q$ divides $s’_1$ and $q$ divides $s’_2$. The probability that $s’_1$, $s’_2$ have common factors satisfies<script type="math/tex; mode=display">
\sum_qp(q\vert s'_1 \cap q\vert s'_2)=\sum_qp(q\vert s'_1)p(q\vert s'_2)\le \sum_q\frac{1}{q^2}\le \sum^\infty_{q=2}\frac{1}{q^2}\le\frac{3}{4}</script></li>
<li>The probability of obtaining the correct $r$ is at least $1/4$</li>
</ul>
</li>
</ol>
<h3 id="5-3-2-Application-factoring" class="heading-control"><a href="#5-3-2-Application-factoring" class="headerlink" title="5.3.2 Application: factoring"></a>5.3.2 Application: factoring<a class="heading-anchor" href="#5-3-2-Application-factoring" aria-hidden="true"></a></h3><ul>
<li>Reducing factor to order finding </li>
<li>Randomly pick $x$ co-prime to $N$, find order $r$ s.t. $x^r\equiv 1(\text{mod }N)$<ul>
<li>if $r$ is odd, try another $r$ (with const probability)</li>
<li>if $r$ is even, compose $x^{r/2} (\text{mod }N)$</li>
</ul>
</li>
<li>We know $(x^{r/2})^2\equiv 1(\text{mod }N)\Rightarrow (x^{r/2}-1)(x^{r/2}+1)\equiv 0(\text{mod }N)$</li>
<li>4 cases, if $N=pq$</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>$\text{gcd}(x^{r/2}-1, N)$</th>
<th>$\text{gcd}(x^{r/2}+1, N)$</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>$pq$</td>
<td>$1$</td>
<td>$x^{r/2}\equiv 1(\text{mod }N)$ impossible</td>
</tr>
<tr>
<td>$1$</td>
<td>$pq$</td>
<td>$x^{r/2}\equiv -1(\text{mod }N)$ const probability</td>
</tr>
<tr>
<td>$p$</td>
<td>$q$</td>
<td>Non-trivial factor, calculate $\text{gcd}(x^{r/2}-1, N)$</td>
</tr>
<tr>
<td>$q$</td>
<td>$p$</td>
<td>Non-trivial factor, calculate $\text{gcd}(x^{r/2}+1, N)$</td>
</tr>
</tbody>
</table>
</div>
<div class="note success">
            <p><em>Theorem 5.2</em>: Suppose $N$ is an $L$ bit composite number, and $x$ is a non-trivial solution to the equation $x^2=1(\text{mod} N)$ in the range $1\le x\le N$, that is, neither $x=1(\text{mod} N)$ nor $x=N-1=-1(\text{mod} N)$. Then at least one of $\text{gcd}(x-1, N)$ and $\text{gcd}(x+1, N)$ is a non-trivial factor of $N$ that can be computed using $O(L^3)$ operations.</p>
          </div>
<div class="note success">
            <p><em>Theorem 5.3</em>: Suppose $N=p^{\alpha_1}_{1}\dots p^{\alpha_m}_m$ is the prime factorization of an odd composite positive integer. Let $x$ be an integer chosen uniformly at random, subject to the requirements that $1\le x\le N-1$ and $x$ is co-prime to $N$. Let $r$ be the order of $x$ module $N$. Then</p><script type="math/tex; mode=display">p(r\text{ is even and }x^{r/2}\ne -1(\text{mod}N))\ge \text{const.}</script>
          </div>
<p>Detail algorithm:</p>
<ul>
<li>Input $N$</li>
</ul>
<ol>
<li>If $N$ can be divided by $2$, return $2$</li>
<li>Determine whether $N=a^b$ by calculating $N^{1/2}, N^{1/3}, \dots, N^{1/L}$ (classical algorithm $O(L^3)$)</li>
<li>Choose random $x$ s.t. $1\le x\le N-1$, calculate $\text{gcd}(x, N)$. If $\text{gcd}(x, N)\ne 1$, return $\text{gcd}(x, N)$<ul>
<li>Now we know $N$ is odd and has more than $1$ prime factor, and $x$ is coprime to $N$.</li>
</ul>
</li>
<li>Use order finding algorithm to find minimum $r$ s.t. $x^r\equiv 1(\text{mod }N)$</li>
<li>If $r$ is even and $x^{r/2}\not\equiv-1 (\text{mod }N)$, return $\text{gcd}(x^{r/2}-1, N)$</li>
<li>Go back to step 3 and try another $x$</li>
</ol>
<ul>
<li>By theorem 5.3, this algorithm terminates in constant number of trails.</li>
</ul>
<h2 id="5-4-General-applications-of-the-quantum-Fourier-transform" class="heading-control"><a href="#5-4-General-applications-of-the-quantum-Fourier-transform" class="headerlink" title="5.4 General applications of the quantum Fourier transform"></a>5.4 General applications of the quantum Fourier transform<a class="heading-anchor" href="#5-4-General-applications-of-the-quantum-Fourier-transform" aria-hidden="true"></a></h2><h3 id="5-4-1-Period-finding" class="heading-control"><a href="#5-4-1-Period-finding" class="headerlink" title="5.4.1  Period-finding"></a>5.4.1  Period-finding<a class="heading-anchor" href="#5-4-1-Period-finding" aria-hidden="true"></a></h3><p>Given oracle $\tilde{U}_f: \tilde{U}_f\vert x\rangle\vert y\rangle = \vert x\rangle\vert f(x)\oplus y\rangle$ for some $f:N\to N$</p>
<ul>
<li>Promise: $\exists 1\le r\le c$ s.t. $f(x)=f(y)$ iff $r\mid \vert x-y\vert$ ($r$ is period of $f$)</li>
<li>Problem: Use $\tilde{U}_f$ to find $r$ s.t. $f(x+r)=f(x)$<ul>
<li>Classical: $\Omega(c)$ queries</li>
<li>Quantum: One query, short alg</li>
</ul>
</li>
</ul>
<div class="note success">
            <p><em>Algorithm</em> (<strong>Period-finding</strong>)</p><ul><li>Inputs: <ol><li>A blackbox with performs the operation $U\vert x\rangle\vert y\rangle = \vert x \rangle\vert y\oplus f(x)\rangle$</li><li>a state to store the function evaluation, initialized to $\vert 0\rangle$ </li><li>$t=O(L+\log(1/\epsilon))$ qubits initialized to $\vert 0\rangle$</li></ol></li><li>Outputs: The least integer $r>0$ such that $f(x+r)=f(x)$</li><li>Runtime: One use of $U$, and $O(L^2)$ operations. Succeeds with probability $O(1)$.</li><li>Prodecure:<ol><li>$\vert 0\rangle\vert 0\rangle$  (initial state)</li><li>$\to \displaystyle\frac{1}{\sqrt{2^t}}\sum^{2^t-1}_{x=0}\vert x\rangle \vert 0\rangle$  (create superposition)</li><li>$\to \displaystyle\frac{1}{\sqrt{2^t}}\sum^{2^t-1}_{x=0}\vert x\rangle\vert f(x)\rangle$ (apply $U$)<br> $\approx\displaystyle\frac{1}{\sqrt{r2^t}}\sum^{r-1}_{\ell=0}\sum^{2^t-1}_{x=0}e^{2\pi i\ell x/r}\vert x\rangle \vert \hat{f}(\ell)\rangle$</li><li>$\to\displaystyle\frac{1}{\sqrt r}\sum^{r-1}_{l=0}\tilde{\vert\ell/r\rangle}\vert \hat{f}(\ell)\rangle$  (apply inverse Fourier transform to first register)</li><li>$\to\tilde{\ell/r}$  (measure first register)</li><li>$\to r$  (apply continued fractions algorithm)</li></ol></li></ul>
          </div>
<h4 id="Hidden-shift-problem" class="heading-control"><a href="#Hidden-shift-problem" class="headerlink" title="Hidden shift problem"></a>Hidden shift problem<a class="heading-anchor" href="#Hidden-shift-problem" aria-hidden="true"></a></h4><p>Given $\tilde{U}_f\vert x\rangle\vert f(y)\rangle=\vert x\rangle \vert f(x+y)\rangle$, also given $(x_0, f(x_0))$ for some $x_0$</p>
<ul>
<li>Problem: find $r$ </li>
<li>$\displaystyle\frac{1}{\sqrt r}\sum_j e^{2\pi i\frac{j}{r}}\vert f(x_0+j)\rangle$ are eigenvalues of $\tilde{U}_f\vert 1\rangle$<ul>
<li>Use phase estimation to get $\displaystyle\frac{s}{r}$</li>
</ul>
</li>
</ul>
<p>Blackbox generalization of order finding</p>
<ul>
<li>Period finding v.s. Hidden shift: Period finding is the mainstream. No phase estimation</li>
</ul>
<h3 id="5-4-2-Discrete-logarithms" class="heading-control"><a href="#5-4-2-Discrete-logarithms" class="headerlink" title="5.4.2 Discrete logarithms"></a>5.4.2 Discrete logarithms<a class="heading-anchor" href="#5-4-2-Discrete-logarithms" aria-hidden="true"></a></h3><blockquote>
<p>待補</p>
</blockquote>
<h3 id="5-4-3-The-hidden-subgroup-problem" class="heading-control"><a href="#5-4-3-The-hidden-subgroup-problem" class="headerlink" title="5.4.3 The hidden subgroup problem"></a>5.4.3 The hidden subgroup problem<a class="heading-anchor" href="#5-4-3-The-hidden-subgroup-problem" aria-hidden="true"></a></h3><blockquote>
<p>待補</p>
</blockquote>
<h3 id="5-4-4-Other-quantum-algorithm" class="heading-control"><a href="#5-4-4-Other-quantum-algorithm" class="headerlink" title="5.4.4 Other quantum algorithm"></a>5.4.4 Other quantum algorithm<a class="heading-anchor" href="#5-4-4-Other-quantum-algorithm" aria-hidden="true"></a></h3><blockquote>
<p>待補</p>
</blockquote>
<h2 id="Bonus" class="heading-control"><a href="#Bonus" class="headerlink" title="Bonus"></a>Bonus<a class="heading-anchor" href="#Bonus" aria-hidden="true"></a></h2><h3 id="Group-theory" class="heading-control"><a href="#Group-theory" class="headerlink" title="Group theory"></a>Group theory<a class="heading-anchor" href="#Group-theory" aria-hidden="true"></a></h3><ul>
<li>A group $G$ is a set of elements $\{a,b,c,\dots\}$ with a “multiplication” operation $\cdot$ satisfying the following properties<ol>
<li>Closure: $(a\cdot b)$ is an element of $G$</li>
<li>Associative: $a\cdot(b\cdot c)=(a\cdot b)\cdot c$</li>
<li>Identity: exist an element $e$ s.t. $e\cdot a=a$ and $a\cdot e=a$, $\forall a\in G$</li>
<li>Inverse: $\forall a\in G$, exist $a^{-1}$ s.t. $a\cdot a^{-1}=e=a^{-1}\cdot a$</li>
</ol>
</li>
<li>Order of a group $\vert G\vert$: number of elements of $G$</li>
<li>Subgroup: subset of $G$ that is also a group (with same multiplication) (most importantly, closure)</li>
<li>If $H$ is a subgroup of $G$, $\vert H\vert$ divides $\vert G\vert$</li>
</ul>
<p>Example: multiplication group mod $7$</p>
<ul>
<li>elements: $\{1,2,3,4,5,6\}\Rightarrow$ order $6$</li>
<li>operation: multiplication mod $7$</li>
<li>closure: $a\nmid 7, b\nmid 7 \Rightarrow ab\nmid 7$</li>
<li>association: from integer multiplication</li>
<li>identity: $1$</li>
<li>inverse: $x^r\equiv 1 (\text{mod } 7)\Rightarrow (x^{r-1})\cdot x^r=1\Rightarrow x^{r-1}\equiv x^{-1}$</li>
<li>subgroup: $\{x, x^2, x^3,\cdots, \underbrace{x^r}_{=1}\}$ is a subgroup, $\forall x$<ul>
<li>$\{1,2,4\}\Rightarrow$ order $3$</li>
<li>$\{1,6\}\Rightarrow$ order $2$</li>
</ul>
</li>
</ul>
<p>In general, multiplication mod $P$ for some prime $P$ is a group $\Rightarrow$ order $P-1$</p>
<script type="math/tex; mode=display">
r\mid(P-1)\Rightarrow (P-1)=c\cdot r\Rightarrow x^{(P-1)}=(x^r)^c\equiv 1(\text{mod }P)</script><p>Suppose $N=pq$, and $p,q$ is prime</p>
<ul>
<li>Define: $\varphi(N)$ (<em>Euler’s totient function</em>) is the number of $x$ s.t. $1\le x\le N-1$ and $\text{gcd}(x,N)=1$</li>
<li>Claim: $\varphi(N)=(p-1)(q-1)$</li>
<li>Example: $p=3$, $q=5$ $\Rightarrow N=15\Rightarrow\varphi(N)=2\cdot4=8$</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>$\text{mod }3 \backslash\text{mod }5$</th>
<th>$1$</th>
<th>$2$</th>
<th>$3$</th>
<th>$4$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$1$</td>
<td>$1$</td>
<td>$10+12=22\\\equiv 7(\text{mod }15)$</td>
<td>$10+18=28\\\equiv 13(\text{mod }15)$</td>
<td>$10+24=34\\\equiv 4(\text{mod }15)$</td>
</tr>
<tr>
<td>$2$</td>
<td>$20+6=26\\\equiv 11(\text{mod }15)$</td>
<td>$20+12=32\\\equiv 2(\text{mod }15)$</td>
<td>$20+18=38\\\equiv 8(\text{mod }15)$</td>
<td>$20+24=44\\\equiv 14(\text{mod }15)$</td>
</tr>
</tbody>
</table>
</div>
<p>$10=5\cdot 2\equiv 1 (\text{mod }3)$</p>
<p>$6=3\cdot 2\equiv 1(\text{mod } 5)$</p>
<p>$\Rightarrow 10\cdot a+6\cdot b\equiv a(\text{mod } 3)\equiv b(\text{mod } 5)$</p>
<p>Let $f(a,b)\equiv 10a+6b (\text{mod }15)\Rightarrow f(a,b)\cdot f(c,d)\equiv f(a+c \text{ mod }3, b+d \text{ mod }5)\text{mod }15$</p>
<h3 id="RSA-public-key-encryption" class="heading-control"><a href="#RSA-public-key-encryption" class="headerlink" title="RSA public key encryption"></a>RSA public key encryption<a class="heading-anchor" href="#RSA-public-key-encryption" aria-hidden="true"></a></h3><p>Construction phase:</p>
<ul>
<li>Select random large prime $p$ and $q$</li>
<li>calculate $N=pq$, $\varphi(N)=(p-1)(q-1)$</li>
<li>pick encryption key $e$ s.t. $\text{gcd}(e, \varphi(N))=1$</li>
<li>calculate decryption key $d$ s.t. $de\equiv 1(\text{mod }\varphi(N))$ (efficiently with extended Euler’s algorithm)</li>
</ul>
<p>Operations:<br><img src="https://i.imgur.com/TVATsRm.png" width="300px"></p>
<ul>
<li>Server send out $(N, e)$ as public key</li>
<li>User encode $m$ as $m^e(\text{mod }N)\equiv E(m)$ (Suppose user’s message is some $m$ coprime to $N$) </li>
<li>Server decrypt by calculating $(E(m))^d=(m^e)^d=m^{ed}=m (\text{mod }N)$</li>
</ul>
<p>Proof:<br>Recall:</p>
<ol>
<li>$ed-1\equiv 0(\text{mod }\varphi(N))$</li>
<li>Recall $r\mid \varphi(N)\Rightarrow \varphi(N)=c’\cdot r$</li>
</ol>
<p>$m^{ed-1}\stackrel{\text{by 1}}{=}m^{c\cdot \varphi(N)}\stackrel{\text{by 2}}{=} m^{cc’r}\equiv 1(\text{mod }N)\Rightarrow m^{ed-1}\equiv 1 \Rightarrow m^{ed}\equiv m(\text{mod }N)$</p>
<p>extend eular:<br>$\varphi(N)=8, e=3\Rightarrow 8-(3\cdot 2)=2, 3-2=1$</p>
<p>$\Rightarrow 1=3-2=3-(8-3\cdot 2)=3\cdot 3-8$</p>
<p>$3\cdot 3-8=1 (\text{mod }8)\Rightarrow 3\cdot 3\equiv 1(\text{mod }8)$</p>
<h3 id="Shor’s-algorithm-without-phase-estimation" class="heading-control"><a href="#Shor’s-algorithm-without-phase-estimation" class="headerlink" title="Shor’s algorithm without phase estimation"></a>Shor’s algorithm without phase estimation<a class="heading-anchor" href="#Shor’s-algorithm-without-phase-estimation" aria-hidden="true"></a></h3><ul>
<li><p>writing out the phase estimation part of order finding (The only quantum part of shor’s algorithm)<br><img src="https://i.imgur.com/VIPJfnF.png" alt></p>
</li>
<li><p>$U_x\vert y\rangle = \vert xy (\text{mod }N)\rangle$</p>
</li>
<li>$CU_x^j\vert j\rangle\vert y\rangle = \vert j\rangle\vert x^j y\rangle$</li>
</ul>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi_1\rangle&=\frac{1}{2^{t/2}}\sum^{2^t-1}_{j=0}\vert j\rangle\vert 1\rangle\\
\vert \psi_2\rangle&=\frac{1}{2^{t/2}}\sum^{2^t-1}_{j=0}\vert j\rangle\vert x^{j}\rangle
\end{align*}</script><p>Two changes we can make on the order finding algorithm:</p>
<ul>
<li>Get $\vert \psi_2\rangle$ from a different oracle</li>
<li>measure the second qubit after the oracle</li>
</ul>
<p>Let $\tilde{U}_x: \tilde{U}_x\vert j\rangle\vert y\rangle=\vert j\rangle\vert x^j\oplus y(\text{mod }N)\rangle$<br><img src="https://i.imgur.com/psCX0h1.png" alt></p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi_1\rangle &= \frac{1}{2^{t/2}}\sum^{2^t-1}_{j=0}\vert j\rangle \vert 0\rangle\\
\vert \psi_2\rangle &= \frac{1}{2^{t/2}}\sum^{2^t-1}_{j=0}\vert j\rangle \vert x^j\rangle
\end{align*}</script><p>Possible $x^j$: $x, x^2, x^3,\dots, x^r$<br>$\Rightarrow$ measure second register, get $x^s$ with some $s\in\{0, 1,\dots r-1\}$</p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi_3\rangle &\propto \left( \vert s\rangle+\vert s+r\rangle+\vert s+2r\rangle+\cdots \right)\otimes \vert x^s\rangle\\
    &=\frac{1}{\sqrt m}\sum^{m-1}_{j=0}\vert s+j\cdot r\rangle \otimes \vert x^s\rangle\qquad m\sim\frac{2^t}{r}
\end{align*}</script><p>$QFT(\sum_j\vert s+jr\rangle)=?$</p>
<ul>
<li><p>Easy case: suppose $r\mid 2^t$, i.e. $2^t=m\cdot r$</p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi_4\rangle &= QFT^{-1}(\sum_j\vert s+jr\rangle)\\
  &= \frac{1}{\sqrt m}\frac{1}{\sqrt{2^t}}\sum^{m-1}_{j=0}\sum^{2^t-1}_{k=0}e^{-2\pi i \frac{(s+jr)k}{2^t}}\vert k\rangle\\
  &= \frac{1}{\sqrt{m2^t}}\sum^{2^t-1}_{k=0}e^{-2\pi i\frac{sk}{2^t}}\sum^{m-1}_{j=0}e^{-2\pi i\frac{jk}{m}}\vert k\rangle\\
  \Rightarrow \left\vert \langle k\vert\psi_4\rangle\right\vert^2 &= \begin{cases}
      \left\vert \sqrt{\frac{m}{2^t}}e^{i\phi_k} \right\vert^2=\frac{1}{r}, & \text{if } m\mid k,\\
      0, & \text{otherwise}\\
  \end{cases}\\
  \Rightarrow m\mid k\Rightarrow &k=cm \Rightarrow \displaystyle\frac{k}{2^t}=\frac{c}{r}
\end{align*}</script></li>
<li><p>Hard case: $r\not\mid 2^t$</p>
<ul>
<li>Has error like when doing phase estimation and $\varphi\ne 0.b_1b_2\dots b_t$</li>
<li>error similar to what we get in phase estimation (geometric sum)</li>
<li>do continued fraction to get rid of error</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/JDiueGQ.png" width="300px"></p>
<p>Some comparisons</p>
<ul>
<li>Same output distribution, different oracles</li>
<li>Oracle of second interpretation has phase</li>
<li>Second interpretation is similar to Simon’s algorithm</li>
</ul>
<h3 id="Shor’s-algorithm-for-N-3-times17-51" class="heading-control"><a href="#Shor’s-algorithm-for-N-3-times17-51" class="headerlink" title="Shor’s algorithm for $N=3\times17=51$"></a>Shor’s algorithm for $N=3\times17=51$<a class="heading-anchor" href="#Shor’s-algorithm-for-N-3-times17-51" aria-hidden="true"></a></h3><div class="note info">
            <p>Recall</p><ul><li>$N=pq$, where $p$ and $q$ are primes.</li><li>Randomly pick $1<x\le N-1$ co-prime to $N$, find order $x^r \equiv 1(\text{mod}~N)$<ul><li>if $r$ is odd, try another $r$ (with const probability)</li><li>if $r$ is even, compose $x^{r/2}(\text{mod}~N)$</li></ul></li><li>We know $(x^{r/2})^2\equiv (\text{mod}~N)\Rightarrow (x^{r/2}-1)(x^{r/2}+1)\equiv 0(\text{mod}~N)$</li></ul>
          </div>
<ol>
<li>Pick $x=2$, find $r$ s.t. $x^r\equiv 1 (\text{mod}~N)$ (Using order-finding algorithm)<br> $\Rightarrow r=8$ ($r$ is even)</li>
<li>$x^{r/2}+1=17, \quad x^{r/2}-1=15$</li>
<li>Find $\text{gcd}(x^{r/2}-1, N)=\text{gcd}(15, 51)=3$</li>
</ol>
<p>Order-finding part (assume $t=6$, $2^t=64$, $2^t-1=63$):<br><img src="https://i.imgur.com/quthZiz.png" alt></p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \psi_1\rangle &= (H^{\otimes 6}\vert 0\rangle^{\otimes 6})\otimes \vert 1\rangle =\frac{1}{8}\sum^{63}_{j=0}\vert j\rangle\otimes \vert 1\rangle\\
\vert \psi_2\rangle &= \frac{1}{8}\sum^{63}_{j=0}\vert j\rangle \otimes \vert x^j\rangle
\end{align*}</script><div class="note info">
            <p>Recall:</p><script type="math/tex; mode=display">\begin{align*}&\frac{1}{\sqrt r}\sum^{r-1}_{s=0}\vert u_s\rangle=\vert 1\rangle\\\vert u_s\rangle &= \frac{1}{\sqrt r}\sum^{r-1}_{k=0}e^{-\frac{2\pi isk}{r}} \vert x^k \text{ mod}~N\rangle\\U_x\vert u_s\rangle &= \frac{1}{\sqrt r}\sum^{r-1}_{k=0}e^{-\frac{2\pi isk}{r}}\vert x^{k+1} \text{ mod}~N\rangle    = e^{\frac{2\pi is}{r}}\vert u_s\rangle\end{align*}</script>
          </div>
<script type="math/tex; mode=display">
\begin{align*}
\Rightarrow&&\vert 1\rangle &= \frac{1}{\sqrt 8}\sum^{7}_{s=0}\vert u_s\rangle, \quad \vert u_s\rangle = \frac{1}{\sqrt8}\sum^{7}_{y=0}e^{-2\pi i\frac{sy}{8}}\vert x^y\rangle\\
&&(x^j)\vert u_s\rangle &= \frac{1}{\sqrt8}\sum^{7}_{y=0} e^{-2\pi i\frac{sy}{8}}\vert x^{j+y}\rangle \qquad (y'=j+y,~~y=y'-j)\\
    &&&= \frac{1}{\sqrt8}\sum^{7}_{y=0} e^{-2\pi i\frac{s(y'-j)}{8}} \vert x^{y'}\rangle\\
    &&&= e^{2\pi i\frac{sj}{8}}\vert u_s\rangle\\
\Rightarrow&&\vert \psi_1\rangle &= \frac{1}{8}\sum^{63}_{j=0}\vert j\rangle \otimes \frac{1}{\sqrt 8}\sum^{7}_{s=0}\vert u_s\rangle\\
\Rightarrow&&\vert \psi_2\rangle &= \frac{1}{8\sqrt8}\sum^{7}_{s=0}\sum^{63}_{j=0}\vert j\rangle \otimes e^{2\pi i\frac{sj}{8}}\vert u_s\rangle\\
&&QFT_t\vert k\rangle &= \frac{1}{8}\sum^{63}_{j=0}e^{2\pi i\frac{kj}{64}}\vert j\rangle,\quad e^{2\pi i\frac{sj}{8}}=e^{2\pi i\frac{(8s)j}{64}}\\
&&\Rightarrow~~&\frac{1}{8}\sum^{63}_{j=0}e^{2\pi i\frac{sj}{8}}\vert j\rangle = \frac{1}{8}\sum^{63}_{j=0}e^{2\pi i\frac{(8s)j}{64}}\vert j\rangle=QFT_t\vert 8s\rangle\\
&&\Rightarrow~~&QFT^{-1}_t\big(QFT_t\vert 8s\rangle\big) = \vert 8s\rangle\\
\Rightarrow&&\vert \psi_3\rangle&=\big(QFT^{-1}_t\otimes I\big)\vert \psi_2\rangle = \frac{1}{\sqrt 8}\sum^{7}_{s=0}\vert 8s\rangle \otimes \vert u_s\rangle
\end{align*}</script><p>Measure the first register will randomly get $\{0, 8, 16, 24, 32, 40, 48, 56\}$</p>
<p>$\Rightarrow$ Solve $r=8$</p>
</body></html>]]></content>
      <categories>
        <category>Quantum Computing</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Course Note</tag>
        <tag>Quantum Computing</tag>
        <tag>Quantum Algorithm</tag>
        <tag>Quantum Circuits</tag>
      </tags>
  </entry>
  <entry>
    <title>[Note] Quantum Computation and Quantum Information - Chapter 6: Quantum search algorithms</title>
    <url>/Ending2015a/8802/</url>
    <content><![CDATA[<html><head></head><body><p>上課筆記，原文書:</p>
<blockquote>
<p>Quantum Computation and Quantum Information, Michael A. Nielsen & Isaac L. Chuang</p>
</blockquote>
<p><img src="https://i.imgur.com/LMzyRKv.png" alt></p>
<a id="more"></a>
<h2 id="6-1-The-quantum-search-algorithm" class="heading-control"><a href="#6-1-The-quantum-search-algorithm" class="headerlink" title="6.1 The quantum search algorithm"></a>6.1 The quantum search algorithm<a class="heading-anchor" href="#6-1-The-quantum-search-algorithm" aria-hidden="true"></a></h2><ul>
<li>Search marked items out of $N$ items<ul>
<li>Classical: $\Theta(N)$ operations</li>
<li>Quantum: $O(\sqrt N)$ operations, $\Omega(\sqrt N)$ matching lower bound</li>
</ul>
</li>
</ul>
<h3 id="6-1-1-The-oracle" class="heading-control"><a href="#6-1-1-The-oracle" class="headerlink" title="6.1.1 The oracle"></a>6.1.1 The oracle<a class="heading-anchor" href="#6-1-1-The-oracle" aria-hidden="true"></a></h3><ul>
<li>Use a blackbox $\tilde{O_f}$ to store information about a function $f: \{0, 1\}^k\to\{0,1\}$.<ul>
<li>$\tilde{O_f}\vert x\rangle\vert q\rangle = \vert x\rangle\vert f(x)\oplus q\rangle$</li>
<li>In the search problem, we think $f(x)=1$ as an marked item, i.e. we want to find an $x$ s.t. $f(x)=1$</li>
</ul>
</li>
<li>Quantum query complexity: number of $\tilde{O_f}$ you need to put in your circuit to solve the problem.</li>
<li>Assume $N$ items (indexed $\{0, 1,\dots, N-1\}$), exactly $M$ marked items (i.e. solutions) $1\le M \le N$.<ul>
<li>find any marked item: $O(\sqrt{\frac{N}{M}})$</li>
<li>find all marked item: $\Theta(\sqrt{MN})$</li>
<li>find first marked item: $O(\sqrt{N})$</li>
</ul>
</li>
</ul>
<h4 id="Phase-version-of-quantum-oracle" class="heading-control"><a href="#Phase-version-of-quantum-oracle" class="headerlink" title="Phase version of quantum oracle"></a>Phase version of quantum oracle<a class="heading-anchor" href="#Phase-version-of-quantum-oracle" aria-hidden="true"></a></h4><script type="math/tex; mode=display">
\begin{align*}
    \tilde{O_f}\vert x\rangle\vert -\rangle 
        &= \tilde{O_f}\vert x\rangle \frac{1}{\sqrt 2}(\vert 0\rangle-\vert 1\rangle)\\
        &= \begin{cases}
                \vert x\rangle\vert -\rangle, &f(x)=0\\
                -\vert x\rangle\vert -\rangle, & f(x)=1
            \end{cases}\\
        &= (-1)^{f(x)}\vert x\rangle\vert -\rangle
\end{align*}</script><ul>
<li>Using one $\tilde{O_f}$, we can simulate one $O_f$ s.t. <script type="math/tex; mode=display">
  O_f\vert x\rangle=(-1)^{f(x)}\vert x\rangle</script></li>
<li>We can also simulate $\tilde{O_f}$ with one controlled-$O_f$:<script type="math/tex; mode=display">
  CO_f\vert x\rangle\vert y\rangle=(-1)^{f(x)}\vert x\rangle\vert y\rangle</script><ul>
<li>$y$ is the control qubit</li>
<li>$\tilde{O_f}$ and $CO_f$ is equivalant</li>
</ul>
</li>
</ul>
<h3 id="6-1-2-The-procedure" class="heading-control"><a href="#6-1-2-The-procedure" class="headerlink" title="6.1.2 The procedure"></a>6.1.2 The procedure<a class="heading-anchor" href="#6-1-2-The-procedure" aria-hidden="true"></a></h3><p><img src="https://i.imgur.com/LMzyRKv.png" alt></p>
<ul>
<li><p>$G$ is the grover iteration, which composed of 4 steps:</p>
<ol>
<li>Apply Oracle $O_f$</li>
<li>Apply $H^{\otimes n}$</li>
<li>Apply an conditioned phase shift $(2\vert 0\rangle\langle 0\vert -I)$ (only $\vert 0\rangle$ get an extra phase of $-1$)<script type="math/tex; mode=display">
\vert x\rangle \to -(-1)^{\delta_{x0}}\vert x\rangle</script></li>
<li>Apply $H^{\otimes n}$<br><img src="https://i.imgur.com/QI4G07L.png" alt></li>
</ol>
</li>
<li><p>Step 3 is a reflection about $\vert 0\rangle$:</p>
<script type="math/tex; mode=display">
\vert x\rangle \to (2\vert 0\rangle\langle 0\vert-I)\vert x \rangle</script></li>
<li>Step 2 3 4 together is a reflection around $\vert \psi\rangle$<script type="math/tex; mode=display">
\begin{align*}
\vert \psi \rangle \equiv \frac{1}{\sqrt N}\sum^{N-1}_{x=0}\vert x\rangle&=H^{\otimes n}\vert 0\rangle\\
H^{\otimes n}(2\vert 0\rangle\langle 0\vert - I)H^{\otimes n} 
  &= 2\vert \psi\rangle\langle\psi\vert - (H^{\otimes n})^2\\
  &= 2\vert \psi\rangle\langle \psi\vert -I
\end{align*}</script></li>
<li>Step 2 3 4 no oracle calls, can be implemented efficiently $O(n)$ gates</li>
<li>$G=(2\vert \psi\rangle\langle\psi\vert -I)O_f$</li>
</ul>
<h3 id="6-1-3-Geometric-visualization" class="heading-control"><a href="#6-1-3-Geometric-visualization" class="headerlink" title="6.1.3 Geometric visualization"></a>6.1.3 Geometric visualization<a class="heading-anchor" href="#6-1-3-Geometric-visualization" aria-hidden="true"></a></h3><p>Suppose there are $M$ marked items (solutions), let $\vert \beta\rangle$ be the superposition of all marked items, $\vert \alpha\rangle$ be the superposition of non-marked items</p>
<script type="math/tex; mode=display">
\begin{align*}
\vert \beta\rangle &\equiv \frac{1}{\sqrt M} \sum_{x:f(x)=1} \vert x\rangle\\
\vert \alpha\rangle &\equiv \frac{1}{\sqrt {N-M}}\sum{x:f(x)=0} \vert x\rangle
\end{align*}</script><script type="math/tex; mode=display">
\Rightarrow \vert \psi\rangle = \sqrt{\frac{N-M}{N}}\vert \alpha\rangle + \sqrt{\frac{M}{N}}\vert \beta\rangle</script><ul>
<li>Consider the 2D space spanned by $\vert \psi\rangle$ and $\vert \beta\rangle$<ul>
<li>Oracle operation $O_f$ performs a reflection about the vector $\vert \alpha\rangle$ in the plane defined by $\vert \alpha\rangle$ and $\vert \beta\rangle$<script type="math/tex; mode=display">
O_f(a\vert \alpha\rangle+b\vert \beta\rangle)=a\vert\alpha\rangle-b\vert \beta\rangle</script></li>
<li>$(2\vert \psi\rangle\langle\psi\vert -I)$ performs a reflection about $\vert \psi\rangle$</li>
<li>$G$ perfoms a rotation. Both $O_f$ and $(2\vert \psi\rangle\langle\psi\vert -I)$ doesn’t bring vectors outside of this 2D plane.<script type="math/tex; mode=display">
G^k\vert \psi\rangle = \cos\left(\frac{2k+1}{2}\theta\right)\vert \alpha\rangle + \sin\left(\frac{2k+1}{2}\theta\right)\vert \beta\rangle</script><img src="https://i.imgur.com/McKd8nJ.png" alt></li>
<li>The Grover iteration can be written as<script type="math/tex; mode=display">
G=\left(\begin{matrix}\cos\theta & -\sin\theta\\\sin\theta & \cos\theta\end{matrix}\right)</script></li>
</ul>
</li>
<li>After that, get $\vert \beta\rangle=\frac{1}{\sqrt M}\sum_{x:f(x)=1}\vert x\rangle \Rightarrow$ measure to get random $x$</li>
</ul>
<h3 id="6-1-4-Performance" class="heading-control"><a href="#6-1-4-Performance" class="headerlink" title="6.1.4 Performance"></a>6.1.4 Performance<a class="heading-anchor" href="#6-1-4-Performance" aria-hidden="true"></a></h3><ul>
<li>How many iterations does Grover’s search need?<ul>
<li>$\displaystyle\langle \beta \vert \psi\rangle = \sqrt{\frac{M}{N}} = \sin\frac{\theta}{2}$</li>
<li>Assume $\displaystyle M\ll N~\Rightarrow \sin\frac{\theta}{2}\approx \frac{\theta}{2}~\Rightarrow~\theta \approx 2\sqrt{\frac{M}{N}} =\Omega(\sqrt{\frac{M}{N}})$</li>
<li>We want $\displaystyle\frac{2k+1}{2}\theta=\frac{\pi}{2}~\Rightarrow~ k=\frac{\pi}{2\theta}-\frac{1}{2}\approx \frac{\pi}{2\theta}\approx \frac{\pi}{4}\sqrt{\frac{M}{N}}\approx O(\sqrt{\frac{N}{M}})$</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/QHCmhbv.png" width="400px"></p>
<ul>
<li>The probability of getting a solution:<script type="math/tex; mode=display">
  Pr(\text{success}) = \displaystyle\sin^2\left((k+\frac{1}{2})\theta\right)</script></li>
<li>The probability of falure: <script type="math/tex; mode=display">
  \begin{align*}
  1-Pr(\text{success}) &=\displaystyle\cos^2\left((k+\frac{1}{2})\theta\right)\\
  &= \sin^2\left(\frac{\pi}{2}-(k+\frac{1}{2})\theta\right)\\
  &= \sin^2\vert\delta\vert\le \sin^2\frac{\theta}{2}=\frac{M}{N}
  \end{align*}</script></li>
<li>This algorith requires you to know $M$. Otherwise, you might <strong>overshoot</strong>.</li>
<li>If you know $M$, it is possible to make success probability $100\%$. The idea is to make $\theta$ a little smaller so $k=\frac{\pi}{2\theta}-\frac{1}{2}$ is an integer</li>
<li>If we don’t know $M$, we can do <strong>quantum counting</strong> to estimate $M$ first.</li>
</ul>
<h2 id="6-2-Quantum-search-as-a-quantum-simulation" class="heading-control"><a href="#6-2-Quantum-search-as-a-quantum-simulation" class="headerlink" title="6.2 Quantum search as a quantum simulation"></a>6.2 Quantum search as a quantum simulation<a class="heading-anchor" href="#6-2-Quantum-search-as-a-quantum-simulation" aria-hidden="true"></a></h2><blockquote>
<p>待補</p>
</blockquote>
<h2 id="6-3-Quantum-Counting" class="heading-control"><a href="#6-3-Quantum-Counting" class="headerlink" title="6.3 Quantum Counting"></a>6.3 Quantum Counting<a class="heading-anchor" href="#6-3-Quantum-Counting" aria-hidden="true"></a></h2><ul>
<li>Estimate the number of solutions $M$ from $N$ items with a probability of success at least $1-\varepsilon$<ul>
<li>Classical: $\Omega(\frac{1}{\varepsilon^2}\frac{N}{M})$</li>
<li>Quantum: $O(\frac{1}{\varepsilon}\sqrt{\frac{N}{M}})$ queries, for some $\varepsilon$ multiplicative error.</li>
</ul>
</li>
<li>Idea: apply phase estimation to Grover iteration $G$<script type="math/tex; mode=display">
  G=\left(\begin{matrix}\cos\theta & -\sin\theta\\\sin\theta & \cos\theta\end{matrix}\right)</script><ul>
<li>Have eigenvalues $e^{i\theta}$ and $e^{i(2\pi-\theta)}$.</li>
<li>Estimate $\theta$ to $m$ bits of accuracy ($\vert\Delta\theta\vert\le2^{-m}$) with a probability of success at least $1-\varepsilon$. </li>
</ul>
</li>
</ul>
<p>Preparation:</p>
<ul>
<li>Assume the search space is augmented $N\to2N$ for convenient, $\sin^2(\theta/2)=M/2N$. </li>
<li>Use $t=m+\lceil\log(2+\frac{1}{2\varepsilon})\rceil$ bits.</li>
<li>$\vert \psi\rangle=\sum_x\vert x\rangle$ as input to oracle (superposition of all possible states)<br><img src="https://i.imgur.com/sgs8t7h.png" alt></li>
</ul>
<div class="note success">
            <p><em>Proof</em></p><p>Recall that $\sin^2\frac{\theta}{2}=\frac{M}{2N}$, we estimate $M$ by estimating $\theta$ (assume $\theta<\frac{\pi}{2}$)</p><script type="math/tex; mode=display">\begin{align*}\frac{\vert \Delta M\vert}{2N}     &= \left\vert \sin^2\frac{(\theta+\Delta\theta)}{2}-\sin^2\frac{\theta}{2} \right\vert\\    &= \left(\sin\frac{(\theta+\Delta\theta)}{2}+\sin\frac{\theta}{2}\right)\left\vert \sin\frac{(\theta+\Delta\theta)}{2}-\sin\frac{\theta}{2} \right\vert\end{align*}</script><p>Two parts:</p><script type="math/tex; mode=display">\begin{align*}\left\vert \sin\frac{(\theta+\Delta\theta)}{2}-\sin\frac{\theta}{2} \right\vert    &= \left\vert \cos\left(\frac{\theta}{2}+\frac{\Delta\theta}{4}\sin\frac{\Delta\theta}{4}\right) \right\vert \\    &\le \frac{\vert \Delta\theta\vert}{4}\le\frac{\vert \Delta\theta\vert}{2}\\\left\vert \sin\left(\frac{\theta+\Delta\theta}{2}\right)\right\vert    &= \left\vert \sin\frac{\theta}{2}\cos\frac{\Delta\theta}{2}+\sin\frac{\Delta\theta}{2}\cos\frac{\theta}{2} \right\vert\\    &\le \sin\frac{\theta}{2}+\frac{\vert\Delta\theta\vert}{2}\end{align*}</script><p>Implies:</p><script type="math/tex; mode=display">\begin{align*}\Rightarrow &&\frac{\vert\Delta M\vert}{2N}    &\le (2\sin\frac{\theta}{2}+\frac{\vert\Delta\theta\vert}{2})\frac{\vert\Delta\theta\vert}{2}\\    &&&\le \left(\sqrt{\frac{2M}{N}}+2^{-(m+1)}\right)\frac{2^{-m}}{2}\\\Rightarrow && \vert \Delta M\vert     &\le (\sqrt{2MN}+\frac{N}{2^{m+1}})2^{-m}\end{align*}</script><ul><li>Pick $\vert \Delta M\vert=\varepsilon M~\Rightarrow~ \varepsilon M\approx 2^{-m}\sqrt{MN}~\Rightarrow~2^m\approx\frac{1}{\varepsilon}\sqrt{\frac{N}{M}}$.</li><li>Use $t=m+\lceil\log(2+\frac{1}{2\delta})\rceil$ bits, need $2^0+2^1+…+2^{t-1}=2^t-1$ Grover iterations for phase estimation $\Rightarrow$ number of quries (oracle calls) $\sim 2^t$<script type="math/tex; mode=display">  2^t=2^m\cdot 2^{\lceil\log(2+\frac{1}{2\delta})\rceil}\approx \frac{1}{\varepsilon}\sqrt{\frac{N}{M}}\left(2+\frac{1}{2\delta}\right)=O(\frac{1}{\varepsilon}\sqrt\frac{N}{M})</script></li></ul>
          </div>
<h2 id="6-4-Speeding-up-the-solution-of-NP-complete-problems" class="heading-control"><a href="#6-4-Speeding-up-the-solution-of-NP-complete-problems" class="headerlink" title="6.4 Speeding up the solution of NP-complete problems"></a>6.4 Speeding up the solution of NP-complete problems<a class="heading-anchor" href="#6-4-Speeding-up-the-solution-of-NP-complete-problems" aria-hidden="true"></a></h2><blockquote>
<p>待補</p>
</blockquote>
<h2 id="6-5-Quantum-search-of-an-unstructured-database" class="heading-control"><a href="#6-5-Quantum-search-of-an-unstructured-database" class="headerlink" title="6.5 Quantum search of an unstructured database"></a>6.5 Quantum search of an unstructured database<a class="heading-anchor" href="#6-5-Quantum-search-of-an-unstructured-database" aria-hidden="true"></a></h2><blockquote>
<p>待補</p>
</blockquote>
<h2 id="6-6-Optimality-of-the-search-algorithm" class="heading-control"><a href="#6-6-Optimality-of-the-search-algorithm" class="headerlink" title="6.6 Optimality of the search algorithm"></a>6.6 Optimality of the search algorithm<a class="heading-anchor" href="#6-6-Optimality-of-the-search-algorithm" aria-hidden="true"></a></h2><ul>
<li>No quantum algorithm can perform oracle search using fewer than $\Omega(\sqrt N)$ quries.</li>
<li>Idea: adversary method<ul>
<li>Quantum query algorithms can be viewed as<br>  <img src="https://i.imgur.com/uFy7hCx.png" alt></li>
<li>Consider the distance between $\vert \psi^f_k\rangle$ and $\vert \psi^{f’}_k\rangle$ for some function $f$ and $f’$<script type="math/tex; mode=display">
  \left\Vert \vert \psi^f_k\rangle-\vert \psi^{f'}_k\rangle \right\Vert^2</script></li>
<li>Start with zer odistance $\vert \psi^f_0\rangle=U_0\vert 0\rangle, ~\forall~f$</li>
<li>For some $(f, f’)$ pairs, $\vert \psi^f_k\rangle$ and $\vert \psi^{f’}_k\rangle$ are distinguishable $\Rightarrow$ const distance.<ul>
<li>for example, $f(x)=0~\forall x$ and $\exists x$ s.t. $f’(x)=1$</li>
</ul>
</li>
<li>In the middle, note that $U_t$ does not change the distance<script type="math/tex; mode=display">
  \begin{align*}
  \left\Vert U\vert a\rangle - U\vert b\rangle \right\Vert
      &= \left\Vert U(\vert a\rangle-\vert b\rangle) \right\Vert^2\\
      &= (\langle a\vert -\langle b\vert)U^\dagger U(\vert a\rangle-\vert b\rangle)\\
      &= \left\Vert \vert a\rangle - \vert b\rangle \right\Vert^2
  \end{align*}</script></li>
<li>For a specific pair of $(f, f’)$, $O_f$ and $O_{f’}$ might change the distance between $\vert \psi^f_t\rangle$ and $\vert \psi^{f’}_{t}\rangle$ a lot. Eg. $\vert \psi^f_t\rangle=\vert \psi^{f’}_t=\vert x\rangle$ for some $x$ s.t. $f(x)=0$ and $f’(x)=1$ $\Rightarrow$ distance changed by const.</li>
<li>So we sum over different pairs of $(f, f’)$, such that the sum of distance cannot increase a lot.</li>
<li>Sum of distance start with zero, end with something big, cannot change too much each step $\Rightarrow$ need many steps.</li>
</ul>
</li>
</ul>
<p>Way to proof:</p>
<ul>
<li>Consider <script type="math/tex; mode=display">
  \begin{align*}
  f_0 &= \text{all zeros}&\\
  f_x &= \begin{cases}f(x)=1\\f(y)=0, &\forall y\ne x\end{cases}
  \end{align*}</script>  we pair between $(f_0, f_x)~~\forall x\in\{1,2,\dots, N\}$</li>
<li>Use the phase version of the oracle (ignore control, ignore ancilla)<script type="math/tex; mode=display">
  O_{f_0}=I, \quad O_{f_x}\vert y\rangle=(-1)^{\delta_{xy}}\vert y\rangle = (I-2\vert x\rangle\langle x\vert)\vert y\rangle</script><ul>
<li>Define $O_x\equiv O_{f_x}=I-2\vert x\rangle\langle x\vert$</li>
</ul>
</li>
<li>Define <script type="math/tex; mode=display">
  \begin{align*}
  \vert \psi^x_k\rangle &\equiv\vert \psi^{f_x}_k\rangle=U_kO_xU_{k-1}O_x\cdots U_1O_xU_0\vert 0\rangle\\
  \vert \psi_k\rangle &\equiv\vert \psi^{f_0}_k\rangle=U_kU_{k-1}\cdots U_0\vert 0\rangle
  \end{align*}</script></li>
<li>Define “Progress function” $D_k$, the deviation after $k$ steps (simplified notations)<script type="math/tex; mode=display">
  \begin{align*}
  D_k\equiv \sum^N_{x=1}\left\Vert\psi^x_k-\psi_k\right\Vert^2
  \end{align*}</script></li>
<li>Need to prove two parts<ol>
<li>$D_k \le O(k^2)$ (Induction)</li>
<li>$D_k=\Omega(N)$ to success</li>
</ol>
</li>
<li>Finally $\Rightarrow~k=\Omega(\sqrt N)$</li>
</ul>
<div class="note success">
            <p><em>Proof</em> $D_k\le4k^2$</p><p>By induction</p><p>Hypothesis: $D_k\le4k^2$</p><p>Base case: $k=0$, $D_k=0$</p><p>Induction step:</p><script type="math/tex; mode=display">\begin{align*}D_{k+1} &= \sum_x \left\Vert U_{k+1}O_x\psi^x_k-U_{k+1}\psi_k \right\Vert^2\\    &= \sum_x\left\Vert O_x\psi^x_k-\psi_k \right\Vert^2\\    &= \sum_x\big\Vert \underbrace{O_x(\psi^x_k-\psi_k)}_{b}+\underbrace{(O_x-I)\psi_k}_{c} \big\Vert^2\end{align*}</script><p>Applying $\left\Vert b+c\right\Vert^2\le \left\Vert b\right\Vert^2 + 2\left\Vert b\right\Vert \left\Vert c\right\Vert+\left\Vert c\right\Vert^2$ with $b\equiv O_x(\psi^x_k-\psi_k)$ and $c\equiv (O_x-I)\psi_k=-2\vert x\rangle\langle x\vert\psi_k\rangle=-2\langle x\vert \psi_k\rangle\vert x\rangle$</p><script type="math/tex; mode=display">D_{k+1}\le \sum_x\left(\underbrace{\left\Vert \psi^x_k-\psi_k\right\Vert^2}_{D_k} +4\underbrace{\left\Vert \psi^x_k-\psi_k \right\Vert\langle x\vert \psi_k\rangle}_{\text{Cauchy}}+4\underbrace{\left\vert \langle \psi_k\vert x\rangle \right\vert^2}_{1} \right)</script><p>Because $\sum_x\left\vert \langle\psi_k\vert x\rangle \right\vert^2=\sum_x\langle\psi_k\vert x\rangle\langle x\vert \psi_k\rangle=\langle \psi_k\vert \underbrace{\left(\sum_x\vert x\rangle\langle x\vert\right)}_{I}\vert\psi_k\rangle=1$. By Cauchy’s inequality $\sum_x a_xb_x\le \sqrt{\sum_x a^2_x}\sqrt{\sum_x b^2_x}$.</p><script type="math/tex; mode=display">\begin{align*}D_{k+1} &\le D_k +4{\textstyle\sqrt{\sum_x\left\Vert \psi^x_k-\psi_k \right\Vert^2}\sqrt{\sum_x\left\vert \langle \psi_k\vert x\rangle \right\vert^2} }+4\\    &\le D_k + 4\sqrt{D_k} + 4\end{align*}</script><p>By induction hypothesis $D_k\le 4k^2$</p><script type="math/tex; mode=display">D_{k+1}\le 4k^2 +4\cdot 2k + 4 = 4(k+1)^2</script><p>Hence, $D_k\le 4k^2$.</p>
          </div>
<div class="note success">
            <p><em>Proof</em> $D_k=\Omega(N)$</p><p>Assume $\exists$ projector $P$ s.t. $\left\Vert P\vert\psi_k\rangle \right\Vert^2\le \frac{1}{9}, ~\left\Vert P\vert\psi^x_k\rangle \right\Vert^2\le \frac{8}{9}, ~\forall x$. (some magic number: distinguish prob. $\approx 1/3$)</p><p><em>Claim</em>: if there $\exists~p$ s.t. $\left\Vert P\vert a\rangle \right\Vert^2\le \frac{1}{9}, ~\left\Vert P\vert b\rangle \right\Vert^2\le \frac{8}{9}~\Rightarrow~\left\Vert\vert a\rangle-\vert b\rangle\right\Vert^2\ge\frac{2}{3}$</p><p><em>Proof</em>: Note that $\left\Vert(I-P)\vert b\rangle\right\Vert^2\le \frac{1}{9}$.</p><p>Consider $\vert \langle a\vert b\rangle\vert$</p><script type="math/tex; mode=display">\begin{align*}\vert \langle a\vert b\rangle\vert    &= \left\vert \langle a\vert (I-P)+p \vert b\rangle \right\vert\\    &\le \vert \langle a \vert (I-P)\vert b\rangle \vert + \vert \langle a\vert P\vert b\rangle\vert\\    &\le \left\Vert a \right\Vert \left\Vert (I-P)b \right\Vert + \left\Vert a(P) \right\Vert \left\Vert b \right\Vert\\    &\le 1\cdot \frac{1}{3}+1\cdot \frac{1}{3} =\frac{2}{3}\end{align*}</script><p>Thus</p><script type="math/tex; mode=display">\begin{align*}\left\Vert \vert a\rangle-\vert b\rangle \right\Vert^2    &= \left\Vert a\right\Vert^2+\left\Vert b\right\Vert^2 -\langle a\vert b\rangle -\langle b\vert a\rangle\\    &= 2 - 2\vert \langle a\vert b\rangle\vert\ge 2(1-\frac{2}{3})=\frac{2}{3}\end{align*}</script><p>Let $\vert a\rangle = \vert \psi_k\rangle$ and $\vert b\rangle=\vert \psi^x_k\rangle$</p><script type="math/tex; mode=display">\begin{align*}D_k&=\sum_x\left\Vert \vert \psi_k\rangle-\vert \psi^x_k\rangle\right\Vert^2 \ge \sum_x\frac{2}{3} = \frac{2N}{3}\\\Rightarrow D_k&=\Omega(N)\end{align*}</script>
          </div>
<div class="note success">
            <p><em>Proof</em> $k=\Omega(\sqrt N)$</p><p>By $D_k\le 4k^2$ and $D_k\ge\frac{2N}{3}$</p><script type="math/tex; mode=display">\frac{2N}{3}\le D_k\le 4k^2~\Rightarrow~k\ge\sqrt{\frac{N}{6}}=\Omega(\sqrt N)</script>
          </div>
<ul>
<li>This can be generalized to adversary method<ul>
<li>Weight on different pairs</li>
<li>Can put negative weight on some pairs. negative weight adversary bound is optimal.</li>
</ul>
</li>
<li>State distinguishment (Ch. 9)<ul>
<li>Trace distance: $D_\text{tr}(\rho, \sigma)=\frac{1}{2}\text{tr}\vert\rho-\sigma\vert$, maximum distinguish prob.</li>
<li>Fidelity: $F(\psi, \phi)=\vert\langle\psi\vert\phi\rangle\vert$</li>
<li>For pure state: $D_\text{tr}(\psi, \phi)^2+F(\psi, \phi)^2=1$</li>
</ul>
</li>
</ul>
<h2 id="6-7-Black-box-algorithm-limits" class="heading-control"><a href="#6-7-Black-box-algorithm-limits" class="headerlink" title="6.7 Black box algorithm limits"></a>6.7 Black box algorithm limits<a class="heading-anchor" href="#6-7-Black-box-algorithm-limits" aria-hidden="true"></a></h2><blockquote>
<p>待補</p>
</blockquote>
<h2 id="Bonus" class="heading-control"><a href="#Bonus" class="headerlink" title="Bonus"></a>Bonus<a class="heading-anchor" href="#Bonus" aria-hidden="true"></a></h2><blockquote>
<p>待補</p>
</blockquote>
<h3 id="Better-bounds-on-distinguish-quantum-states" class="heading-control"><a href="#Better-bounds-on-distinguish-quantum-states" class="headerlink" title="Better bounds on distinguish quantum states"></a>Better bounds on distinguish quantum states<a class="heading-anchor" href="#Better-bounds-on-distinguish-quantum-states" aria-hidden="true"></a></h3><p>Suppose we have two quantum states $\vert a\rangle$, $\vert b\rangle$, projective measurements $\{P, I-P\}$ s.t. $\left\Vert P\vert a\rangle \right\Vert^2 \le \varepsilon$, $\left\Vert P\vert b\rangle \right\Vert^2\ge 1-\varepsilon$, $~\varepsilon\in [0, \frac{1}{2}]$.</p>
<p>Claim: $\vert\langle a\vert b\rangle\vert\le 2\sqrt{\varepsilon(1-\varepsilon)}$</p>
<div class="note success">
            <p><em>Proof</em> $\vert\langle a\vert b\rangle\vert\le 2\sqrt{\varepsilon(1-\varepsilon)}$</p><p><strong>Proof 1</strong></p><script type="math/tex; mode=display">\begin{align*}\vert \langle a \vert b \rangle \vert &= \vert \langle a \vert P+(I-P)\vert b\rangle\vert\\    &= \vert \langle a\vert P^2+(I-P)^2\vert b\rangle\vert\\    &\le \vert \langle a \vert PP\vert b\rangle \vert + \vert \langle a\vert (I-P)(I-P)\vert b\rangle\vert\\    &\le\big\Vert \underbrace{P\vert a\rangle}_{x_1}\big\Vert \big\Vert \underbrace{P\vert b\rangle}_{x_2}\big\Vert + \big\Vert\underbrace{(I-P)\vert b\rangle}_{y_1}\big\Vert \big\Vert\underbrace{(I-P)\vert a\rangle}_{y_2}\big\Vert\\\end{align*}</script><p>By Cauchy theory $x_1x_2+y_1y_2\le \sqrt{x_1^2+x_2^2}+\sqrt{y_1^2+y_2^2}$</p><script type="math/tex; mode=display">\begin{align*}    \vert \langle a \vert b \rangle \vert &\le \sqrt{x_1^2+x_2^2} + \sqrt{y_1^2+y_2^2}\\\end{align*}</script><p>By</p><script type="math/tex; mode=display">\begin{cases}x_1^2+y_2^2=1,&x_1^2\le \varepsilon\\x_2^2+y_1^2=1,&y_1^2\le \varepsilon\end{cases}</script><script type="math/tex; mode=display">\vert \langle a \vert b \rangle \vert \le \sqrt{(x_1^2+y_1^2)(2-(x_1^2+y_1^2))}</script><p>By $x\le2\varepsilon$</p><script type="math/tex; mode=display">\vert \langle a \vert b \rangle \vert \le \sqrt{2\varepsilon(2-2\varepsilon)}=2\sqrt{\varepsilon(1-\varepsilon)}</script><p><strong>Proof 2</strong></p><p>The best way to distingish $\vert a\rangle$ and $\vert b\rangle$: $\vert b\rangle \sim \vert 0\rangle$ and $\vert a\rangle \sim \vert 1\rangle$</p><p><img src="https://i.imgur.com/H0PpikY.png" width="300px"></p><script type="math/tex; mode=display">\begin{align*}\text{error} = \sin^2\delta \le \varepsilon\end{align*}</script><script type="math/tex; mode=display">\begin{align*}\Rightarrow~\vert \langle a\vert b\rangle &= \vert \cos(\frac{\pi}{2} - 2\delta)\vert\\    &= \vert \sin(2\delta)\vert \\    &= 2\vert \sin\delta \vert \vert\cos\delta\vert = 2\sqrt{\varepsilon(1-\varepsilon)}\end{align*}</script><p><strong>Proof 3</strong><br>Let $T$ be the trace distance between $\vert a\rangle, \vert b\rangle$, $F$ be the Fidelity between $\vert a\rangle$, $\vert b\rangle$.</p><ul><li>Fact 1: $T\ge$ bias betweem measurement probability $\ge(1-\varepsilon)-\varepsilon=1-2\epsilon$<ul><li>$|P\vert b\rangle|^2\ge 1-\varepsilon$</li><li>$|P\vert a\rangle|^2 \le \varepsilon$</li></ul></li><li>Fact 2: For $\vert a\rangle$ and $\vert b\rangle$ pur states<ul><li>$T^2+F^2=1$</li><li>$F=|\langle a\vert b\rangle|=\sqrt{1-T^2}\le \sqrt{1-(1-2\varepsilon)^2}=2\sqrt{\varepsilon(1-\varepsilon)}$</li></ul></li></ul>
          </div>
<h3 id="Applications-of-Grover’s-algorithm" class="heading-control"><a href="#Applications-of-Grover’s-algorithm" class="headerlink" title="Applications of Grover’s algorithm"></a>Applications of Grover’s algorithm<a class="heading-anchor" href="#Applications-of-Grover’s-algorithm" aria-hidden="true"></a></h3><h4 id="Speed-up-for-NP-complete-problems" class="heading-control"><a href="#Speed-up-for-NP-complete-problems" class="headerlink" title="Speed up for $NP$-complete problems"></a>Speed up for $NP$-complete problems<a class="heading-anchor" href="#Speed-up-for-NP-complete-problems" aria-hidden="true"></a></h4><ul>
<li>Satisfiability: given boolean formula $\phi(i_1,i_2,\dots,i_n)$, determine whether there exist $i=i_1i_2\dots i_n$ s.t. $\phi(i)=1$ ($\phi(i)$ is easy to compute, but there are $N=2^n$ possible assignment of $i$)<ul>
<li>Quantum alg: construct $q$ circuit $Q_\phi$ s.t. $\vert i\rangle\to(-1)^{\phi(i)}\vert i\rangle$ ($\vert i,b\rangle\to\vert i,b\oplus \phi(i)\rangle$)</li>
<li>$O_\phi$ is easy to construct because $\phi(i)$ is easy to compute<ul>
<li>$\Rightarrow$ use $O_\phi$ as the oracle in Grover’s search</li>
<li>$\Rightarrow$ determine whether $\exists~i,$ s.t. $\phi(i)=1$ in $O(\sqrt{2^n}~\text{cost of}~O_\phi)\sim 2^{m/2}$ time</li>
</ul>
</li>
</ul>
</li>
<li>However, not always getting quadratic speedup to $NP$ complete problem. Because $\sqrt{\text{num of assignment}}$ might be slower than known classical alg.<ul>
<li>e.g. Travelling Salesmans Problem (TSP) on $n$<ul>
<li>number of assignment $=n!\sim n^n$</li>
<li>Classical alg $\sim O(2^n)$</li>
<li>Best quantum algorithm $\sim O(1.728^n)$ non-trivial application of Grover’s search</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="OR-of-AND" class="heading-control"><a href="#OR-of-AND" class="headerlink" title="$OR$ of $AND$"></a>$OR$ of $AND$<a class="heading-anchor" href="#OR-of-AND" aria-hidden="true"></a></h4><ul>
<li>A boolean matrix<script type="math/tex; mode=display">
  \left[\begin{matrix}
  0 & 0 & 0 & 1 & 0 & 0\\
  1 & 1 & 1 & 0 & 1 & 1\\
  1 & 0 & 1 & 1 & 1 & 1\\
  1 & 1 & 1 & 1 & 1 & 1\\
  1 & 0 & 0 & 0 & 0 & 0\\
  0 & 0 & 1 & 0 & 1 & 0\\
  \end{matrix}\right]\xrightarrow{\text{row-wise}~AND}
  \left[\begin{matrix}
  0\\0\\0\\1\\0\\0\\
  \end{matrix}\right]\xrightarrow{OR}1</script></li>
<li>Question: is there a row with all $1$’s?<ul>
<li>Classical: easy to see need $\Omega(N)$ queries</li>
<li>Quantum alg 1: Grover search each row for an $0$<ul>
<li>time for each row $\sqrt{\sqrt{N}}\sim N^{1/4}$ (times $\log N$ because we want $1/N$ error)</li>
<li>total time<br>  $=\text{num of rows}\times\text{time for each row}=\sqrt{N}\cdot N^{1/4}\cdot \log N=N^{3/4}\cdot\log N$</li>
</ul>
</li>
<li>Quantum alg 2: run Grover’s search recursively, have an outer Grover that search through rows, and a row is “marked” iff the inner Grover cannot find zero in it<ul>
<li>total running time $\sqrt{\sqrt{N}}\cdot\sqrt{\sqrt{N}}\cdot\log N=\sqrt{N}\log N$ (quadratic speedup)</li>
<li>$\Omega(\sqrt{N})$ lower bound</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="AND-OR-tree" class="heading-control"><a href="#AND-OR-tree" class="headerlink" title="$AND$-$OR$ tree"></a>$AND$-$OR$ tree<a class="heading-anchor" href="#AND-OR-tree" aria-hidden="true"></a></h4><p><img src="https://i.imgur.com/FBpOCas.png" width="400px"></p>
<ul>
<li>Game tree for chess or Go etc.<ul>
<li>$1$: win, $0$: lose (ignore tie)</li>
<li>$OR$: Is there a move that makes me win?</li>
<li>$AND$: Is there a move that makes me lose?</li>
</ul>
</li>
<li>Recursive Grover? Don’t actually work because error builds up through recursion</li>
<li>Still has $O(\sqrt N)$ quantum alg. through quantum walk</li>
</ul>
<h4 id="Collision-problem" class="heading-control"><a href="#Collision-problem" class="headerlink" title="Collision problem"></a>Collision problem<a class="heading-anchor" href="#Collision-problem" aria-hidden="true"></a></h4><ul>
<li>Given blackbox access to the function $f:\{1,2,\dots,N\}\to\{1,2,\dots,N\}$</li>
<li>Promise: $f$ is either one-to-one or two-to-one (has lots of collision)<ul>
<li>e.g. one-to-one $f:1\to1,~2\to2,~3\to4,~4\to3$</li>
<li>e.g. two-to-one $f:1\to1,~2\to4,~3\to1,~4\to4$</li>
</ul>
</li>
<li>Problom: decide whether $f$ is one-to-one or two-to-one<ul>
<li>Classical $\Theta(\sqrt N)$<ul>
<li>Birthday attach: random pair $O(1/N)$ probability to be a collision.</li>
<li>If we pick $O(\sqrt N)$ items, there are $O(N)$ pairs $\Rightarrow$ const probability finding a collision in those pairs.</li>
</ul>
</li>
<li>Quantum alg 1: $O(\sqrt N)$ queries (no speedup)<ul>
<li>query $f(1)$, Grover search for an $x\ne1$ s.t. $f(x)=f(1)$</li>
</ul>
</li>
<li>Quantum alg 2: $\Omega(N^{1/3})$ lowerbound<ul>
<li>pick $N^{1/3}$ random elements $S_1$ query them classically and write down the answers</li>
<li>pick another $N^{2/3}$ elements $S_2$ (don’t query them yet). Grover search through $y\in S_2$, an item $y\in S_2$ is marked iff $\exists~x\in S_1$ s.t. $f(x)=f(y)$</li>
<li>Since we already reordered everything in $S_1$ only need one query $f(y)$ to determined whether $y$ is marked.</li>
<li>Total number of queries: $N^{1/3}+\sqrt{N^{2/3}}=O(N^{1/3})$</li>
<li>Number of pirs searched: $N^{1/3}\cdot N^{2/3}\sim O(N)\Rightarrow$ const success probability</li>
<li>with smart data structure can also do $\tilde{O}(N^{1/3})$ time.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Element-distinctness" class="heading-control"><a href="#Element-distinctness" class="headerlink" title="Element distinctness"></a>Element distinctness<a class="heading-anchor" href="#Element-distinctness" aria-hidden="true"></a></h4><ul>
<li>also $f:\{1,2,\dots,N\}\to\{1,2,\dots,N\}$, no promise</li>
<li>Problem: determine whether $f$ is one-to-one (ie. whether $f$ have collision)<ul>
<li>Classical: $\Theta(N)$</li>
<li>Quantum alg 1: $O(N^{3/4})$<ul>
<li>Divide the inputs into $\sqrt{N}$ blocks of size $\sqrt{N}$</li>
<li>Outer Grover search through each block $s$ and query all elements in it (check collision inside)</li>
<li>Inner Grover search between pair ($x\in s, y\in s$)</li>
<li>If $s$ contain an element of the collision, we find it $\Rightarrow$ prob $\frac{1}{\sqrt N}$</li>
<li>Total queries: $\sqrt{\sqrt N}\cdot \sqrt{N}=O(N^{3/4})$</li>
</ul>
</li>
<li>Quantum alg 2: $O(N^{2/3})$ by quantum walk<ul>
<li>$\Omega(N^{2/3})$ lower bound by reduction from collision (pick $\sqrt{N}$ elements)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Exact-Grover-search" class="heading-control"><a href="#Exact-Grover-search" class="headerlink" title="Exact Grover search"></a>Exact Grover search<a class="heading-anchor" href="#Exact-Grover-search" aria-hidden="true"></a></h3><ul>
<li>Promised that an unique marked item, find alg. that find the marked item with prob $1$, in $O(\sqrt N)$ steps.<script type="math/tex; mode=display">
  f:\{1,2,\dots,N\}\to\{0,1\},\quad \exists~x^* ~\text{s.t.}~ f(x)\begin{cases}
  1, &x=x^*\\
  0, &\text{otherwise}
  \end{cases}</script></li>
<li>Define $f’:\{1,\dots,2N\}\to\{0,1\}$<script type="math/tex; mode=display">
  f'(x)=\begin{cases}
      f'(x)=f(x), &\text{if}~1\le x\le N\\
      f'(x)=0, &\text{if}~N+1\le x\le 2N
  \end{cases}</script></li>
<li>can simulate $O_{f’}$ with one $\tilde{O_f}$<script type="math/tex; mode=display">
  \tilde{O}_f\vert x,b\rangle = \vert x,f(x)\oplus b\rangle</script>  <img src="https://i.imgur.com/qJk8fnu.png" alt><ul>
<li>define <script type="math/tex; mode=display">
  U_\gamma=\left(\begin{matrix}\cos\gamma & -\sin\gamma \\ \sin\gamma & \cos\gamma\end{matrix}\right),\quad A=H^{\otimes n}\otimes U_\gamma</script></li>
<li>We have <script type="math/tex; mode=display">
  A\vert 0\rangle^{\otimes n+1}=\frac{1}{\sqrt N}\sum^N_{x=1}\vert x\rangle \otimes (\cos\gamma\vert 0\rangle+\sin\gamma\vert 1\rangle)</script></li>
<li>The probability that $A\vert 0\rangle^{\oplus n+1}$ is marked<script type="math/tex; mode=display">
  \begin{align*}
  \text{Prob}(A\vert 0\rangle^{\oplus n+1}~\text{marked})
      &= \left|\langle x^*,0|A|0\rangle\right|^2\\
      &= \frac{1}{N} \cos^2\gamma
  \end{align*}</script>  <img src="https://i.imgur.com/W6cIhAs.png" alt><script type="math/tex; mode=display">
  \sin\theta_\gamma=\frac{\cos\theta_\gamma}{\sqrt N},\quad \text{write}~\lceil\tilde k\rceil=k</script><script type="math/tex; mode=display">
  \begin{align*}
  (2k+1)\theta_\gamma=\frac{\pi}{2} 
      &\Rightarrow \theta_\gamma =\frac{\pi}{2(2k+1)}\\
      &\Rightarrow \frac{\cos\gamma}{\sqrt N}=\sin\frac{\pi}{2(2k+1)}\quad(\text{let}~\theta_\gamma\approx\gamma)\\
      &\Rightarrow \gamma=\cos^{-1}\left(\sqrt N\cdot \sin\frac{\pi}{2(2\lceil \tilde k\rceil+1)}\right)
  \end{align*}</script></li>
</ul>
</li>
</ul>
<h3 id="Omega-sqrt-frac-N-M-query-lower-bound-for-M-marked-items" class="heading-control"><a href="#Omega-sqrt-frac-N-M-query-lower-bound-for-M-marked-items" class="headerlink" title="$\Omega(\sqrt{\frac{N}{M}})$ query lower bound for $M$ marked items"></a>$\Omega(\sqrt{\frac{N}{M}})$ query lower bound for $M$ marked items<a class="heading-anchor" href="#Omega-sqrt-frac-N-M-query-lower-bound-for-M-marked-items" aria-hidden="true"></a></h3><ul>
<li>easy solution<br>  $Q($search $M$ marked items from $N$ items $)$<br>  $\ge Q($search $M$ marked items from $N$ items promised that marked items are blocks$)$<br>  $\ge Q($search one marked item in $\frac{N}{M}$ items$)$<br>  $=\Omega(\sqrt{\frac{N}{M}})$</li>
<li>harder solution<ul>
<li>consider all $C^N_M$ ways to mark $M$ items</li>
<li>Let $S\subset\{1,2,\dots,N\}$ with size $M$. $C^N_M$ possible $S$<script type="math/tex; mode=display">
  D_k=\sum_{\substack{S:S\le[N]\\\vert S\vert=M}} \vert\psi^S_k-\psi\vert^2,\quad D_T=\Omega(C^N_M)</script></li>
<li>By induction<script type="math/tex; mode=display">
  \begin{align*}
  D_{k+1}&=\sum_S\vert O_S\psi^S_k-\psi_k\vert^2\\
      &=\sum_S\big\vert \underbrace{O_S(\psi^S_k-\psi)}_{\equiv~ b_S}+\underbrace{(O_S-I)\psi_k)}_{\equiv ~c_S}\big\vert^2\\
      &\le \sum_S\vert b_S\vert^2+2\vert b_S\vert\vert c_S\vert+\vert c_S\vert^2
  \end{align*}</script><script type="math/tex; mode=display">
  (O_S-I)=-2\sum_{x\in S}\vert x\rangle\langle x\vert, \quad (O_S-I)\vert \psi_k\rangle=-2\sum_{x\in S}\vert x\rangle\langle x\vert \psi_k\rangle</script><script type="math/tex; mode=display">
  \begin{align*}
  \sum_S\vert c_S\vert^2
      &= 4\sum_S\left\|\sum_{x\in S}\vert x\rangle\langle x\vert \psi_k\rangle\right\|^2\\
      &=4\sum_S\sum_{x\in S}\left\vert \langle x\vert \psi_k\rangle\right\vert^2 \quad (\text{recall}~\sum^N_{x=1}\vert\langle x\vert \psi_s\rangle\vert^2=1)\\
      &=4 C^{N-1}_{M-1}\\
  \sum_S\vert b_S\vert\vert c_S\vert 
      &\le \sqrt{\sum_S\vert b_S\vert^2}\sqrt{\sum_S\vert c_S\vert^2},\quad \sum_S\vert b_s\vert^2 =D_k\\
  \end{align*}</script><script type="math/tex; mode=display">
  \begin{align*}
  \Rightarrow &&D_{k+1}&\le D_k+2\sqrt{D_k}\sqrt{4C^{N-1}_{M-1}}+4C^{N-1}_{M-1}\\
  \Rightarrow &&D_{k+1} -D_k 
      &\le 4\sqrt{D_k}\sqrt{C^{N-1}_{M-1}}+4C^{N-1}_{M-1}\\
      &&&\le O(\sqrt{C^N_M}\sqrt{C^{N-1}_{M-1}})
  \end{align*}</script><script type="math/tex; mode=display">
  \begin{align*}
  T\ge \frac{D_T-D_0}{\Delta D} &=\Omega\left(\frac{C^N_M}{\sqrt{C^N_M}\sqrt{C^{N-1}_{M-1}}}\right) = \Omega\left(\sqrt{\frac{C^N_M}{C^{N-1}_{M-1}}}\right)\\
      &= \Omega\left(\sqrt{\frac{N!}{M!(N-M)!}\frac{(M-1)!(N-M)!}{(N-1)!}}\right)\\
      &= \Omega\left(\sqrt{\frac{N}{M}}\right)
  \end{align*}</script></li>
</ul>
</li>
</ul>
<h3 id="Minimum-finding" class="heading-control"><a href="#Minimum-finding" class="headerlink" title="Minimum finding"></a>Minimum finding<a class="heading-anchor" href="#Minimum-finding" aria-hidden="true"></a></h3><ul>
<li>Given quantum access to input of $N$ numbers, $x_1,x_2,\dots,x_N$, find the $\min$ of them in $O(\sqrt N\log N)$ steps.</li>
<li>start with random $s\in\{1,\dots,N\}$ query $x_s$.</li>
<li>Grover search among the items to find some $r$ s.t. $x_r\le x_s$<ul>
<li>$O(\sqrt N)$ times find a random one</li>
<li>keep doing this until you cannot find a smaller one</li>
<li>every iteration expect number of smaller items $\Rightarrow~\log N$ iterations</li>
</ul>
</li>
</ul>
</body></html>]]></content>
      <categories>
        <category>Quantum Computing</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Course Note</tag>
        <tag>Quantum Computing</tag>
        <tag>Quantum Algorithm</tag>
        <tag>Quantum Circuits</tag>
      </tags>
  </entry>
  <entry>
    <title>[Note] Quantum Computation and Quantum Information - Chapter 2: Introduction to quantum mechanics</title>
    <url>/Ending2015a/55689/</url>
    <content><![CDATA[<html><head></head><body><p>上課筆記，原文書:</p>
<blockquote>
<p>Quantum Computation and Quantum Information, Michael A. Nielsen & Isaac L. Chuang</p>
</blockquote>
<h2 id="2-1-Linear-algebra" class="heading-control"><a href="#2-1-Linear-algebra" class="headerlink" title="2.1 Linear algebra"></a>2.1 Linear algebra<a class="heading-anchor" href="#2-1-Linear-algebra" aria-hidden="true"></a></h2><div class="table-container">
<table>
<thead>
<tr>
<th>Notation</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>$z^*$</td>
<td>Complex conjugate of the complex number $z$. <br>$(1+i)^*=1-i$</td>
</tr>
<tr>
<td>$\vert\psi\rangle$</td>
<td>Column vector. Also known as a <em>ket</em></td>
</tr>
<tr>
<td>$\langle\psi\vert$</td>
<td>Row vector dual to $\vert\psi\rangle$. Also known as <em>bra</em></td>
</tr>
<tr>
<td>$\langle \varphi\vert\psi\rangle$</td>
<td>Inner product between the vectors $\vert\varphi\rangle$ and $\vert\psi\rangle$</td>
</tr>
<tr>
<td>$\vert\varphi\rangle \otimes\vert\psi\rangle$</td>
<td>Tensor product of $\vert\varphi\rangle$ and $\vert\psi\rangle$</td>
</tr>
<tr>
<td>$A^*$</td>
<td>Complex conjugate of the $A$ matrix</td>
</tr>
<tr>
<td>$A^T$</td>
<td>Transpose of the $A$ matrix</td>
</tr>
<tr>
<td>$A^\dagger$</td>
<td>Hermitian conjugate or adjoint of the $A$ matrix, $A^\dagger=(A^T)^*$. $\left[\begin{matrix}a & b\\ c & d\end{matrix}\right]^\dagger = \left[\begin{matrix} a^* & c^*\\b^* & d^*\end{matrix}\right]$</td>
</tr>
<tr>
<td>$\langle \varphi\vert A\vert\psi \rangle$</td>
<td>Inner product between $\vert\varphi\rangle$ and $A\vert\psi\rangle$. <br>Equivalently, inner product between $A^\dagger\vert\varphi\rangle$ and $\vert\psi\rangle$.</td>
</tr>
</tbody>
</table>
</div>
<a id="more"></a>
<h3 id="2-1-1-Bases-and-linear-independence" class="heading-control"><a href="#2-1-1-Bases-and-linear-independence" class="headerlink" title="2.1.1 Bases and linear independence"></a>2.1.1 Bases and linear independence<a class="heading-anchor" href="#2-1-1-Bases-and-linear-independence" aria-hidden="true"></a></h3><ul>
<li>Spanning set：a set of vectors $\vert v_1\rangle,\dots,\vert v_n\rangle$ such that any vector $\vert v\rangle$ in the vector space can be written as a linear combination $\vert v \rangle=\sum_i a_i\vert v_i\rangle$ of vectors in that set.</li>
<li>A set of non-zero vectors $\vert v_1 \rangle, \dots, \vert v_n\rangle$ are <em>lineraly dependent</em> if there exists a set of complex numbers $a_1,\dots, a_n$ with $a_i\ne 0$ for at least one value of $i$, such that<script type="math/tex; mode=display">
a_i\vert v_1\rangle + a_2\vert v_2\rangle + \cdots + a_n\vert v_n\rangle = 0</script></li>
</ul>
<h3 id="2-1-2-Linear-operators-and-matrics" class="heading-control"><a href="#2-1-2-Linear-operators-and-matrics" class="headerlink" title="2.1.2 Linear operators and matrics"></a>2.1.2 Linear operators and matrics<a class="heading-anchor" href="#2-1-2-Linear-operators-and-matrics" aria-hidden="true"></a></h3><ul>
<li>$A: V\to W$ is said to be a linear operator between vector spaces $V$ and $W$ if<script type="math/tex; mode=display">
A\left(\sum_i a_i\vert v_i\rangle\right)=\sum_i a_i A\vert v_i\rangle</script></li>
<li>Suppose $A:V\to W$ is a linear operator between vector spaces $V$ and $W$. Suppose $\vert v_1\rangle,\dots,\vert v_m\rangle$ is a basis for $V$ and $\vert w_1\rangle,\dots, \vert w_n\rangle$ is a basis for $W$. Then for each $j$ in the range $1,\dots,m$, there exist complex numbers $A_{1j}$ through $A_{nj}$ such that<script type="math/tex; mode=display">
A\vert v_j \rangle=\sum_i A_{ij} \vert w_i\rangle</script></li>
</ul>
<h3 id="2-1-3-The-Pauli-matrics" class="heading-control"><a href="#2-1-3-The-Pauli-matrics" class="headerlink" title="2.1.3 The Pauli matrics"></a>2.1.3 The Pauli matrics<a class="heading-anchor" href="#2-1-3-The-Pauli-matrics" aria-hidden="true"></a></h3><script type="math/tex; mode=display">
\begin{eqnarray*}
\sigma_0 \equiv I \equiv \left[\begin{matrix} 1 & 0 \\0 & 1\end{matrix}\right] \quad & \sigma_1\equiv\sigma_x\equiv X\equiv \left[\begin{matrix} 0 & 1\\ 1 & 0 \end{matrix}\right]\\
\sigma_2\equiv\sigma_y\equiv Y\equiv \left[\begin{matrix} 0 & -i\\ i & 0 \end{matrix}\right] \quad & \sigma_3\equiv\sigma_z\equiv Z\equiv\left[\begin{matrix}1&0\\0&-1\end{matrix}\right]
\end{eqnarray*}</script><p>The $X$ matrix is often known as the quantum <code>NOT</code> gate, by analogy to the classical <code>NOT</code> gate:</p>
<ul>
<li>$X\vert 0\rangle=\vert 1\rangle$, $X\vert 1\rangle=\vert 0\rangle$</li>
<li>$X\vert +\rangle=\vert +\rangle$, $X\vert -\rangle=(-1)\vert -\rangle$<br><br>where $\vert+\rangle=\frac{1}{\sqrt{2}}(\vert 0\rangle + \vert 1\rangle)$, and $\vert-\rangle=\frac{1}{\sqrt{2}}(\vert 0\rangle - \vert 1\rangle)$</li>
</ul>
<p>The $X$ and $Z$ are also sometimes referred toas the <em>bit flip</em> and <em>phase flip</em> matrices. $Z\vert 1\rangle=(-1)\vert 1\rangle$, $-1$ is known as a <em>phase factor</em>.</p>
<h3 id="2-1-4-Inner-products" class="heading-control"><a href="#2-1-4-Inner-products" class="headerlink" title="2.1.4 Inner products"></a>2.1.4 Inner products<a class="heading-anchor" href="#2-1-4-Inner-products" aria-hidden="true"></a></h3><ol>
<li>$(\cdot,\cdot)$ is linear in the second argument,<script type="math/tex; mode=display">
\left(\vert v\rangle, \sum_i\lambda_i\vert w_i\rangle\right)=\sum_i\lambda_i\left(\vert v\rangle, \vert w_i\rangle\right)</script></li>
<li>$\left(\vert v\rangle, \vert w\rangle\right)=\left(\vert w\rangle, \vert v\rangle\right)^*$</li>
<li>$\left(\vert v\rangle, \vert v\rangle\right)\ge 0$ with equality if and only if $\vert v\rangle=0$</li>
</ol>
<p>Let $\vert w\rangle=\sum_i w_i\vert i \rangle$ and $\vert v\rangle=\sum_j v_j\vert j \rangle$ be representations of vectprs $\vert w\rangle$ and $\vert v \rangle$ with respect to some orthonormal basis $\vert i\rangle$. Then, since $\langle i\vert j\rangle=\delta_{ij}$,</p>
<script type="math/tex; mode=display">
\begin{align*}
\langle v\vert w\rangle &= \left(\sum_j v_i\vert i \rangle, \sum_i w_j\vert j \rangle\right) = \sum_{ij}v^*_iw_j\delta_{ij}=\sum_iv^*_iw_i\\
&= \left[\begin{matrix}v^*_1 \dots v^*_n\end{matrix}\right]\left[\begin{matrix}w_1\\ \vdots\\w_n\end{matrix}\right]
\end{align*}</script><h3 id="2-1-5-Eigenvectors-and-eigenvalues" class="heading-control"><a href="#2-1-5-Eigenvectors-and-eigenvalues" class="headerlink" title="2.1.5 Eigenvectors and eigenvalues"></a>2.1.5 Eigenvectors and eigenvalues<a class="heading-anchor" href="#2-1-5-Eigenvectors-and-eigenvalues" aria-hidden="true"></a></h3><ul>
<li>An <em>eigenvector</em> of a linear operator $A$ on a vector space is a non-zero vector $\vert v \rangle$ such that $A\vert v \rangle=\lambda\vert v\rangle$, where $\lambda$ is a complex number knwon as the <em>eigenvalue</em> of $A$ corresponding to $\vert v \rangle$.</li>
<li>The <em>characteristic function</em> is defined to be $c(\lambda)\equiv\det\vert A-\lambda I\vert$, where $\det$ is the <em>determinant</em> function for matrics.</li>
</ul>
<h3 id="2-1-6-Adjoints-and-Hermitian-operators" class="heading-control"><a href="#2-1-6-Adjoints-and-Hermitian-operators" class="headerlink" title="2.1.6 Adjoints and Hermitian operators"></a>2.1.6 Adjoints and Hermitian operators<a class="heading-anchor" href="#2-1-6-Adjoints-and-Hermitian-operators" aria-hidden="true"></a></h3><ul>
<li>Suppose $A$ is any linear operator on a Hilbert space, $V$. It turns out that there exists a unique linear operator $A^\dagger$ on $V$ such that for all vectors $\vert v\rangle, \vert w \rangle\in V$,<script type="math/tex; mode=display">
(\vert v \rangle, A\vert w\rangle)=(A^\dagger\vert v\rangle, \vert w\rangle)</script></li>
<li>$(AB)^\dagger=B^\dagger A^\dagger$</li>
<li>$\vert v \rangle^\dagger \equiv \langle v \vert$</li>
<li>A matrix $U$ is said to be <em>unitary</em> if $U^\dagger U=I$.</li>
<li>Unitary operators preserve inner products between vectors. Let $\vert v\rangle$ and $\vert w \rangle$ be any two vectors.<script type="math/tex; mode=display">
(U\vert v \rangle, U\vert w \rangle)=\langle v\vert U^\dagger U\vert w\rangle=\langle v \vert I\vert w\rangle=\langle v\vert w \rangle</script></li>
<li>A <em>positive operator</em> $A$ is defined to be an operator such that for any vector $\vert v\rangle$, $(\vert v\rangle, A\vert v\rangle)$ is a real, non-negative number. If $(\vert v\rangle, A\vert v\rangle)>0, ~~\forall \vert v\rangle \ne 0$，we say that $A$ is <em>positive definite</em>.</li>
</ul>
<h3 id="2-1-7-Tensor-products" class="heading-control"><a href="#2-1-7-Tensor-products" class="headerlink" title="2.1.7 Tensor products"></a>2.1.7 Tensor products<a class="heading-anchor" href="#2-1-7-Tensor-products" aria-hidden="true"></a></h3><ul>
<li>Suppose $V$ and $W$ are vector spaces of dimension $m$ and $n$ respectively, then $V\otimes W$ (read V tensor W) is a $mn$ dimensional vector space.</li>
<li>The elements of $V\otimes W$ are linear combinations of tensor products $\vert v\rangle \otimes \vert w \rangle, ~~\text{where}~\vert v\rangle\in V, \vert w\rangle\in W$</li>
<li>$\vert i\rangle\otimes \vert j\rangle$ is a basis for $V\otimes W$ if $\vert i\rangle$ and $\vert j\rangle$ are orthonormal bases for the spaces $V$ and $W$</li>
<li>Sometimes abbreviate notations to $\vert v \rangle\vert w\rangle$, $\vert v, w\rangle$, or even $\vert vw\rangle$</li>
</ul>
<p>Properties:</p>
<ul>
<li>For an arbitrary scalar $z$ and elements $\vert v \rangle$ of $V$ and $\vert w\rangle$ of W,<script type="math/tex; mode=display">
z(\vert v\rangle\otimes\vert w\rangle)=(z\vert v\rangle)\otimes \vert w\rangle=\vert v\rangle \otimes (z\vert w\rangle)</script></li>
<li>For arbitrary $\vert v_1\rangle$ and $\vert v_2\rangle$ in $V$ and $\vert w\rangle$ in $W$,<script type="math/tex; mode=display">
(\vert v_1\rangle + \vert v_2\rangle)\otimes\vert w \rangle=\vert v_1\rangle\otimes\vert w\rangle + \vert v_2\rangle \otimes\vert w\rangle</script></li>
<li>For arbitrary $\vert v\rangle$ in $V$ and $\vert w_1\rangle$ and $\vert w_2\rangle$ in $W$,<script type="math/tex; mode=display">
\vert v\rangle \otimes (\vert w_1\rangle + \vert w_2\rangle)=\vert v\rangle \otimes \vert w_1\rangle + \vert v\rangle \otimes \vert w_2\rangle</script></li>
</ul>
<p>Suppose $\vert v\rangle$ and $\vert w \rangle$ are vectors in $V$ and $W$, and $A$ and $B$ are linear operators on $V$ and $W$, respectively. Then we can define a linear operator $A\otimes B$ on $V\otimes W$ by the equation</p>
<script type="math/tex; mode=display">
(A\otimes B)(\vert v \rangle\otimes \vert w\rangle)\equiv A\vert v\rangle \otimes B\vert w\rangle</script><p>Suppose A is an $m$ by $n$ matrix, and $B$ is a $p$ by $q$ matrix. Then we have the matrix representation:</p>
<script type="math/tex; mode=display">
A\otimes B = \underbrace{\left.\left[ 
                  \vphantom{\begin{matrix}{c}1\\1\\1\\1\end{matrix}}
                  \begin{matrix}
                             A_{11}B&A_{12}B&\cdots &A_{1n}B\\
                             A_{21}B&A_{22}B&\cdots &A_{2n}B\\
                             \vdots&\vdots&\ddots&\vdots&\\
                             A_{m1}B&A_{m2}B&\cdots &A_{mn}B
                  \end{matrix}
              \right]\right\}}_{nq\text{ columns}}
              \,mp\text{ rows}</script><ul>
<li>$\vert \psi\rangle^{\otimes k}$ means $\vert \psi\rangle$ tensored with itself $k$ times.</li>
</ul>
<h3 id="2-1-8-Operator-functions" class="heading-control"><a href="#2-1-8-Operator-functions" class="headerlink" title="2.1.8 Operator functions"></a>2.1.8 Operator functions<a class="heading-anchor" href="#2-1-8-Operator-functions" aria-hidden="true"></a></h3><blockquote>
<p>待補</p>
</blockquote>
<h3 id="2-1-9-The-commutator-and-anti-commutator" class="heading-control"><a href="#2-1-9-The-commutator-and-anti-commutator" class="headerlink" title="2.1.9 The commutator and anti-commutator"></a>2.1.9 The commutator and anti-commutator<a class="heading-anchor" href="#2-1-9-The-commutator-and-anti-commutator" aria-hidden="true"></a></h3><blockquote>
<p>待補</p>
</blockquote>
<h3 id="2-1-10-The-polar-and-singular-value-decompositions" class="heading-control"><a href="#2-1-10-The-polar-and-singular-value-decompositions" class="headerlink" title="2.1.10 The polar and singular value decompositions"></a>2.1.10 The polar and singular value decompositions<a class="heading-anchor" href="#2-1-10-The-polar-and-singular-value-decompositions" aria-hidden="true"></a></h3><blockquote>
<p>待補</p>
</blockquote>
<h2 id="2-2-The-postulates-of-quantum-mechanics" class="heading-control"><a href="#2-2-The-postulates-of-quantum-mechanics" class="headerlink" title="2.2 The postulates of quantum mechanics"></a>2.2 The postulates of quantum mechanics<a class="heading-anchor" href="#2-2-The-postulates-of-quantum-mechanics" aria-hidden="true"></a></h2><h3 id="2-2-1-State-space" class="heading-control"><a href="#2-2-1-State-space" class="headerlink" title="2.2.1 State space"></a>2.2.1 State space<a class="heading-anchor" href="#2-2-1-State-space" aria-hidden="true"></a></h3><div class="note success">
            <p><strong>Postulate 1</strong>: Associated to any isolated physical system is a complex vector space with inner product (that is, a <u>Hilbert space</u>) known as the <em>state space</em> of the system. The system is completely described by its <em>state vector</em>, which is a <u>unit vector</u> in the system’s state space.</p>
          </div>
<ul>
<li>A <em>qubit</em> has a two-dimensional state space.</li>
<li>Suppose $\vert 0 \rangle$ and $\vert 1 \rangle$ form an orthonormal basis for that state space. Then an arbitrary state vector in the state space can be written<script type="math/tex; mode=display">
\vert \psi\rangle=a\vert 0\rangle + b \vert 1\rangle</script>where, $a$ and $b$ are complex numbers.</li>
<li>$\vert \psi\rangle$ is a unit vector implies that $\langle \psi \vert \psi \rangle=1$, and also equivalent to $\vert a\vert^2 + \vert b\vert^2=1$.</li>
<li>We say that any linear combination $\sum_i\alpha_i\vert \psi_i\rangle$ is a superposition of the states $\vert \psi_i\rangle$ with <em>amplitude $\alpha_i$ for the state $\vert \psi_i\rangle$.</em></li>
</ul>
<h3 id="2-2-2-Evolution" class="heading-control"><a href="#2-2-2-Evolution" class="headerlink" title="2.2.2 Evolution"></a>2.2.2 Evolution<a class="heading-anchor" href="#2-2-2-Evolution" aria-hidden="true"></a></h3><div class="note success">
            <p><strong>Postulate 2</strong>: The evolution of a <em>closed</em> quantum system is described by a <em>unitary transformation</em>. That is, the state $\vert \psi \rangle$ of the system at time $t_1$ is related to the state $\vert \psi’\rangle$ of the system at time $t_2$ by unitary operator $U$ which depends only on the times $t_1$ and $t_2$,</p><script type="math/tex; mode=display">\vert \psi'\rangle=U\vert \psi\rangle</script>
          </div>
<p><em>Hadamard gate</em> $H$ is an unitary operator：</p>
<script type="math/tex; mode=display">
H=\frac{1}{\sqrt2}\left[\begin{matrix}1&1\\1&-1\end{matrix}\right]</script><p>This has the action：</p>
<ul>
<li>$H\vert 0\rangle\equiv\frac{1}{\sqrt2}(\vert 0\rangle + \vert 1\rangle)$</li>
<li>$H\vert 1\rangle\equiv\frac{1}{\sqrt2}(\vert 0\rangle - \vert 1\rangle)$</li>
</ul>
<div class="note success">
            <p><strong>Postulate 2’</strong>: The time evolution of the state of a closed quantum system is described by the <em>Schrödinger equation</em>,</p><script type="math/tex; mode=display">i\hbar\frac{d}{dt}\vert \psi(t)\rangle=\hat{H}\vert\psi(t)\rangle</script><ul><li>$\hbar$ is known as <em>Planck’s constant</em></li><li>$\hat{H}$ is referred to <em>Hamiltonian operator</em>.</li></ul>
          </div>
<h3 id="2-2-3-Quantum-measurement" class="heading-control"><a href="#2-2-3-Quantum-measurement" class="headerlink" title="2.2.3 Quantum measurement"></a>2.2.3 Quantum measurement<a class="heading-anchor" href="#2-2-3-Quantum-measurement" aria-hidden="true"></a></h3><div class="note success">
            <p><strong>Postulate 3</strong>: Quantum measurements are described by a collection $\{M_m\}$ of <em>measurement operators</em>. These are opaertors acting on the state space of the system being measured. The index $m$ refers to the measurement outcomes that may occur in the experiment. If the state of the quantum system is $\vert\psi\rangle$ immediately before the measurement then the probability that result $m$ occurs is given by</p><script type="math/tex; mode=display">p(m)=\langle\psi\vert M^\dagger_m M_m\vert\psi\rangle</script><p>and the state of the system after the measurement (post-measurement state) is </p><script type="math/tex; mode=display">\frac{M_m\vert\psi\rangle}{\sqrt{\langle\psi\vert M^\dagger_m M_m\vert\psi\rangle}}</script><p>The measurement operators satisfy the <em>completeness equation</em>,</p><script type="math/tex; mode=display">\sum_m M^\dagger_m M_m=I</script><p>The completeness equation expresses the fact that probabilities sum to one:</p><script type="math/tex; mode=display">1=\sum_m p(m)=\sum_m\langle\psi\vert M^\dagger_m M_m\vert \psi\rangle</script>
          </div>
<ul>
<li><em>measurement of a qubit in the computational basis</em>. $M_0=\vert0\rangle\langle0\vert$, $M_1=\vert1\rangle\langle1\vert$.<ul>
<li>Hermitian</li>
<li>$M_0^2=M_0, M_1^2=M_1$</li>
<li>completeness relation is obeyed</li>
</ul>
</li>
</ul>
<ul>
<li><p>Pauli measurements:</p>
<script type="math/tex; mode=display">
X\equiv\left[\begin{matrix}0& 1\\1&0\end{matrix}\right], \quad Y\equiv\left[\begin{matrix}0& -i\\-i&0\end{matrix}\right], \quad Z\equiv\left[\begin{matrix}1& 0\\0&-1\end{matrix}\right]</script></li>
<li><p>Pauli-X (Classical NOT gate):</p>
<script type="math/tex; mode=display">
\begin{align*}
X\vert 0\rangle = \vert 1\rangle, &\quad X\vert 1\rangle = \vert 0\rangle\\
X\vert +\rangle = \vert +\rangle, &\quad X\vert -\rangle = (-1)\vert -\rangle
\end{align*}</script></li>
</ul>
<h3 id="2-2-4-Distinguishing-quantum-states" class="heading-control"><a href="#2-2-4-Distinguishing-quantum-states" class="headerlink" title="2.2.4 Distinguishing quantum states"></a>2.2.4 Distinguishing quantum states<a class="heading-anchor" href="#2-2-4-Distinguishing-quantum-states" aria-hidden="true"></a></h3><ul>
<li>Non-orthogonal quantum states cannot be distinguished.</li>
<li>Orthogonal quantum states are perfectly distinguishable</li>
</ul>
<p><img src="https://i.imgur.com/ELjQSWC.png" alt></p>
<h3 id="2-2-5-Projective-measurement" class="heading-control"><a href="#2-2-5-Projective-measurement" class="headerlink" title="2.2.5 Projective measurement"></a>2.2.5 Projective measurement<a class="heading-anchor" href="#2-2-5-Projective-measurement" aria-hidden="true"></a></h3><ul>
<li>The special class of measurements (Postulate 3) is known as <em>projective measurements</em>.</li>
<li>Primarily concerned measurements with <em>projective measurements</em>.</li>
<li>$P_i^2=P_i$, where $P_i$ is a measurement operator, and is called <em>projector</em>.</li>
</ul>
<div class="note success">
            <p><strong>Projective measurements</strong>: A projective measurement is described by an <em>observable</em>, $M$, a Hermitian operator on the state space of the system being observed. The observable has a spectral decomposition,</p><script type="math/tex; mode=display">M=\sum_m mP_m</script><p>where $P_m$ is the projector onto the eigenspace of $M$ with eigenvalue $m$. The possible outcomes of the measurement correspond to the eigenvalues, $m$, of the observable. Upon measuring the state $\vert\psi\rangle$, the probability of getting result $m$ is given by </p><script type="math/tex; mode=display">p(m)=\langle\psi\vert P_m\vert\psi\rangle</script><p>Given that outcome $m$ occurred, the state of the quantum system immediately after the measurement is</p><script type="math/tex; mode=display">\frac{P_m\vert\psi\rangle}{\sqrt{p(m)}}</script>
          </div>
<ul>
<li><p>The average value of the measurement:</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathbb{E}(M) =\langle M\rangle &= \sum_m m~p(m)\\
  &= \sum_m m\langle \psi\vert P_m\vert\psi\rangle\\
  &= \langle \psi \vert \left(\sum_m mP_m\right)\vert\psi\rangle\\
  &= \langle \psi\vert M\vert\psi\rangle
\end{align*}</script></li>
<li><p>The square of the standard deviation:</p>
<script type="math/tex; mode=display">
\begin{align*}
[\Delta(M)]^2 &= \langle (M-\langle M \rangle)^2\rangle\\
  &= \langle M^2 \rangle - \langle M\rangle ^2
\end{align*}</script></li>
<li><p>Standard deviation $\Delta(M)=\sqrt{\langle M^2 \rangle - \langle M\rangle ^2}$</p>
</li>
<li><p>Every hermitian matrix $M=\sum_\lambda \lambda\vert\psi_\lambda\rangle\langle\psi_\lambda\vert$ can be viewed as a measurement in physics. They are called <em>observables</em> and most physical quantities are observables e.g. position, momentum, electric field, strength, number of photon</p>
</li>
</ul>
<p><img src="https://i.imgur.com/XuPeScW.jpg" width="500px"></p>
<h3 id="2-2-6-POVM-measurements" class="heading-control"><a href="#2-2-6-POVM-measurements" class="headerlink" title="2.2.6 POVM measurements"></a>2.2.6 POVM measurements<a class="heading-anchor" href="#2-2-6-POVM-measurements" aria-hidden="true"></a></h3><ul>
<li>POVM (Positive Operator-Valued Measure)<script type="math/tex; mode=display">
E_m\equiv M^\dagger_mM_m</script></li>
<li>$E_m$ is a positive operator such that $\sum_mE_m=I$ and $p(m)=\langle\psi\vert E_m\vert\psi\rangle$<ul>
<li>Positive operator is a positive semi-definite matrix.</li>
<li>$E_m$ is also known as <em>POVM element</em></li>
</ul>
</li>
<li>A complete set $\{E_m\}$ is known as <em>POVM</em></li>
</ul>
<p>Example: If we want to distinguish $\{\vert 0\rangle, \vert + \rangle\}$. We can set measurement operations to:</p>
<script type="math/tex; mode=display">
\begin{align*}
E_1 &= \frac{\sqrt{2}}{1+\sqrt{2}}\vert 1\rangle\langle 1\vert\\
E_2 &= \frac{\sqrt{2}}{1+\sqrt{2}}\vert -\rangle\langle -\vert\\
E_3 &= Id-E_1-E_2 \quad \text{(Should check if $E_3$ is a positive operator)}
\end{align*}</script><p>Since $\vert 0\rangle$ is orthogonal to the measurement $E_1$, that is $\langle 0\vert E_1\vert 0\rangle=0$. Also, $\vert+\rangle$ is orthogonal to the measurement $E_2$, that is $\langle +\vert E_2\vert +\rangle=0$:</p>
<ol>
<li>If we see outcome 1, it must be $\vert+\rangle$.</li>
<li>If we see outcome 2, it must be $\vert 0\rangle$.</li>
<li>If we see outcome 3, we cannot distinguish it.</li>
</ol>
<h3 id="2-2-7-Phase" class="heading-control"><a href="#2-2-7-Phase" class="headerlink" title="2.2.7 Phase"></a>2.2.7 Phase<a class="heading-anchor" href="#2-2-7-Phase" aria-hidden="true"></a></h3><p><strong>Global state</strong>:<br>Consider the state $e^{i\theta}\vert\psi\rangle$</p>
<ul>
<li>$\vert\psi\rangle$ is a state vector</li>
<li>$e^{i\theta}$ is the <em>global phase factor</em></li>
<li>Global phase do not matter. Let $\vert\psi’\rangle=e^{i\theta}\vert\psi\rangle$ measured with $\{M_i\}$:<script type="math/tex; mode=display">
\begin{align*}
p'_i &= \langle \psi'\vert M^\dagger_iM_i\vert\psi'\rangle\\
  &= \langle \psi\vert M^\dagger_i e^{-i\theta}e^{i\theta}M_i\vert\psi\rangle\\
  &= p_i
\end{align*}</script></li>
</ul>
<p><strong>Relative state</strong>:<br>Consider $\vert+\rangle=\frac{1}{\sqrt{2}}\vert 0\rangle+\frac{1}{\sqrt{2}}\vert 1\rangle$, $\vert-\rangle=\frac{1}{\sqrt{2}}\vert0\rangle-\frac{1}{\sqrt{2}}\vert1\rangle$. The relative phase of the state $\vert 1\rangle$ in $\vert+\rangle$ and $\vert-\rangle$ are orthogonal.</p>
<h3 id="2-2-8-Composite-systems" class="heading-control"><a href="#2-2-8-Composite-systems" class="headerlink" title="2.2.8 Composite systems"></a>2.2.8 Composite systems<a class="heading-anchor" href="#2-2-8-Composite-systems" aria-hidden="true"></a></h3><div class="note success">
            <p><strong>Postulate 4</strong>: The state space of a composite physical system is the tensor product of the state spaces of the component physical systems. Moreover, if we have systems numbered $1$ through $n$, and system number $i$ is prepared in the state $\vert \psi_i\rangle$, then the joint state of the total system is $\vert \psi_1\rangle\otimes\vert\psi_2\rangle\otimes\cdots\otimes\vert\psi_n\rangle$</p>
          </div>
<ul>
<li>For a single quantum system, we can discribe the composite state of two quantum states $\vert x\rangle$ and $\vert y\rangle$ as $\alpha\vert x\rangle+\beta\vert y\rangle$, where $\vert\alpha\vert^2+\vert\beta\vert^2=1$. We call this <em>superposition principle of quantum mechanics</em>.</li>
<li>Similary, for two quantum systems $A$ and $B$, we use <em>tensor product</em> to define the joint system $AB$, and we might denote the state as $\vert A\rangle\vert B\rangle$ for the joint system, where each $\vert A\rangle$ and $\vert B\rangle$ are a state of the system $A$ and $B$, respectively.</li>
</ul>
<p>Letting $\vert 0\rangle$ be any fixed state of an ancilla system (附屬系統) $M$, having an orthonormal basis $\vert m\rangle$ in one-to-one correspondence with the possible outcomes of the measurement $\{M_m\}$, define an operator $U$ on products $\vert\psi\rangle\vert0\rangle$ of states $\vert\psi\rangle$ from a system $Q$ with the state $\vert0\rangle$ by</p>
<script type="math/tex; mode=display">
U\vert\psi\rangle\vert0\rangle\equiv\sum_m M_m\vert\psi\rangle\vert m\rangle</script><p>we can see that $U$ preserves inner products between states of the form $\vert\psi\rangle\vert 0\rangle$:</p>
<script type="math/tex; mode=display">
\begin{align*}
\langle\varphi\vert\langle0\vert U^\dagger U\vert\psi\rangle\vert 0\rangle
    &= \sum_{m,m'}\langle\varphi\vert M_m^\dagger M_{m'}\vert\psi\rangle\langle m\vert m'\rangle\\
    &= \sum_{m}\langle\varphi\vert M^\dagger_m M_m\vert\phi\rangle\\
    &= \langle \varphi\vert\psi\rangle
\end{align*}</script><p>We have proved that $U: Q\to Q\otimes M$ is an unitary operator If we can prove Exercise 2.67, then we can say $U$ can be extended to a unitary operator on the space $Q\otimes M$.<br></p><div class="note info">
            <p><em>Exercise 2.67</em>: Suppose $V$ is a Hilbert space with a subspace $W$. Suppose $U:W\to V$ is a linear operator which preserves inner products, that is, for any $\vert w_1\rangle$ and $\vert w_2\rangle$ in W,</p><script type="math/tex; mode=display">\langle w_1\vert U^\dagger U\vert w_2\rangle = \langle w_1\vert w_2\rangle</script><p>Prove that there exists a unitary operator $U’:V\to V$ which extends $U$. That is, $U’\vert w\rangle=U\vert w\rangle$ for all $\vert w\rangle$ in $W$, but $U’$ is defined on the entire space $V$.</p><p><em>Proof</em>: In order to complete the proof, we first show that $U’$ is an unitary operator, then we show that $U’ \vert w\rangle=U\vert w\rangle$ is hold for all $\vert w\rangle$ in $W$:</p><p>Let $\vert v\rangle=U\vert w\rangle$, where $\vert v\rangle \in V$, $\vert w\rangle \ne \vert w_2\rangle$ is in $W$. It is reasonable to define $U=\vert v\rangle\langle w\vert$ and the conjugate transpose $U^\dagger=\vert w\rangle\langle v\vert$, since $\vert v\rangle=\vert v\rangle\langle w\vert w\rangle=U\vert w\rangle$. However, we discovered that $U^\dagger U=\vert w\rangle\langle v\vert v\rangle\langle w\vert =\vert w\rangle\langle w\vert \ne I$, which means $\langle w_1\vert U^\dagger U\vert w_2\rangle = \langle w_1\vert w_2\rangle$ is not hold unless we can show that $U^\dagger U=\sum_i\vert w_i\rangle\langle w_i\vert=I$ for any orthonormal basis $\{\vert w_i\rangle\}$ in $W$. </p><p>Hense, we let $\{\vert w_i\rangle\}$ and $\{\vert v_k\rangle\}$ be an orthonormal basis that span $W$ and $V$, where $i\in\dim(W)$ and $k\in\dim(V)$. We redefine $U=\sum_i\vert v_i\rangle\langle w_i\vert$ and the conjugate transpose $U^\dagger=\sum_i\vert w_i\rangle\langle v_i\vert$. Now the equation holds, since</p><script type="math/tex; mode=display">\begin{align*}U^\dagger U    =\left(\sum_i\vert w_i\rangle\langle v_i\vert\right)\left(\sum_i\vert v_i\rangle\langle w_i\vert\right)    =\sum_i \vert w_i\rangle\langle v_i\vert v_i\rangle\langle w_i\vert    =\sum_i\vert w_i\rangle\langle w_i\vert=I\end{align*}</script><p>Let $\{\vert x_j\rangle\}$ be an orthonormal basis in complement space $X=V\setminus W$, where $j\in\dim(X)$. The mutually orthogonal set $\{\vert w_i\rangle, \vert x_j\rangle~\vert~ \forall i, j\}$, which can span $V$, is also an orthonormal basis in $V$. We then define $U’=\sum_i\vert v_i\rangle\langle w_i\vert + \sum_j\vert v_j\rangle\langle x_j\vert$, and the conjugate transpose $(U’)^\dagger = \sum_i\vert w_i\rangle\langle v_i\vert + \sum_j\vert x_j\rangle\langle v_j\vert$. As such, we can show that $U’$ is an unitary operator by</p><script type="math/tex; mode=display">\begin{align*}(U')^\dagger U' &= \left(\sum_i\vert w_i\rangle\langle v_i\vert + \sum_j\vert x_j\rangle\langle v_j\vert\right)\left(\sum_i\vert v_i\rangle\langle w_i\vert + \sum_j\vert v_j\rangle\langle x_j\vert\right)\\    &= \left(\sum_i\vert w_i\rangle\langle v_i\vert\right)\left(\sum_i\vert v_i\rangle\langle w_i\vert\right)+\left(\sum_j\vert x_j\rangle\langle v_j\vert\right)\left(\sum_j\vert v_j\rangle\langle x_j\vert\right)\\    &= \sum_i\vert w_i\rangle\langle w_i\vert + \sum_j \vert x_j\rangle\langle x_j \vert = I\end{align*}</script><p>Same for $U’(U’)^\dagger = I$. Next, we show that $U’$ extends $U$</p><script type="math/tex; mode=display">\begin{align*}U'\vert w\rangle &= \left(\sum_i\vert v_i\rangle\langle w_i\vert + \sum_j\vert v_j\rangle\langle x_j\vert\right)\vert w\rangle\\    &= \left(\sum_i\vert v_i\rangle\langle w_i\vert\right)\vert w\rangle \quad\quad(\because \vert x_j\rangle \bot \vert w\rangle, ~\forall~\vert x_j\rangle\in X\\     &= U\vert w\rangle\end{align*}</script>
          </div><p></p>
<h2 id="2-3-Application-superdense-coding" class="heading-control"><a href="#2-3-Application-superdense-coding" class="headerlink" title="2.3 Application: superdense coding"></a>2.3 Application: superdense coding<a class="heading-anchor" href="#2-3-Application-superdense-coding" aria-hidden="true"></a></h2><blockquote>
<p>待補</p>
</blockquote>
<h2 id="2-4-The-density-operator" class="heading-control"><a href="#2-4-The-density-operator" class="headerlink" title="2.4 The density operator"></a>2.4 The density operator<a class="heading-anchor" href="#2-4-The-density-operator" aria-hidden="true"></a></h2><h3 id="2-4-1-Ensembles-of-quantum-states" class="heading-control"><a href="#2-4-1-Ensembles-of-quantum-states" class="headerlink" title="2.4.1 Ensembles of quantum states"></a>2.4.1 Ensembles of quantum states<a class="heading-anchor" href="#2-4-1-Ensembles-of-quantum-states" aria-hidden="true"></a></h3><ul>
<li><p>Density operator, or density matrix:</p>
<script type="math/tex; mode=display">
\rho\equiv\sum_i p_i\vert \psi_i\rangle\langle \psi_i\vert</script><p>where $\vert \psi_i\rangle$ is an unknown state, $p_i$ is its probability. Then we call $\{p_i, \vert\psi_i\rangle\}$ an <strong>ensemble of pure states</strong>.</p>
</li>
<li><p>The evolution descried by the unitary operator $U$ of the density operator:</p>
<script type="math/tex; mode=display">
\rho=\sum_i p_i\vert \psi_i\rangle\langle \psi_i\vert \xrightarrow{U} \sum_i p_i U\vert \psi_i\rangle\langle\psi_i\vert U^\dagger=U\rho U^\dagger</script></li>
<li><p>Measurement, if the initial state is $\vert \psi_i\rangle$, the probability of getting result $m$ is</p>
<script type="math/tex; mode=display">
p(m\vert i)=\langle \psi_i\vert M^\dagger_m M_m \vert \psi_i\rangle=\text{tr}(M^\dagger_m M_m\vert \psi_i\rangle\langle \psi_i\vert)</script></li>
<li><p>The total probability of getting result $m$ is:</p>
<script type="math/tex; mode=display">
\begin{align*}
p(m) &= \sum_i p(m\vert i)p_i\\
  &=\sum_i p_i \text{tr}(M^\dagger_mM_m\vert \psi_i\rangle\langle \psi_i\vert)\\
  &= \text{tr}(M^\dagger_m M_m \rho)
\end{align*}</script></li>
<li><p>The post-measurement state of $\vert \psi_i\rangle$:</p>
<script type="math/tex; mode=display">
\vert \psi^m_i\rangle = \frac{M_m\vert \psi_i\rangle}{\sqrt{\langle \psi_i\vert M^\dagger_mM_m\vert \psi_i\rangle}}</script></li>
<li><p>The density matrix after measurement:</p>
<script type="math/tex; mode=display">
\rho_m=\sum_i p(i\vert m)\vert\psi^m_i\rangle\langle \psi^m_i\vert=\sum_i p(i\vert m)\frac{M_m\vert\psi_i\rangle\langle\psi_i\vert M^\dagger_m}{\langle \psi_i\vert M^\dagger_mM_m\vert \psi_i\rangle}</script><p>since $p(i\vert m)=p(m, i)/p(m)=p(m\vert i)p_i/p(m)$, we obtain</p>
<script type="math/tex; mode=display">
\begin{align*}
\rho_m &= \sum_i p_i\frac{M_m\vert \psi_i\rangle\langle\psi_i\vert M^\dagger_m}{\text{tr}(M^\dagger_m M_m \rho)}\\
  &= \frac{M_m\rho M^\dagger_m}{\text{tr}(M^\dagger_m M_m\rho)}
\end{align*}</script></li>
</ul>
<blockquote>
<p>What we have shown is that the basic postulates of quantum mechanics related to<br>unitary evolution and measurement can be rephrased in the language of density operators.</p>
</blockquote>
<ul>
<li>A quantum systen whose state $\vert \psi\rangle$ is known exactly is said to be in a <strong>pure state</strong>, the density operator is $\rho=\vert\psi\rangle\langle \psi \vert$. Otherwise, $\rho$ is in <strong>mixed state</strong> (a <strong>mixture</strong> of the different pure state).</li>
<li>A pure state $\text{tr}(\rho^2)=1$, mixed state $\text{tr}(\rho^2)<1$</li>
<li>A quantum system in mixed state $\rho_i$ arising from some ensemble $\{p_{i, j}, \vert \psi_{ij}\rangle\}$ of pure states with probability $p_i$. The density matrix for the system is <script type="math/tex; mode=display">
\rho = \sum_{ij}p_ip_{ij}\vert \psi_{ij}\rangle\langle\psi_{ij}\vert=\sum_i p_i\rho_i</script>We say $\rho$ is a <em>mixture</em> of the state $\rho_i$ with probabilities $p_i$.</li>
<li>A quantum system is in the state $\rho_m$ with probability $p(m)$ after the measurement $m$. The state of the quantum system would be described by the density operator<script type="math/tex; mode=display">
\begin{align*}
\rho' &=\sum_m p(m)\rho_m\\
  &= \sum_m \text{tr}(M^\dagger_m M_m\rho)\frac{M_m\rho M^\dagger_m}{\text{tr}(M^\dagger_mM_m\rho)}\\
  &= \sum_m M_m\rho M^\dagger_m
\end{align*}</script></li>
</ul>
<h3 id="2-4-2-General-properties-of-the-density-operator" class="heading-control"><a href="#2-4-2-General-properties-of-the-density-operator" class="headerlink" title="2.4.2 General properties of the density operator"></a>2.4.2 General properties of the density operator<a class="heading-anchor" href="#2-4-2-General-properties-of-the-density-operator" aria-hidden="true"></a></h3><div class="note success">
            <p><em>Theorem 2.5</em> (<strong>Characterization of density operators</strong>) An operator $\rho$ is the density operator associated to some ensemble $\{p_i, \vert \psi_i\rangle\}$ if and only if satisfies the conditions:</p><ol><li>(<strong>Trace condition</strong>) $\rho$ has trace equal to one.</li><li>(<strong>Positive condition</strong>) $\rho$ is a positive operator.</li></ol><p><em>Proof</em><br>Suppose $\rho=\sum_ip_i\vert\psi_i\rangle\langle\psi_i\vert$ is a density operator. Then</p><script type="math/tex; mode=display">\text{tr}(\rho)=\sum_ip_i\text{tr}(\vert\psi_i\rangle\langle\psi_i\vert)=\sum_ip_i=1</script><p>so the trace condition $\text{tr}(\rho)=1$ is satisfied. Suppose $\vert\varphi\rangle$ is an arbitrary vector in state space. Then</p><script type="math/tex; mode=display">\begin{align*}\langle \varphi\vert\rho\vert\varphi\rangle    &= \sum_ip_i\langle\varphi\vert\psi_i\rangle\langle\psi_i\vert\varphi\rangle\\    &= \sum_i p_i\vert\langle \varphi\vert\psi_i\rangle\vert^2\\    & \ge 0\end{align*}</script><p>Conversely, suppose $\rho$ is any operator satifsying the trace and positivity conditions. Since $\rho$ is positive, it must have a spectral decomposition</p><script type="math/tex; mode=display">\rho = \sum_j\lambda_j\vert j\rangle\langle j\vert</script><p>where the vectors $\vert j\rangle$ are orthogonal, and $\lambda_j$ are real, non-negative eigenvalues of $\rho$.</p><p>From the trace condition we see that $\sum_j \lambda_j=1$. Therefore, a system in state $\vert j\rangle$ with probability $\lambda_j$ will have density opeartor $\rho$. That is, the ensemble $\{\lambda_j, \vert j\rangle\}$ is an ensemble of states giving rise to the density operator $\rho$.</p>
          </div>
<p>We can reformulate the postulates:<br></p><div class="note success">
            <p><strong>Postulate 1</strong>: Associated to any isolated physical system is a complex vector space with inner product (that is, a Hilbert space) known as the <em>state space</em> of the system. The system is completely discribed by its <em>density operator</em>, which is a positive operator $\rho$ with trace one, acting on the state space of the system. If a quantum system is in the state $\rho_i$ with probability $p_i$, then the density operator for the system is $\sum_i p_i\rho_i$.</p><p><strong>Postulate 2</strong>: The evolution of a <em>closed</em> quantum system is described by a <em>unitary transformation</em> That is, the state $\rho$ of the system at time $t_1$ is related to the state $\rho’$ of th esystem at time $t_2$ by a unitary operator $U$ which depends only on the times $t_1$ and $t_2$,</p><script type="math/tex; mode=display">\rho' = U\rho U^\dagger</script><p><strong>Postulate 3</strong>: Quantum measurements are described by a collection $\{M_m\}$ of <em>measurement operators</em>. These are operators acting on the state space of the system being measured. The index $m$ refers to the measurement outcomes that may occur in the experiment. If the state of the quantum system is $\rho$ immediately before the measurement then the probability that result $m$ occurs is given by</p><script type="math/tex; mode=display">p(m)=\text{tr}(M^\dagger_mM_m\rho)</script><p>and the state of the system after the measurement is </p><script type="math/tex; mode=display">\frac{M_m\rho M^\dagger_m}{\text{tr}(M^\dagger_mM_m\rho)}</script><p>The measurement operators satisfy the <em>completeness equation</em>,</p><script type="math/tex; mode=display">\sum_m M^\dagger_mM_m=I</script><p><strong>Postulate 4</strong>: The state space of a composite physical system is the tensor product of the state spaces of the compoenet physical systems. Moreover, if we have systems numbered $1$ through $n$, and system number $i$ is prepared in the state $\rho_i$, then the joint state of the total system is $\rho_1\otimes \rho_2\otimes \dots \otimes \rho_n$</p>
          </div><p></p>
<div class="note success">
            <p><em>Theorem 2.6</em> (<strong>Unitary freedom in the ensemble for density matrices</strong>) The sets $\vert\tilde{\psi}_i\rangle$ amd $\vert\tilde{\varphi}_j\rangle$ generate the same density matrix if and only if</p><script type="math/tex; mode=display">\vert\tilde{\psi}_i\rangle=\sum_j u_{ij}\vert\tilde\varphi_j\rangle</script><p>where $u_{ij}$ is a unitary matrix of complex numbers, with indices $i$ and $j$, and we ‘pad’ whichever set of vectors $\vert\tilde\psi_i\rangle$ or $\vert\tilde\varphi_j\rangle$ is smaller with additional vectors $0$ so that the two sets have the same number of elements.</p><p>Also, $\rho=\sum_ip_i\vert\psi_i\rangle\langle\psi_i\vert=\sum_jq_j\vert\varphi_j\rangle\langle\varphi_j\vert$ if and only if</p><script type="math/tex; mode=display">\sqrt{p_i}\vert\psi_i\rangle=\sum_j u_{ij}\sqrt{q_j}\vert\varphi_j\rangle</script><p>where $u_{ij}$ is the element of some unitary matrix $U$.</p><p><em>Proof</em><br>Suppose $\vert\tilde\psi_i\rangle=\sum_ju_{ij}\vert\tilde\varphi_j\rangle$ for some unitary $u_{ij}$. Then</p><script type="math/tex; mode=display">\begin{align*}\sum_i\vert\tilde\psi_i\rangle\langle\tilde\psi_i\vert &= \sum_{ijk}u_{ij}u_{ik}^*\vert\tilde\varphi_j\rangle\langle\tilde\varphi_k\vert\\    &= \sum_{jk}\left(\sum_i u_{ki}^\dagger u_{ij}\right)\vert\tilde\varphi_j\rangle\langle\tilde\varphi_k\vert\\    &= \sum_{jk}\delta_{kj}\vert\tilde\varphi_j\rangle\langle\tilde\varphi_k\vert\\    &= \sum_j \vert\tilde\varphi_j\rangle\langle\tilde\varphi_j\vert\end{align*}</script><p>Conversely, suppose</p><script type="math/tex; mode=display">A=\sum_i\vert\tilde\psi_i\rangle\langle\tilde\psi_i\vert=\sum_j\vert\tilde\varphi_j\rangle\langle\tilde\varphi_j\vert</script><p>Let $A=\sum_k\lambda_k\vert k\rangle\langle k\vert$ be a spectral decomposition for $A$ such that states $\vert k \rangle$ are orthonormal, and the $\lambda_k$ are strictly positive. Our strategy is to relate the states $\vert\tilde\psi_i\rangle$ to the states $\vert\tilde k\rangle\equiv\sqrt{\lambda_k}\vert k\rangle$, and similarly relate the states $\vert \tilde\varphi_j\rangle$ to the states $\vert\tilde k\rangle$. Combining the two relations will give the result. Let $\vert\psi\rangle$ be any vector orthonormal to the space spanned by the $\vert\tilde k\rangle$, so $\langle \psi\vert\tilde k\rangle\langle\tilde k\vert\psi\rangle=0$ for all $k$, abd thus we see that</p><script type="math/tex; mode=display">0=\langle\psi\vert A\vert\psi\rangle=\sum_i\langle\psi\vert\tilde\psi_i\rangle\langle\tilde\psi_i\vert\psi\rangle=\sum_i\vert\langle\psi\vert\tilde\psi_i\rangle\vert^2</script><p>Thus $\langle\psi\vert\tilde\psi_i\rangle=0$ for all $i$ and all $\vert\psi\rangle$ orthonormal to the space spanned by the $\vert\tilde k\rangle$. It follows that each $\vert\tilde\psi\rangle$ can be expressed as a linear combination of the $\vert\tilde k\rangle$, $\vert\tilde\psi_i\rangle=\sum_kc_{ik}\vert\tilde k\rangle$. Since $A=\sum_k\vert \tilde k \rangle\langle\tilde k\vert=\sum_i\vert\tilde\psi_i\rangle\langle\tilde\psi_i\vert$ we see that</p><script type="math/tex; mode=display">\sum_k\vert\tilde k\rangle\langle \tilde k\vert = \sum_{kl}\left(\sum_i c_{ik}c_{il}^*\right)\vert\tilde k\rangle\langle\tilde l\vert</script><p>The operator $\vert\tilde k\rangle\langle \tilde l\vert$ are easily seen to be linearly independent, and thus it must be that $\sum_i c_{ik}c_{il}^*=\delta_{kl}$. This ensures that we may append extra columns to $c$ to obtain a unitary matrix $v$ such that $\vert\tilde\psi_i\rangle=\sum_kv_{ik}\vert\tilde k\rangle$, where we have appended zero vectors to the list of $\vert\tilde k \rangle$. Similarly, we can find a unitary matrix $w$ such that $\vert\tilde\varphi_j\rangle=\sum_kw_{jk}\vert\tilde k\rangle$. Thus $\vert\tilde\psi_i\rangle=\sum_j u_{ij}\vert\tilde\varphi_j\rangle$<br>, where $u=vw^\dagger$ is unitary.</p>
          </div>
<h3 id="2-4-3-The-reduced-density-operator" class="heading-control"><a href="#2-4-3-The-reduced-density-operator" class="headerlink" title="2.4.3 The reduced density operator"></a>2.4.3 The reduced density operator<a class="heading-anchor" href="#2-4-3-The-reduced-density-operator" aria-hidden="true"></a></h3><p>Suppose we have physical systems $A$ and $B$, whose state is described by a density operator $\rho^{AB}$.</p>
<ul>
<li><p>The <strong>reduced density operator</strong> for system $A$ is</p>
<script type="math/tex; mode=display">
\rho^A\equiv \text{tr}_B(\rho^{AB})</script><p>where $\text{tr}_B$ is called <strong>partial trace</strong> over system $B$, defined by</p>
<script type="math/tex; mode=display">
\text{tr}_B(\vert a_1\rangle\langle a_2\vert\otimes\vert b_1\rangle\langle b_2\vert)\equiv\vert a_1\rangle\langle a_2\vert \text{tr}(\vert b_1\rangle\langle b_2\vert)</script></li>
<li><p>Suppose a quantum system is in the product state $\rho^{AB}=\rho\otimes \sigma$, where $\rho$ is a density operator for system $A$, and $\sigma$ is a density operator for system $B$. Then</p>
<script type="math/tex; mode=display">
\rho^A=\text{tr}_B(\rho\otimes\sigma)=\rho\text{tr}(\sigma) = \rho</script></li>
</ul>
<h2 id="2-5-The-Schmidt-decomposition-and-purification" class="heading-control"><a href="#2-5-The-Schmidt-decomposition-and-purification" class="headerlink" title="2.5 The Schmidt decomposition and purification"></a>2.5 The Schmidt decomposition and purification<a class="heading-anchor" href="#2-5-The-Schmidt-decomposition-and-purification" aria-hidden="true"></a></h2><div class="note success">
            <p><em>Theorem 2.7</em> (<strong>Schmidt decomposition</strong>) Suppose $\vert\psi\rangle$ is a pure state of a composite system, $AB$. Then there exist orthonormal states $\vert i_A\rangle$ for system $A$, and orthonormal states $\vert i_B\rangle$ of system $B$ such that</p><script type="math/tex; mode=display">\vert \psi\rangle = \sum_i \lambda_i \vert i_A\rangle\vert i_B\rangle</script><p>where $\lambda_i$ are non-negative real numbers satisfying $\sum_i \lambda_i^2=1$ known as <em>Schmidt co-efficients</em>.</p><p><em>Proof</em><br>Let $\vert j\rangle$ and $\vert k\rangle$ be any fixed orthonormal bases for systems $A$ and $B$, respectively. Then $\vert \psi\rangle$ can be written</p><script type="math/tex; mode=display">\vert \psi\rangle = \sum_{jk} a_{jk}\vert j\rangle \vert k\rangle</script><p>for some matrix $A$ of complex numbers $a_{jk}$. By the singular value decomposition $A=UDV$, where $D$ is a positive, diagonal matrix. $U$ and $V$ are unitary matrices. Thus</p><script type="math/tex; mode=display">\vert \psi\rangle = \sum_{ijk}u_{ji}d_{ii}v_{ik}\vert j\rangle\vert k\rangle</script><p>Defining $\vert i_A\rangle\equiv\sum_{j}u_{ji}\vert j\rangle$, $\vert i_B\rangle\equiv\sum_k v_{ik}\vert k\rangle$, and $\lambda_i\equiv d_{ii}$, this gives</p><script type="math/tex; mode=display">\vert\psi\rangle\equiv\sum_i\lambda_i\vert i_A\rangle\vert i_B\rangle</script><p>It is easy to check that $\vert i_A\rangle$ forms an orthonormal set, from the unitarity of $u$ and the orthonormality of $\vert j\rangle$, and similarlt that the $\vert i_B\rangle$ form an orthonormal set.</p>
          </div>
<ul>
<li>$\vert i_A\rangle$ and $\vert i_B\rangle$ are called <em>Schmidt bases</em> for $A$ and $B$, respectively.</li>
<li>$\lambda_i$ is called <em>Schmidt co-efficients</em> and the number of $\lambda_i$ is called <em>Schmidt number</em> or <em>Schmidt rank</em> for the state $\vert\psi\rangle$.<ul>
<li>The Schmidt number is preserved under unitary transformations on system $A$ or system $B$ alone.</li>
<li>$\vert\psi\rangle=\sum_i\lambda_i\vert i_A\rangle\vert i_B\rangle \xrightarrow{U_A} U_A\vert\psi\rangle=\sum_i\lambda_i\left(U_A\vert i_A\rangle\right)\vert i_B\rangle$</li>
</ul>
</li>
</ul>
<p>Suppose we are given a state $\rho^A$ of a quantum system $A$. It is possible to introduce another system, which we denote $R$, and <em>define</em> a <em>pure state</em> $\vert AR\rangle$ for the joint system $AR$ such that $\rho^A=\text{tr}_R(\vert AR\rangle\langle AR\vert)$. This procedure is called <em>purification</em>. </p>
<ul>
<li>We call the system $R$ a <em>reference</em> system.</li>
<li>Only a mathematical procedure</li>
<li>Usful in simplifying proofs because pure states are easier to manipulate</li>
</ul>
<div class="note success">
            <p><em>Theorem</em> Every density matrix $\rho$ can be purified.</p><p><em>Proof</em><br>To prove that we can purify any state in system $A$, we show that we can construct a system $R$ and purification $\vert AR\rangle$ for $\rho^A$:</p><p>Suppose the orthonormal decomposition for $\rho^A=\sum_i p_i\vert i^A\rangle\langle i^A\vert$. We introduce a system $R$ which has the same state space as system $A$, with orthonormal basis states $\vert i^R\rangle$, and define a pure state for the combined system:</p><script type="math/tex; mode=display">\vert AR\rangle\equiv\sum_i \sqrt{p_i}\vert i^A\rangle\vert i^R\rangle</script><p>Verify that $\vert AR\rangle$ is actually a purification of $\rho^A$</p><script type="math/tex; mode=display">\begin{align*}\text{tr}_R(\vert AR\rangle\langle AR\vert)    &= \sum_{ij}\sqrt{p_ip_j}\vert i^A\rangle\langle j^A\vert \text{tr}(\vert i^R\rangle\langle j^R\vert)\\    &= \sum_{ij}\sqrt{p_ip_j}\vert i^A\rangle\langle j^A\vert \delta_{ij}\\    &= \sum_i p_i\vert i^A\rangle\langle i^A\vert\\    &= \rho^A\end{align*}</script>
          </div>
<h2 id="2-6-EPR-and-the-Bell-inequality" class="heading-control"><a href="#2-6-EPR-and-the-Bell-inequality" class="headerlink" title="2.6 EPR and the Bell inequality"></a>2.6 EPR and the Bell inequality<a class="heading-anchor" href="#2-6-EPR-and-the-Bell-inequality" aria-hidden="true"></a></h2><blockquote>
<p>待補</p>
</blockquote>
</body></html>]]></content>
      <categories>
        <category>Quantum Computing</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Quantum Computing</tag>
      </tags>
  </entry>
  <entry>
    <title>[Note] 神的語言 Metaprogramming: one_of</title>
    <url>/Ending2015a/58824/</url>
    <content><![CDATA[<html><head></head><body><h2 id="發想" class="heading-control"><a href="#發想" class="headerlink" title="發想"></a>發想<a class="heading-anchor" href="#發想" aria-hidden="true"></a></h2><p>最近在寫程式的時候遇到一個情景，讓我非常困擾。<br>下面這個情況我不需要多加解釋，應該很多人也都有遇過類似這種困擾。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">test_status</span><span class="params">(STATUS_t unknown_status)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span>(unknown_status == STATUS_1 || unknown_status == STATUS_3 || unknown_status == STATUS_5 || unknown_status == STATUS_7)</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">// do some stuff</span></span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(unknwon_status == STATUS_2 || unknown_status == STATUS_3 || unknown_status == STATUS_4)</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">// do some stuff</span></span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(unknwon_status == STATUS_5 || unknwon_status == STATUS_6 || unknwon_status == STATUS_9 || unknwon_status == STATUS_11 || unknwon_status == STATUS_27 || unknwon_status == STATUS_38)</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">//do some stuff</span></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<a id="more"></a>
<p>當你遇到狀況 1、3、5、7 時要處理一些事情，遇到狀況 2、3、4 時要處理一些事情，最後遇到狀況 5、6、9、11、27、38 的時候又要再處理一些事情….，可能有些人遇到的，後面還要再拉更多的 if 跟重複寫一堆相同且沒有意義的變數名 (unknown_status) 跟operator (||)。不但程式變得很長，不容易閱讀，你也很容易在寫這一長串的時候不小心出錯，例如 operator == 寫成 operator = ，結果還 de 不出 bug，諸如此類的小陷阱。</p>
<p>當然你可以抱怨說，到底是誰這麼沒水準，定義這種沒有規則的 STATUS。但是有時候可能你因為一些被限制的因素而只能使用這種不符合你預期的規則的 lib ，你也無從選擇只好接受。</p>
<p>於是就讓我萌生了一些想法：<strong>我有沒有辦法用一個很簡單的表達式來省略掉這些高度重複的變數名、還有 operator</strong>。當然其實我早就知道這是可行的，而且方法非常多，隨便想都可以想的到 3~5種偷懶的方法，像是開個 vector 把狀況們都捆成一包，再用 for 回圈去檢查、或是用 std::any_of 搭配 lambda function 的方式解決，又甚至自己重新 mapping 一次 STATUS，變成可以使用 Binary OR 的方式檢查。</p>
<p>問題就在於，我要如何在解決問題的同時又能夠解決的<strong>漂亮</strong>，這是一個很大的問題。我當然我也可以選擇不動腦就寫一堆垃圾 Code 來解決這種不起眼的小問題，但是這就不是我的 Style 啦。於是我決定要動手設計了一個新的 operator (我稱他是 operator 啦，雖然他只是一堆 function 跟 struct 的疊加)，這個 operator 的特點就是看起來要極其順眼，非常容易使用，最重要的是<strong>在 compile 之後的效能要能夠跟原本的暴力破解垃圾 Code 不相上下。</strong></p>
<p>於是我第一個想到的 operator 就是 one_of，這是我的目標：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">test_status</span><span class="params">(STATUS unknown_status)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span>(unknown_status == one_of(STATUS_1, STATUS_3,</span><br><span class="line">                                STATUS_5, STATUS_7))</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">//do some stuff</span></span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(unknwon_status == one_of(STATUS_2, STATUS_3, STATUS_4))</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">// do some stuff</span></span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(unknwon_status == one_of(STATUS_5, STATUS_6,</span><br><span class="line">                                STATUS_9, STATUS_11,</span><br><span class="line">                                STATUS_27, STATUS_38))</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">//do some stuff</span></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>跟原本的寫法比起來是不是變得更順眼、更易讀、更易寫（不容易出錯）？這個 operator 非常口語化的詮釋了我想做的事情：</p>
<ul>
<li>if: 當</li>
<li>unknown_status: 某變數</li>
<li>==: 等於</li>
<li>one_of: 下列其中一個</li>
<li>STATUS_1, STATUS_3, STATUS_5, STATUS_7</li>
<li>我就 do some stuff</li>
</ul>
<h2 id="目標" class="heading-control"><a href="#目標" class="headerlink" title="目標"></a>目標<a class="heading-anchor" href="#目標" aria-hidden="true"></a></h2><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(unknown_status == one_of(STATUS_1, STATUS_3,</span><br><span class="line">                            STATUS_5, STATUS_7))</span><br><span class="line">{</span><br><span class="line">        <span class="comment">//do some stuff</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>我想要定義一個新的 operator one_of 來解決掉一堆難看的垃圾 Code 問題。而且他擁有以下特點：</p>
<ul>
<li>易寫、易讀</li>
<li>可以接受不定個數的參數 (可以使用 parameter pack 實現)</li>
<li>可以接受任何型態 (可以使用 template 實現)</li>
<li>編譯後效能可以跟原本的垃圾 Code 一樣好 (metaprogramming 實現)</li>
</ul>
<p>你看看，從上列開出的特點來看，就是只能用 metaprogramming 實現了。</p>
<h2 id="實現" class="heading-control"><a href="#實現" class="headerlink" title="實現"></a>實現<a class="heading-anchor" href="#實現" aria-hidden="true"></a></h2><p>好了，現在有了目標，問題就在於要如何實現這個 one_of？</p>
<p>首先，可以看的出來，我們要把 unknown_status 跟 one_of(…) 做比較，能做到的方法其實就幾個：第一個是 one_of 可能是一個 struct/class，他的 Constructor 能夠接納無限個 parameters，然後我拿某個變數 unknown_status 跟這個 struct/class 做 <code>==</code> 比較 (operator overloading)。第二個是 one_of 可能是一個 function，他能夠接納無限個 parameters，呼叫後會回傳一個包好的 struct/class (我傳入的 parameters 都在裏面)，然後再用 unknown_status 去跟這個 struct/class 做 <code>==</code> 比較 (operator overloading)。</p>
<p>我們使用第一個方法來實作，流程是這樣，one_of 的 constructor 可以接受 parameter pack，之後我們將 parameter pack 存到 <code>std::tuple</code> 裏面放著，等到呼叫 == 時再從 <code>std::tuple</code> unpack 一個一個做判斷：</p>
<p>所以第一步先建立好 one_of 的 constructor (這邊使用 struct 是因為 struct 的 member 預設是 public，我們不需要再多一步用 <code>public:</code> 來指定)</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">one_of</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span>... Ts></span><br><span class="line">    one_of(Ts&&...args);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>這邊的語法 <code>typename...Ts</code> 就是 parameter pack，意思就是我會傳入不定個數的參數。不多解釋，不懂的自己去 <code>google</code>。接下來我們要把傳進來的參數 <code>args</code> 包成一份 <code>std::tuple</code>，而因為 <code>std::tuple</code> 也需要不定個數的欄位，我們勢必必須把 <code>struct one_of</code> 也宣告成 template struct：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><tuple></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><utility></span></span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">one_of</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="built_in">std</span>::tuple<ArgTypes...> args;  <span class="comment">//tuple</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span>... Ts></span><br><span class="line">    one_of(Ts&&...args) : args(<span class="built_in">std</span>::forward<Ts>(args)...){};  <span class="comment">// constructor</span></span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure>
<p>這邊我們使用 <code>std::tuple</code> 因此我們必須 <code>#include <tuple></code>，還有我們使用 <code>std::forward</code> 來 unpack parameter pack 變成一列 arguments 傳進 tuple 的 constructor 裏面結束這回合，因此我們還必須 <code>#include <utility></code>。</p>
<p>但是接下來我意識到一件事情，如果我們使用這種方法來設計我們的 one_of 的話，由於現在 one_of struct 已經變成 template struct 了，到時候呼叫的方法就會變成:<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(unknown_status == one_of<STATUS_t, STATUS_t, STATUS_t, STATUS_t>(STATUS_1, STATUS_3, STATUS_5, STATUS_7))</span><br></pre></td></tr></tbody></table></figure><p></p>
<p><em>喔不！</em> 我們必須指定傳入參數的型態給 template！這可不是我們當初所預期的 one_of 啊！<br>但是不用擔心，他還有救，讓我們來給他包上一層 helper function：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><tuple></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><utility></span></span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="built_in">std</span>::tuple<ArgTypes...> args;  <span class="comment">//tuple</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span>... Ts></span><br><span class="line">    _type_one_of(Ts&&...args) : args(<span class="built_in">std</span>::forward<Ts>(args)...){};  <span class="comment">// constructor</span></span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line">constexpr auto one_of(ArgTypes&&... args) -> _type_one_of<ArgTypes...></span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> _type_one_of<ArgTypes...>(<span class="built_in">std</span>::forward<ArgTypes>(args)...);</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>我做了什麼事情呢？<br>首先，我把 <code>struct one_of</code> 改名了，改成 <code>struct _type_one_of</code> 接著我新增了一個 helper function 就是我們最愛的互動介面 <code>one_of</code>。他的功能就是當我們傳入參數到 one_of 時他會幫我們建立一個包好的 <code>struct _type_one_of</code> 這樣我們就不用自己手動包了！</p>
<p>helper function 的 return type 是 <code>_type_one_of<ArgTypes...></code> 而使用 <code>-></code> 的寫法稱做 trailing return type，他只是可以延後到 function declaration 後面才指定 return type，在這個 case，這樣寫跟把 return type 寫在前面其實沒有差別，單純只是我覺得因為 return type 長的比較醜，放在後面這樣比較好看。而回傳的東西一樣就是把東西 unpack。</p>
<p>除此之外，在寫 metaprogramming 的時候要記得，你希望 compiler 自動幫你拆開來的 function 都要加上 <code>constexpr</code> specifier，這樣 compiler 才會盡可能幫你拆開。而 <code>std::forward</code> 這個 function，很幸運的在 c++ 14 的時候已經改成 <code>constexpr</code>，因此這個 constructor 我很有信心 compiler 絕對會幫我們拆開來。</p>
<p>這樣就完成了我們的 one_of，接下來是要寫 <code>==</code> 的部份。我們可以用 operator overloading 來自定義一個 <code>operator==</code>，把任意 type 跟 <code>struct _type_one_of</code> 做比較：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>==(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> _type_one_of<ArgTypes...> &rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> rhs.__match_op(lhs, rhs.args);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊我們定義好了 <code>operator==</code> 的部份，而接下來因為我們需要把預先存起來的 <code>std::tuple</code> 拿出來用，因此我希望我們可以把 unpack tuple 的部份使用 struct 內部的 function 來實現。當然你也可以不要像我一樣，你也可以直接把 function 定義在外面。</p>
<p>接下來實作 unpack tuple 的部份，回到 <code>struct _type_one_of</code> 裏面：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span>{</span></span><br><span class="line">    <span class="built_in">std</span>::tuple<ArgTypes...> args;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span>... Ts></span><br><span class="line">    _type_one_of(Ts&&... args): args(<span class="built_in">std</span>::forward<Ts>(args)...) {}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> Inds = <span class="built_in">std</span>::make_index_sequence<<span class="keyword">sizeof</span>...(Ts)>></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple<Ts...> &tup) <span class="keyword">const</span></span><br><span class="line">        { <span class="keyword">return</span> __match_op_impl(lhs, tup, Inds{}); }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>首先我們在 <code>operator==</code> 裏面呼叫了 <code>__match_op</code> 這個 function，因此我們定義一下 <code>__match_op</code> 。第一個參數是型別為 <code>T</code> 的 <code>lhs</code> (left-hand-side)，第二個是我們的 tuple <code>tup</code>。然後我們產生一個 <code>std::integer_sequence</code>，並且呼叫 <code>__match_op_impl</code>。</p>
<p><code>std::make_index_sequence</code> 定義在 <code><utility></code>，是 c++ 14 才有的 type。他的功能是可以產生一個 template parameters 為一個數列的 class。而 <code>sizeof...(Ts)</code> 是 c++ 11 的語法，他其實是叫作 <code>sizeof...</code> operator，用途是計算 parameter pack 裏面元素的數量。因此當我呼叫 <code>std::make_index_sequence<sizeof...(Ts)></code> 時，假設 <code>Ts</code> 裏面有 5 個元素，他會產生一個長的像這樣的 type：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::index_sequence<<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>></span><br></pre></td></tr></tbody></table></figure><br>然後我們定義 template 的最後一個 parameter type 的預設 type 是這個東西，這樣我們就得到<br><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">typename</span> Inds = <span class="built_in">std</span>::index_sequence<<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>></span><br></pre></td></tr></tbody></table></figure><br>這個 Inds 是一個 class，因此我們在呼叫 <code>__match_op_impl</code> 並把他當參數傳入時，使用 <code>Inds{}</code> 等於是創建一個 object 的 instance。<p></p>
<p>之所以要用這種二段式呼叫的原因主要是因為 <code>std::tuple</code> 限制的關係，如果要取得 tuple 裏面的元素，我們必須使用 <code>std::get</code> 這個 function，而 <code>std::get</code> 這個 function 會需要指定元素的 Index。例如，如果要取出 tuple 的第<code>N</code>個元素，則我們必須這樣寫：<code>std::get<N>(tup)</code>。因此，我們利用兩段式呼叫，第一次呼叫先用 <code>std::make_index_sequence</code> 取得元素 Index 的 sequence 後再進行第二次呼叫，unpack tuple。</p>
<p>接下來是定義<code>__match_op_impl</code> 的部份：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span>{</span></span><br><span class="line">    <span class="built_in">std</span>::tuple<ArgTypes...> args;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span>... Ts></span><br><span class="line">    _type_one_of(Ts&&... args): args(<span class="built_in">std</span>::forward<Ts>(args)...) {}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="built_in">std</span>::<span class="keyword">size_t</span>... I></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op_impl(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple<Ts...> &tup, <span class="built_in">std</span>::index_sequence<I...>) <span class="keyword">const</span></span><br><span class="line">        { <span class="keyword">return</span> __match_one_of_op(lhs, <span class="built_in">std</span>::get<I>(tup)...); }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> Inds = <span class="built_in">std</span>::make_index_sequence<<span class="keyword">sizeof</span>...(Ts)>></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple<Ts...> &tup) <span class="keyword">const</span></span><br><span class="line">        { <span class="keyword">return</span> __match_op_impl(lhs, tup, Inds{}); }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊我們定義了 <code>__match_op_impl</code> 函數，一樣，第一個參數是 <code>lhs</code>，第二個是 tuple <code>tup</code>，第三個是 Index sequence，他的 type 是 <code>std::index_sequence<I...></code> 由於實體變數我們並不是很 care (甚至這個 class 裏面根本沒包多少東西，重點是他的 template parameter pack)，所以我們第三個參數只寫 type 而沒有寫 variable 的名稱。我們在 template 裏面定義 Index sequence 的 template parameter pack <code>std::size_t... I</code>。這樣我們就可以用 <code>std::get<I>(tup)...</code> 來讓 Compiler 自動幫我們 unpack tuple。</p>
<p>接著我們呼叫 <code>__match_one_of_op</code>，我們第一個參數傳入 <code>lhs</code>，後面的參數則是用 <code>...</code> 來 unpack。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_one_of_op(<span class="keyword">const</span> T& lhs, ArgTypes&&... args)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">sizeof</span>...(args) == <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="literal">false</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> any( (lhs == <span class="built_in">std</span>::forward<ArgTypes>(args)) ...);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>這邊我們定義 <code>__match_one_of_op</code> 的內容，首先這先使用了一個 <code>if constexpr else</code> 的表達式，從 c++ 17 開始可以指定 if else 是 <code>constexpr</code>，這樣 compiler 就會幫我們拆開來。而如果你想要使用 <code>constexpr</code> 的 <code>if</code> <code>else if</code> <code>else</code> 的話，你可以這樣寫：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="comment">/*...*/</span>)</span></span></span><br><span class="line"><span class="function"><span class="keyword">else</span> <span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="comment">/*...*/</span>)</span></span></span><br><span class="line"><span class="function"><span class="keyword">else</span></span></span><br></pre></td></tr></tbody></table></figure><br>實際上 <code>else if</code> 就是 <code>else{ if(/*...*/){} }</code>。<p></p>
<p>如果 <code>sizeof...(args) == 0</code> 的話，也就是如果 parameter pack 裏面一個東西都沒有，我們直接 return false。否則我們要把 <code>args</code> 用 <code>...</code> 拆開來一個一個跟 <code>lhs</code> 做比較。</p>
<p>到這邊，如果 compiler 把 any 裏面的東西拆開來，他會得到類似這樣的東西 (這只是pseudocode)：<br></p><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">any( lhs==args[0], lhs==args[1], lhs==args[2], ...) </span><br></pre></td></tr></tbody></table></figure><p></p>
<p>而由於我們當初定義 <code>one_of</code> 是：<strong>只要其中一項等於 <code>lhs</code></strong> 就會回傳 <code>true</code>，因此我們可以寫一個 <code>any</code> 這個 function 來負責統整所有比較的結果，只要有其中一個 expression 是 <code>true</code>，則 <code>any</code> 會回傳 <code>true</code>，否則回傳 <code>false</code>。</p>
<p>因此我們定義 <code>any</code> 這個 function<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="function"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="title">any</span><span class="params">(ArgTypes&&... args)</span> </span>{ <span class="keyword">return</span> (... || args); }</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊我們使用 c++ 17 的語法 <code>(... || args)</code> 這個語法叫作 fold expression，他的用途就是他會把 parameter packs 拆開後中間全部用同樣的 <strong>operator</strong> 連接起來，因此會得到類似這樣的效果 (這只是pseudocode)：<br></p><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">(... args[2] || args[1] || args[0])</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊需要注意的是我寫成 left fold 的型式，但其實也可以使用普通的 right fold 型式<br><code>(args || ...)</code>。差別只在於展開的方向不同。</p>
<p>left fold:<br></p><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">(... args[2] || args[1] || args[0])</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>right fold:<br></p><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">(args[0] || args[1] || args[2] ...)</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>詳細請看：<a href="https://en.cppreference.com/w/cpp/language/fold">fold expression(since C++17) - cppreference.com</a></p>
<p>到這邊我們就真正完成了我們的 one_of operator，下面是完整的 code：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><tuple></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><utility></span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="function"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="title">any</span><span class="params">(ArgTypes&&... args)</span> </span>{ <span class="keyword">return</span> (... || args); }</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_one_of_op(<span class="keyword">const</span> T& lhs, ArgTypes&&... args)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">sizeof</span>...(args) == <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="literal">false</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> any( (lhs == <span class="built_in">std</span>::forward<ArgTypes>(args)) ...);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span>{</span></span><br><span class="line">    <span class="built_in">std</span>::tuple<ArgTypes...> args;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span>... Ts></span><br><span class="line">    _type_one_of(Ts&&... args): args(<span class="built_in">std</span>::forward<Ts>(args)...) {}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="built_in">std</span>::<span class="keyword">size_t</span>... I></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op_impl(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple<Ts...> &tup, <span class="built_in">std</span>::index_sequence<I...>) <span class="keyword">const</span></span><br><span class="line">        { <span class="keyword">return</span> __match_one_of_op(lhs, <span class="built_in">std</span>::get<I>(tup)...); }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> Inds = <span class="built_in">std</span>::make_index_sequence<<span class="keyword">sizeof</span>...(Ts)>></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple<Ts...> &tup) <span class="keyword">const</span></span><br><span class="line">        { <span class="keyword">return</span> __match_op_impl(lhs, tup, Inds{}); }</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line">constexpr auto one_of(ArgTypes&&... args) -> _type_one_of<ArgTypes...></span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> _type_one_of<ArgTypes...>(<span class="built_in">std</span>::forward<ArgTypes>(args)...);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>==(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> _type_one_of<ArgTypes...> &rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> rhs.__match_op(lhs, rhs.args);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>接著你可以試試看使用這個程式<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> g;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cin</span> >> g;</span><br><span class="line">    <span class="keyword">if</span>(g == one_of(<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>))</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"It's a multiple of 10 !"</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<ul>
<li>one_of 裏面可以塞不定個數的參數</li>
<li>one_of 可以塞任何型態的變數</li>
</ul>
<p>你可以在<a href="https://godbolt.org/z/LjS1WS">這邊</a> 比較一下看看編譯後的結果是不是跟原本的垃圾 Code 一模一樣。</p>
<p>除此之外，因為我們使用 template parameter pack 的關係，one_of 可以傳入每個型態都不一樣的參數：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> g;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cin</span> >> g;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> i = <span class="number">35</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> f = <span class="number">12.6</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span> d = <span class="number">-4.9</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span> str = <span class="string">"Hello"</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span><<span class="keyword">double</span>> vd{<span class="number">-1.4</span>, <span class="number">6.8</span>};</span><br><span class="line">    <span class="keyword">if</span>(g == one_of(i, f, d, str, vd)) <span class="comment">// int, float, double, string, vector</span></span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"g is in the set !"</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>但是這樣你必須自己 overload 不同型態間的 operator==，不然 compile 的時候會出錯。</p>
<h2 id="進階" class="heading-control"><a href="#進階" class="headerlink" title="進階"></a>進階<a class="heading-anchor" href="#進階" aria-hidden="true"></a></h2><h3 id="做比較測試" class="heading-control"><a href="#做比較測試" class="headerlink" title="做比較測試"></a>做比較測試<a class="heading-anchor" href="#做比較測試" aria-hidden="true"></a></h3><p>雖然 one_of 已經可以跟任意型態做比較了，但是實際上這麼做是非常危險的。如上所言，有時候使用者並不會記得要實作出對應任意型態的 operator==，甚至，為每一對型態實作一組 operator== 是非常費時的時間，因此我們有沒有辦法寫個功能讓 compiler 自動判定兩個型態能不能做 == 比較，如果可以的話就做比較，不行的話就回傳 <code>false</code>？</p>
<p>這裡我們就必須使用一個 metaprogramming 的特殊技巧叫作 <strong>SFINAE</strong>，他的核心理念就是實作一個 General 的 template，再實作一個專做測試用的 Specialized template，如果我們想要的功能能夠吻合到 Specialized template 表示測試合格(e.g. 測試某 Type 擁有某個 member、測試某 Type 有支援某 Operator等等)，如果不合格，Compiler 也會自動把他吻合到 General 的 template 上面而不會跳出 Compiler error。這個的運作原理不難理解，我在這邊就不多做解釋，想知道的自行 <code>google</code>。</p>
<p>因此我們繼續更改 Code，我希望在 <code>__match_one_of_op</code> 裏面呼叫 <code>any</code> 前先加入比較測試，<strong>讓 Compiler 幫我們檢查</strong>兩個型態能不能做比較 <em>(重點：要 Compiler 幫我們檢查！)</em>：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_one_of_op(<span class="keyword">const</span> T& lhs, ArgTypes&&... args)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">sizeof</span>...(args) == <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="literal">false</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> any( (__match_comparable_one_of_op(lhs, args)) ...);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊我們直接呼叫<code>__match_comparable_one_of_op</code> 並用 <code>...</code> 來幫我們逐一配對檢查看能不能做比較。</p>
<p>這邊的實作方式非常多，我們也可以使用 tag dispatching 、SFINAE、或是<code>std::enable_if</code> 的方式實作，也可以直接用 <code>if constexpr else</code> 的方式實作，而這次我們就先從簡用 <code>if constexpr else</code> 的方式實作。實際上我覺得用 SFINAE 實作我覺得比較優美，因為在 metaprogramming 中出現 <code>if else</code> 這種東西在瀏覽 Code 的時候感覺就是特別礙眼。<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> LT& lhs, <span class="keyword">const</span> RT& rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op<LT, RT>::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> lhs</span>==rhs;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊首先使用 struct type 的 SFINAE <code>_whether_support_op</code> 來判斷 <code>LT</code> 跟 <code>RT</code> 這兩個型態能不能做比較。<br>註：SFINAE 也有 function type 的，有機會再介紹。</p>
<p>如果 <code>LT</code> 跟 <code>RT</code> 可以做比較，則回傳 <code>lhs == rhs</code> 比較結果，否則回傳 <code>false</code>。這邊注意因為他是 <code>constexpr</code> specified 的 <code>if else</code> 因此如果 <code>if</code> 的條件不成立則 Compiler 不會編譯 <code>if</code> 裏面的內容。有需要的話，我們甚至可以印出一些資訊來看看程式的運作：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> LT& lhs, <span class="keyword">const</span> RT& rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op<LT, RT>::value)</span></span></span><br><span class="line"><span class="function">    </span>{</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"[Comparable] "</span>;</span><br><span class="line">        <span class="keyword">return</span> lhs==rhs;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"[Not Comparable] "</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    }</span><br><span class="line">        </span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>接下來就是實作 struct 型的 SFINAE <code>_whether_support_op</code>：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>, <span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span><>></span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">{};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span><LT, RT, std::void_t<</span></span><br><span class="line"><span class="class">        decltype(std::declval<LT>()==std::declval<RT>) >> :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">{};</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>我也是最近才注意到 c++ 17 中出現了一個新的 Type 叫作 <code>std::void_t</code> 而且根據 <a href="https://en.cppreference.com/w/cpp/types/void_t">cppreference.com</a> 的資訊，這個 Type 就是專門拿來玩 SFINAE 的！由於 <code>std::false_type</code>、<code>std::true_type</code>、<code>std::void_t</code> 都是出自 <code><type_traits></code> ，因此必須加上 <code>#include <type_traits></code>。而 <code>std::void_t</code> 其實有個很有趣的事情就是不管我們塞入什麼型態，最後的 Type 他都會是 void。他的定義類似這樣：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>...></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">void_t</span> = <span class="keyword">void</span>;</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>所以不管我們塞什麼型態給他，他都是 void。</p>
<p>首先先從 General 的 <code>struct _whether_support_op</code> 開始 (他其實有個名字叫作 primary template)，這邊定義他的 template 參數是 <code><typename, typename, typename = std::void_t<> ></code>。之所以都不寫名字是因為我們根本不 care 那個變數型態（簡稱變態）叫作什麼名字，反正他就是會有三個變態進來，然後第三的變態預設為 <code>std::void_t<></code>就是為了玩 SFINAE 用的。</p>
<p>如果 Compiler 在配對 <code>_whether_support_op</code> 的時候配對到這個 General 版的 ，就表示我們想要的功能無法使用，因此我們讓這種 General 版的 struct 繼承 <code>std::false_type</code>。繼承這個 <code>std::false_type</code> 的時候，<code>_whether_support_op</code> 會繼承到一個 <code>static member</code> 叫作 <code>value</code>，而且 <code>value</code> 值會是 <code>false</code>。因此當我們呼叫 <code>_whether_support_op<LT, RT></code> 後去取得他的 <code>value</code> 值，會得到 <code>false</code>：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">static_assert</span>(<span class="literal">false</span> == _whether_support_op<LT, RT>::value);</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>接下來定義一個 Specialized 的 struct <code>_whether_support_op</code> (specialized template)，這邊 template 只需要定義兩個變態 <code><typename LT, typename RT></code> 就可以了，因為第三個變態是我們要玩 SFINAE 用的。接下來就是客製化，這個行為稱做 partial specialization，我們只真對部份的變態做 specialization：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span><LT, RT, std::void_t</*..specialize..*/> ></span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>可以看出，<code>LT</code>、<code>RT</code> 前兩個變態沒有特別 specialize， 但是第三個變態，我們指定他是 <code>std::void_t</code>，並且在 <code>std::void_t</code> 的 template 變態塞入 <code>decltype(std::declval<LT>()==std::declval<RT>)</code>。如果 Compiler 成功配對這個 struct 的話，他會使用這個 specialized 的 struct，而這個 specialized 的 struct 有繼承 <code>std::true_type</code>， 同 <code>std::false_type</code>，如果去取他的 <code>value</code> 值會得到 <code>true</code>：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">static_assert</span>(<span class="literal">true</span> == _whether_support_op<LT, RT>::value);</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>至於，解釋 <code>decltype(std::declval<LT>()==std::declval<RT>)</code> 這一串東西是什鬼，要先從 <code>decltype</code> 開始解釋。<code>decltype</code> 跟 <code>std::declval</code> 都不是新東西了，他們在 c++ 11就存在了。<code>decltype</code> 的用途是可以得到 <code>decltype(expression)</code> 裏面 expression 的回傳型態。例如：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> x = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">int</span> y = <span class="number">5</span>;</span><br><span class="line"><span class="keyword">decltype</span>(x+y) z = x+y;</span><br></pre></td></tr></tbody></table></figure><br>我們可以知道 <code>x</code> 和 <code>y</code> 都是 <code>int</code> 型態，所以 <code>x+y</code> 也會回傳 <code>int</code> 型態，利用 <code>decltype</code> 這個 operator，可以得到 <code>x+y</code> 的回傳型態 <code>int</code> 之後再宣告一個新變數 <code>z</code>，<code>z</code> 的型態就會是 <code>int</code>。<p></p>
<p><code>std::declval</code> 定義在 <code><utility></code> 裏面，他可以將一個指定的 Type 轉換成該 Type 的 Reference type，但是他並不會呼叫該 Type 的 Constructor，藉此我們可以呼叫他的 member function。所以 <code>std::declval<LT>()</code> 跟 <code>std::declval<RT>()</code> 就會分別產生一個 <code>LT</code>、<code>RT</code> 的 reference type <code>LT&&</code> 跟 <code>RT&&</code> ，我們可以呼叫他們的 member function，或是 operator。但是要注意的是，並不是只要用 <code>std::declval</code> 就可以無限上綱，首先他不會產生一個實體的 instance/object，再來就是他只能用在類似 <code>sizeof</code>、<code>decltype</code> 這類只需要 function definition 的 specifier 上，以及他還有一些規則，如果傳入的變態是 non cv-qualified (非 const 或 volatile) 或是 non ref-qualified (非 lvalue type <code>&</code>) 則會回傳 rvalue type <code>&&</code>，而如果傳入的參數是cv-qualified 或是 ref-qualified 則會回傳同樣的變態。詳細的自己 <code>google</code>。</p>
<p>總之，當我們呼叫 <code>std::declval<LT>()</code> 跟 <code>std::declval<RT>()</code> 時 Compiler 會產生這兩的變態的 reference type 接著使用 <code>std::declval<LT>() == std::declval<RT>()</code> 嘗試呼叫這兩個變態的 operator==，然後取得回傳的變態 <code>decltype(std::declval<LT>() == std::declval<RT>())</code>，然後將這個變態放入 <code>std::void_t<...></code>，最後放入 struct <code>_whether_support_op</code> 的第三個 argument。</p>
<p>當我們從外部宣告一個實體 struct 時 (e.g. <code>_whether_support_op<int, std::string></code> )，Compiler 會發生一系列事情，這邊就會關係到 Compiler 在呼叫 template function 或 template class 的決策流程：</p>
<ul>
<li>第一個階段會先進行 name lookup，找出對應名稱的 function / class</li>
<li>第二個階段會進行 template argument deduction，推導出所有 candidate function / class</li>
<li>第三個階段會進行篩選，選出最吻合的 function / class</li>
</ul>
<p>詳細請看：<a href="https://en.cppreference.com/w/cpp/language/template_argument_deduction">Template argument deduction - cppreference.com</a></p>
<p>以我們的例子來說，第一個階段的 name lookup，Compiler 可以得到我們有兩個 <code>_whether_support_op</code> 的 template struct。</p>
<p>第二個階段 argument deduction 就會產生變化了，首先他會將指定的型態 <code>int</code> 跟 <code>std::string</code> 帶入第一個 <code>_whether_support_op</code> (注意，到這邊為止，Compiler 都還不知道誰是 primary 誰是 specialized)。由於我們只有指定兩個型態，第三個我們使用預設的 <code>typename = std::void_t<></code>，Compiler 會產生第一個可行的候選名單 <code>_whether_support_op<int, std::string, void></code>，但是注意，這個候選名單第三個變態是 default，而非在宣告時指定的。</p>
<p>接下來 deduce 第二個 <code>_whether_support_op</code> 會產生兩種狀況。<br>狀況一：如果我們有宣告 <code>LT</code> 與 <code>RT</code> 的 operator==，則 <code>std::declval<LT>() == std::declval<RT>()</code> 判斷式會成立，<code>decltype</code>可以得到正確的回傳變態 (通常是 <code>bool</code>)，接著 <code>std::void_t<bool></code> 也能夠正常成立，最後得到完整的 <code>_whether_support_op<int, std::string, void></code>，這邊的第三個變態就是宣告時指定的，他是從 <code>std::void_t<></code> 特化成 <code>void</code> 的，因此 Compiler 會把這個 struct 判斷成是一種 specialization。而 specialized template 的優先權會大於 primary template，因此 Compiler 最後會選擇這個 template。這時候我們取出 <code>value</code> 值會得到 <code>true</code>。</p>
<p>狀況二：如果我們沒有宣告 <code>LT</code> 與 <code>RT</code> 的 operator==，則 <code>std::declval<LT>() == std::declval<RT>()</code> 判斷式無法成立，<code>decltype</code> 得不到正確的回傳變態，<code>std::void_t<...></code> 也無法成立，最後 Compiler 沒有辦法得到完整的 specialization，因此這個 struct 就會被 Compiler 從 candidate list 裏面剔除。Compiler 最後選擇使用 primary template。這時候我們取出 <code>value</code> 值會得到 <code>false</code>。</p>
<p>最後的程式碼：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><tuple></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><utility></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><type_traits></span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="function"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="title">any</span><span class="params">(ArgTypes&&... args)</span> </span>{ <span class="keyword">return</span> (... || args); }</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>, <span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span><>></span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">{};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span><LT, RT, std::void_t<</span></span><br><span class="line"><span class="class">        decltype(std::declval<LT>()==std::declval<RT>) >> :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">{};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> LT& lhs, <span class="keyword">const</span> RT& rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op<LT, RT>::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> lhs</span>==rhs;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_one_of_op(<span class="keyword">const</span> T& lhs, ArgTypes&&... args)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">sizeof</span>...(args) == <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="literal">false</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> any( (__match_comparable_one_of_op(lhs, args)) ...);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span>{</span></span><br><span class="line">    <span class="built_in">std</span>::tuple<ArgTypes...> args;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span>... Ts></span><br><span class="line">    _type_one_of(Ts&&... args): args(<span class="built_in">std</span>::forward<Ts>(args)...) {}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="built_in">std</span>::<span class="keyword">size_t</span>... I></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op_impl(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple<Ts...> &tup, <span class="built_in">std</span>::index_sequence<I...>) <span class="keyword">const</span></span><br><span class="line">        { <span class="keyword">return</span> __match_one_of_op(lhs, <span class="built_in">std</span>::get<I>(tup)...); }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> Inds = <span class="built_in">std</span>::make_index_sequence<<span class="keyword">sizeof</span>...(Ts)>></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple<Ts...> &tup) <span class="keyword">const</span></span><br><span class="line">        { <span class="keyword">return</span> __match_op_impl(lhs, tup, Inds{}); }</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line">constexpr auto one_of(ArgTypes&&... args) -> _type_one_of<ArgTypes...></span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> _type_one_of<ArgTypes...>(<span class="built_in">std</span>::forward<ArgTypes>(args)...);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>==(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> _type_one_of<ArgTypes...> &rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> rhs.__match_op(lhs, rhs.args);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>如此一來我們就可以用來做更狂的比較，還不會跳 Error 出來：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> T></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">is_in_the_set</span><span class="params">(<span class="keyword">const</span> T& X)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">if</span>(X == one_of(<span class="number">10</span>, </span><br><span class="line">                   <span class="number">23.5465</span>, </span><br><span class="line">                   <span class="string">"Hello"</span>, </span><br><span class="line">                   <span class="built_in">std</span>::<span class="built_in">string</span>(<span class="string">"foo"</span>), </span><br><span class="line">                   <span class="built_in">std</span>::<span class="built_in">vector</span><<span class="keyword">double</span>>{<span class="number">12.5</span>, <span class="number">64.5</span>},</span><br><span class="line">                   <span class="string">'c'</span>) )</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"X is in the set"</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    is_in_the_set(<span class="number">10</span>); <span class="comment">//true</span></span><br><span class="line">    is_in_the_set(<span class="built_in">std</span>::<span class="built_in">string</span>(<span class="string">"Hello"</span>)); <span class="comment">//true</span></span><br><span class="line">    is_in_the_set(<span class="built_in">std</span>::<span class="built_in">vector</span><<span class="keyword">float</span>>{<span class="number">0.1</span>, <span class="number">0.2</span>}); <span class="comment">//false</span></span><br><span class="line">    is_in_the_set(<span class="built_in">std</span>::<span class="built_in">vector</span><<span class="keyword">double</span>>{<span class="number">12.5</span>, <span class="number">64.5</span>}); <span class="comment">//true</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<h3 id="Generalization：套用到任意-comparison-operators" class="heading-control"><a href="#Generalization：套用到任意-comparison-operators" class="headerlink" title="Generalization：套用到任意 comparison operators"></a>Generalization：套用到任意 comparison operators<a class="heading-anchor" href="#Generalization：套用到任意-comparison-operators" aria-hidden="true"></a></h3><p>在上面的例子中我們是針對特定的 operator== 做設計，但是如果因為 one_of 實在太方便，我想要實作 one_of 也可以支援其他 operator 我是不是每次都得重頭設計一遍？其實不用，我們只需要連 operator 都當成是一個 template argument 傳進去就行了！因此開始設計ㄅ！</p>
<p>首先，由於 operator== 沒辦法直接當作 argument 傳入 template，因此我先把 operator== 用 <code>struct _op_equal_to</code> 包起來，並在每一個 template 上都加上一個 Fn 的變態：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><tuple></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><utility></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><type_traits></span></span></span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">op_equal_to</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="comment">//TODO</span></span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="function"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="title">any</span><span class="params">(ArgTypes&&... args)</span> </span>{ <span class="keyword">return</span> (... || args); }</span><br><span class="line"></span><br><span class="line"><span class="comment">//TODO</span></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>, <span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span><>></span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">{};</span><br><span class="line"></span><br><span class="line"><span class="comment">//TODO</span></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span><LT, RT, std::void_t<</span></span><br><span class="line"><span class="class">        decltype(std::declval<LT>()==std::declval<RT>) >> :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">{};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> LT& lhs, <span class="keyword">const</span> RT& rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="comment">//TODO</span></span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op<LT, RT>::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> lhs</span>==rhs;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_one_of_op(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> T& lhs, ArgTypes&&... args)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">sizeof</span>...(args) == <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="literal">false</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> any( (__match_comparable_one_of_op(op, lhs, args)) ...);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span>{</span></span><br><span class="line">    <span class="built_in">std</span>::tuple<ArgTypes...> args;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span>... Ts></span><br><span class="line">    _type_one_of(Ts&&... args): args(<span class="built_in">std</span>::forward<Ts>(args)...) {}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="built_in">std</span>::<span class="keyword">size_t</span>... I></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op_impl(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> T& lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple<Ts...> &tup, <span class="built_in">std</span>::index_sequence<I...>) <span class="keyword">const</span></span><br><span class="line">        { <span class="keyword">return</span> __match_one_of_op(op, lhs, <span class="built_in">std</span>::get<I>(tup)...); }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> Inds = <span class="built_in">std</span>::make_index_sequence<<span class="keyword">sizeof</span>...(Ts)>></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> T& lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple<Ts...> &tup) <span class="keyword">const</span></span><br><span class="line">        { <span class="keyword">return</span> __match_op_impl(op, lhs, tup, Inds{}); }</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line">constexpr auto one_of(ArgTypes&&... args) -> _type_one_of<ArgTypes...></span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> _type_one_of<ArgTypes...>(<span class="built_in">std</span>::forward<ArgTypes>(args)...);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>==(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> _type_one_of<ArgTypes...> &rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> rhs.__match_op(_op_equal_to{}, lhs, rhs.args);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>有加上 <code>TODO</code> 的都是還沒有完成的部份。首先我們已經可以在 <code>__match_comparable_one_of_op</code> 裏面取得用 struct 包好的 operator 的，接下來就是要設計把 <code>op</code> 也傳入 <code>_whether_support_op</code> 裏面檢查。</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span><>></span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">{};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span><Fn(Ts...), std::void_t<</span></span><br><span class="line"><span class="class">        decltype(/*TODO*/) >> :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">{};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> LT& lhs, <span class="keyword">const</span> RT& rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="comment">//TODO</span></span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op<Fn(LT, RT)>::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> lhs</span>==rhs;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>這邊注意，我們的 specialized template 只剩下兩格，第一個是 <code>Fn(Ts...)</code>，第二個是<code>std::void_t<></code>，因此對應的 primary template 的 <code>typename</code> 格數也要剩下兩格。而 <code>std::void_t<></code> 的內容物還沒設計。</p>
<p>接下來設計 <code>struct _op_equal_to</code>：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">op_equal_to</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line">    constexpr auto operator()(const LT& lhs, const RT& rhs) const -> decltype(std::declval<LT&>() == std::declval<RT&>());</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊我使用 overload operator() 來讓 <code>_op_equal_to</code> 變成很像是 function call 的方式來設計，而回傳變態是 <code>decltype(std::declval<LT&>() == std::declval<RT&>())</code>，如果這個東西成立的話，他會變成正確的型態 (通常是 <code>bool</code>)。函式的內容我們不需要定義，剛剛說的，因為我們只會用 <code>decltype</code> 讓 Compiler 檢查 expression 會不會成立而已，我們關心的是那個回傳變態會不會成立，如果成立的話就行了。</p>
<p>接下來就是回到 <code>_whether_support_op</code> 裏面設計 <code>std::void_t<></code><br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span><Fn(Ts...), std::void_t<</span></span><br><span class="line"><span class="class">        decltype( std::declval<Fn>()(std::declval<Ts>()...) ) >> :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">{};</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊我用 <code>std::declval<Fn>()</code> 產生一個 <code>_op_equal_to</code> 的 reference type 並呼叫他的 member function，傳入的參數是一堆 <code>Ts</code> 型態的 reference type <code>std::declval<Ts>()...</code>。</p>
<p>這樣就完成 General 版的 operator supporting 檢查了。</p>
<p>但這邊還有一個問題是，我們真正在比較的地方還沒有 generalize：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op<Fn(LT, RT)>::value)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">return</span> lhs</span>==rhs;  <span class="comment">//here</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這個地方該怎麼辦？該不會 <code>_op_equal_to</code> 裏面除了有一個虛擬的比較後又要再實作一個單獨的 member function 來做實體的比較？這樣不會太冗嘛？</p>
<p>會。</p>
<p>但是我們只需要動一點手腳就可以做出同時可以虛擬的比較又可以實體的比較的 function 了：<br>首先比較的部份：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op<Fn(LT, RT)>::value)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">return</span> <span class="title">op</span><span class="params">(lhs, rhs)</span></span>;  <span class="comment">//here</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>接下來 <code>_op_equal_to</code> 的部份：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">op_equal_to</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line">    constexpr auto operator()(const LT& lhs, const RT& rhs) const -> decltype(std::declval<LT&>() == std::declval<RT&>())</span><br><span class="line">    { <span class="keyword">return</span> lhs==rhs; }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>到這邊就完成了</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><tuple></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><utility></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><type_traits></span></span></span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">op_equal_to</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line">    constexpr auto operator()(const LT& lhs, const RT& rhs) const -> decltype(std::declval<LT&>() == std::declval<RT&>())</span><br><span class="line">    { <span class="keyword">return</span> lhs==rhs; }</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="function"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="title">any</span><span class="params">(ArgTypes&&... args)</span> </span>{ <span class="keyword">return</span> (... || args); }</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span><>></span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">{};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span><Fn(Ts...), std::void_t<</span></span><br><span class="line"><span class="class">        decltype( std::declval<Fn>()(std::declval<Ts>()...) ) >> :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">{};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> LT& lhs, <span class="keyword">const</span> RT& rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op<Fn(LT, RT)>::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="title">op</span><span class="params">(lhs, rhs)</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_one_of_op(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> T& lhs, ArgTypes&&... args)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">sizeof</span>...(args) == <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="literal">false</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> any( (__match_comparable_one_of_op(op, lhs, args)) ...);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span>{</span></span><br><span class="line">    <span class="built_in">std</span>::tuple<ArgTypes...> args;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span>... Ts></span><br><span class="line">    _type_one_of(Ts&&... args): args(<span class="built_in">std</span>::forward<Ts>(args)...) {}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="built_in">std</span>::<span class="keyword">size_t</span>... I></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op_impl(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> T& lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple<Ts...> &tup, <span class="built_in">std</span>::index_sequence<I...>) <span class="keyword">const</span></span><br><span class="line">        { <span class="keyword">return</span> __match_one_of_op(op, lhs, <span class="built_in">std</span>::get<I>(tup)...); }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> Inds = <span class="built_in">std</span>::make_index_sequence<<span class="keyword">sizeof</span>...(Ts)>></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> T& lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple<Ts...> &tup) <span class="keyword">const</span></span><br><span class="line">        { <span class="keyword">return</span> __match_op_impl(op, lhs, tup, Inds{}); }</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line">constexpr auto one_of(ArgTypes&&... args) -> _type_one_of<ArgTypes...></span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> _type_one_of<ArgTypes...>(<span class="built_in">std</span>::forward<ArgTypes>(args)...);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>==(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> _type_one_of<ArgTypes...> &rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> rhs.__match_op(_op_equal_to{}, lhs, rhs.args);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>接下來就可以嘗試定義其他 operators</p>
<ul>
<li><p><code>operator!=</code> ，這東西沒有什麼好定義的，把 <code>operator==</code> 前面加上 <code>!</code> 就好了：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>!=(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> _type_one_of<ArgTypes...> &rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> !rhs.__match_op(_op_equal_to{}, lhs, rhs.args);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p><code>operator<</code>：</p>
<figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">op_less_than</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line">    constexpr auto operator()(const LT& lhs, const RT& rhs) const -> decltype(std::declval<LT&>() < std::declval<RT&>())</span><br><span class="line">    { <span class="keyword">return</span> lhs < rhs; }</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span><(<span class="keyword">const</span> T& lhs, <span class="keyword">const</span> _type_one_of<ArgTypes...> &rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> rhs.__match_op(_op_less_than{}, lhs, rhs.args);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>其他自己試</p>
</li>
</ul>
<p><a href="https://godbolt.org/z/wjmLZI">完整的測試 Code</a></p>
<h3 id="其他討論" class="heading-control"><a href="#其他討論" class="headerlink" title="其他討論"></a>其他討論<a class="heading-anchor" href="#其他討論" aria-hidden="true"></a></h3><h4 id="op-equal-to-的其他寫法" class="heading-control"><a href="#op-equal-to-的其他寫法" class="headerlink" title="_op_equal_to 的其他寫法"></a><code>_op_equal_to</code> 的其他寫法<a class="heading-anchor" href="#op-equal-to-的其他寫法" aria-hidden="true"></a></h4><p>其實 <code>_op_equal_to</code> 這個 struct 還有其他寫法，例如也可以把 decltype 寫到 template 裏面判斷：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">op_equal_to</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    <span class="keyword">template</span><<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT, <span class="keyword">typename</span> = <span class="keyword">decltype</span>(<span class="built_in">std</span>::declval<LT&>() == <span class="built_in">std</span>::declval<RT&>())></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>()(<span class="keyword">const</span> LT& lhs, <span class="keyword">const</span> RT& rhs) <span class="keyword">const</span></span><br><span class="line">    { <span class="keyword">return</span> lhs==rhs; }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>設在 template 的第三個 parameter，然後把 <code>auto</code> 換成 <code>bool</code>。但是我覺得這樣沒有比較好的原因是，<code>lhs==rhs</code> 並沒有保證回傳值一定是 <code>bool</code>。雖然在 comparison 裏面回傳非 <code>bool</code> 值本身就很奇怪。</p>
<h4 id="冗字" class="heading-control"><a href="#冗字" class="headerlink" title="冗字"></a>冗字<a class="heading-anchor" href="#冗字" aria-hidden="true"></a></h4><p>後來發現其實有些地方的 <code>std::forward</code> 可以拿掉。</p>
<p>第一個就是 <code>one_of</code> 裏面呼叫 <code>_type_one_of</code> 的 constructor<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... ArgTypes></span><br><span class="line">constexpr auto one_of(ArgTypes&&... args) -> _type_one_of<ArgTypes...></span><br><span class="line">{</span><br><span class="line">    <span class="keyword">return</span> _type_one_of<ArgTypes...>(args...);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><br>因為 parameter pack 傳到 parameter pack 直接用 <code>...</code> unpack 就行了。<p></p>
<p>第二個是 <code>_type_one_of</code> 的 constructor 裏面呼叫 tuple 的 constructor<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>... Ts></span><br><span class="line">    _type_one_of(Ts&&... args): args(args...) {}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>實際上應該還有其他地方可以簡化，只是目前還沒有更多想法。</p>
<h4 id="function-type-的-SFINAE" class="heading-control"><a href="#function-type-的-SFINAE" class="headerlink" title="function type 的 SFINAE"></a>function type 的 SFINAE<a class="heading-anchor" href="#function-type-的-SFINAE" aria-hidden="true"></a></h4><p>有些人可能會以為要用 <code>std::void_t</code> 才能玩 SFINAE，其實 SFINAE 也不是什麼新概念了，而是因為有了這個概念，才會在 c++ 17 裏面新增 <code>std::void_t</code> 這個東西。在這之前其實也是可以用類似的方法實現 SFINAE，其中一種方式就是用 function 的方式。</p>
<p>這邊示範怎麼用 function type 來寫 SFINAE，首先這是原本 struct type 的 SFINAE<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span><>></span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">{};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span><Fn(Ts...), std::void_t<</span></span><br><span class="line"><span class="class">        decltype( std::declval<Fn>()(std::declval<Ts>()...) ) >> :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">{};</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊來定義 <code>__whether_support_op</code> function definition (不需要 function 實體)：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> = <span class="keyword">decltype</span>( <span class="built_in">std</span>::declval<Fn>()(<span class="built_in">std</span>::declval<Ts>()...) )></span><br><span class="line"><span class="built_in">std</span>::true_type __whether_support_op(<span class="keyword">const</span> Fn&, <span class="keyword">const</span> Ts&...);</span><br><span class="line">    </span><br><span class="line"><span class="built_in">std</span>::false_type __whether_support_op(...);</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>感覺比 struct type 的 SFINAE 更簡單易懂。</p>
<p>可以看的出來，同樣道理，如果第一個 template function 的第三個 template parameter 成立，我們可以得到 return type 為 <code>std::true_type</code> 的 function，如果不成立，則配對到 return type 為 <code>std::false_type</code> 的 function，然後傳進去的參數就像是垃圾一樣隨便包成一包 parameter pack <code>...</code>。</p>
<p>接著把呼叫的地方改掉：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> LT& lhs, <span class="keyword">const</span> RT& rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">decltype</span>(__whether_support_op(<span class="built_in">std</span>::declval<Fn>(), <span class="built_in">std</span>::declval<LT>(), <span class="built_in">std</span>::declval<RT>()))::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="title">op</span><span class="params">(lhs, rhs)</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>首先 <code>__whether_support_op</code> 是一個 function，我們可以藉由傳入參數的 reference type <code>std::declval<></code> 來讓 Compiler 驗證 function。然後用 <code>decltype()</code> 取得 function 的 return type。最後再取出 <code>value</code> 值看看是 <code>true</code> 還是 <code>false</code>。記住！使用 <code>decltype()</code> 呼叫 function，Compiler 不會執行 function 實體，因此我們只需要有 function definition 就好了。</p>
<p>但是這邊我們就使用了一個很醜的方式呼叫我們的 function。實際上我們也可以用漂亮一點的方式，再包一層 SFINAE 的 struct helper：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>> </span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span><Fn(Ts...)> :</span> <span class="keyword">decltype</span>(__whether_support_op(<span class="built_in">std</span>::declval<Fn>(), <span class="built_in">std</span>::declval<Ts>()...))</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>然後呼叫的部份改成原本的：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> LT& lhs, <span class="keyword">const</span> RT& rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op<Fn(LT, RT)>::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="title">op</span><span class="params">(lhs, rhs)</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>完整的 function type SFINAE：<br></p><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> = <span class="keyword">decltype</span>( <span class="built_in">std</span>::declval<Fn>()(<span class="built_in">std</span>::declval<Ts>()...) )></span><br><span class="line"><span class="built_in">std</span>::true_type __whether_support_op(<span class="keyword">const</span> Fn&, <span class="keyword">const</span> Ts&...);</span><br><span class="line">    </span><br><span class="line"><span class="built_in">std</span>::false_type __whether_support_op(...);</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>> </span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span><Fn(Ts...)> :</span> <span class="keyword">decltype</span>(__whether_support_op(<span class="built_in">std</span>::declval<Fn>(), <span class="built_in">std</span>::declval<Ts>()...))</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> LT& lhs, <span class="keyword">const</span> RT& rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op<Fn(LT, RT)>::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="title">op</span><span class="params">(lhs, rhs)</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><br>跟單純只用 struct type 的 SFINAE 比起來相對就比較冗一點：<br><figure class="highlight c"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span><>></span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">{};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span><Fn(Ts...), std::void_t<decltype( std::declval<Fn>()(std::declval<Ts>()...) ) >> :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">{};</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span><<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn& op, <span class="keyword">const</span> LT& lhs, <span class="keyword">const</span> RT& rhs)</span><br><span class="line">{</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op<Fn(LT, RT)>::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="title">op</span><span class="params">(lhs, rhs)</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><br>所以我才會使用 struct type 的 SFINAE。<p></p>
</body></html>]]></content>
      <categories>
        <category>C++</category>
        <category>Meta-programming</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>C++</tag>
        <tag>C++17</tag>
        <tag>Programming</tag>
        <tag>Meta-programming</tag>
      </tags>
  </entry>
  <entry>
    <title>[教學] 夢魘のCUDA: 使用 Preconditioned Conjugate Gradient 輕鬆解決大型稀疏線性方程組</title>
    <url>/Ending2015a/52045/</url>
    <content><![CDATA[<html><head></head><body><p><img src="https://i.imgur.com/Duz436p.png" width="60%"><br><em>圖片來源：<a href="http://fourier.eng.hmc.edu/e176/lectures/NM/node29.html">http://fourier.eng.hmc.edu/e176/lectures/NM/node29.html</a></em></p>
<div class="note info">
            <p>閱讀難度 ✦✦✧✧✧</p>
          </div>
<p>特別感謝 <em>冠大大</em>、<em>王大大</em>。</p>
<p>《夢魘のCUDA》是 CUDA Programming 系列，<strong>此系列不會介紹任何 CUDA 的基礎知識</strong>，而會介紹一些 CUDA 相關的應用。本篇作為《夢魘のCUDA》系列的首篇文，將會介紹既實用又不實用的兩套 CUDA 內建 Library — cuBLAS / cuSPARSE；除此之外，本篇也會講解如何使用這兩套 Library 實作出經典應用 — Preconditioned Conjugate Gradient。之所以說是經典應用的原因是因為，cuSPARSE 幾乎是為了這個應用而誕生的。</p>
<a id="more"></a>
<h2 id="1-Introduction" class="heading-control"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction<a class="heading-anchor" href="#1-Introduction" aria-hidden="true"></a></h2><ul>
<li><a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">Conjugate gradient method - wiki</a></li>
</ul>
<p>Conjugate gradient method 是一種數值分析 (Numerical analysis) 方法，用來解決大型稀疏線性方程組 (Large-scale sparse linear systems)。其中，線性方程組的矩陣必須是對稱正定矩陣 (Symmetric positive definite matrix)。</p>
<h3 id="1-1-Inner-Product-Space" class="heading-control"><a href="#1-1-Inner-Product-Space" class="headerlink" title="1-1. Inner Product Space"></a>1-1. Inner Product Space<a class="heading-anchor" href="#1-1-Inner-Product-Space" aria-hidden="true"></a></h3><ul>
<li><a href="https://en.wikipedia.org/wiki/Inner_product_space">Inner product space - wiki</a></li>
</ul>
<p>首先必須知道內積 (inner product) 的定義，對一個 map function $\langle\cdot,\cdot\rangle: V\times V \to F$，其中 vector space $V$，  field $F$，若滿足以下三個條件則稱這個 map function 是 inner product，其中 $x,y,z\in V$：</p>
<ul>
<li>Conjugate symmetry：<script type="math/tex; mode=display">
\langle x, y \rangle=\overline{\langle y, x \rangle}</script><div class="note info">
            <p>這通常是定義給複數的 Vector space，對於實數的 Vector space 只需滿足 $\langle x, y \rangle=\langle y, x \rangle$</p>
          </div></li>
<li>Linearity:<script type="math/tex; mode=display">
\begin{gathered}
\langle ax, y \rangle=a\langle x,y\rangle\\
\langle x+y,z\rangle = \langle x,z\rangle + \langle y,z \rangle
\end{gathered}</script><div class="note info">
            <p>這應該不需多做解釋</p>
          </div></li>
<li>Positive-definiteness:<script type="math/tex; mode=display">
\langle x,x\rangle > 0, \quad x\in V \setminus\{\mathbf{0}\}</script><div class="note info">
            <p>注意 $\mathbf{0}$ 代表 Vector space $V$ 的零向量。<br>此外，這個性質也 imply 若 $\langle x,x\rangle=0$ 則 $x=\mathbf{0}$。</p>
          </div>
</li>
</ul>
<h3 id="1-2-Symmetric-Positive-Definite-Matrix" class="heading-control"><a href="#1-2-Symmetric-Positive-Definite-Matrix" class="headerlink" title="1-2. Symmetric Positive Definite Matrix"></a>1-2. Symmetric Positive Definite Matrix<a class="heading-anchor" href="#1-2-Symmetric-Positive-Definite-Matrix" aria-hidden="true"></a></h3><p>若一個 $N\times N$ 的 Real matrix $A$ 滿足以下條件，則稱 $A$ 為 Symmetric positive definite matrix：</p>
<ul>
<li>Symmetric：$A=A^T$</li>
<li>Positive definite：對於任意非 $0$ 向量 $x\in\mathbb{R}^N$，滿足 $x^TAx>0$</li>
</ul>
<p>Symmetric matrix (複數則是 Hermitian matrix) 具有以下特性：</p>
<ul>
<li>Eigenvalues 一定為實數</li>
<li>一定存在一組 Orthonormal eigenvectors</li>
</ul>
<div class="note info">
            <p>Prove Eigenvalues 一定為實數：<br>假設 Symmetric (Hermitian) matrix $A$，假設 $\lambda$ 為 $A$ 的 Eigenvalue，以及對應的 Eigenvector $x$，則 $Ax=\lambda x$。取 Conjugate transpose $\bar{x}^TA^\dagger=\bar{x}^T\bar{\lambda}$。由於 $A$ 為 Symmetric (Hermitian) matrix，$A^\dagger=A$，因此 $\bar{x}^TA=\bar{x}^T\bar{\lambda}$。兩邊同乘 $x$ 得到：</p><script type="math/tex; mode=display">\bar{x}^TAx=\bar{x}^T\bar{\lambda}x</script><p>對 $Ax=\lambda x$ 兩邊同乘 $\bar{x}$：</p><script type="math/tex; mode=display">\bar{x}^TAx=\bar{x}^T\lambda x</script><p>得到 $\bar{x}^T\bar{\lambda}x=\bar{x}^T\lambda x$，若 $x\ne \mathbf{0}$，則 $\bar{\lambda}=\lambda$，因此 Eigenvalue 一定為 Real number。</p>
          </div>
<div class="note info">
            <p>Prove 一定存在一組 Orthonormal eigenvectors：<br>假設兩個 Symmetric (Hermitian) matrix $A$ 的 Eigenvalue $\lambda_a$ 與 $\lambda_b$，且 $\lambda_a\ne \lambda_b$，以及其分別對應的 Eigenvectors $x_a$, $x_b$，</p><script type="math/tex; mode=display">\begin{align*}\lambda_a \langle x_a, x_b \rangle     &= \langle \lambda_ax_a, x_b \rangle\\    &= \langle Ax_a, x_b \rangle\\    &= \langle x_a, Ax_b \rangle\\    &= \langle x_a, \lambda_bx_b \rangle\\    &= \lambda_b \langle x_a, x_b \rangle \end{align*}</script><p>得到 $\lambda_a \langle x_a, x_b \rangle=\lambda_b \langle x_a, x_b \rangle $，移項整理後得到：</p><script type="math/tex; mode=display">(\lambda_a-\lambda_b)\langle x_a, x_b \rangle=0</script><p>由於 $\lambda_a \ne \lambda_b \Leftrightarrow \lambda_a-\lambda_b\ne 0$，因此得到 $\langle x_a, x_b\rangle=0$，表示 $x_a$ 與 $x_b$ 為 Orthogonal。</p>
          </div>
<p>Symmetric positive definite matrix 有更棒的特性：Eigenvalues 一定都為正數</p>
<div class="note info">
            <p>Prove Symmetric (Hermitian) matrix $A$ 的 Eigenvalue 都是正數 $\Leftrightarrow$ Matrix $A$ 為 Symmetric positive definite matrix：<br>$(\Rightarrow)$：假設 $\Lambda$ 為由 $A$ 所有 Eigenvalues 組成的 Diagonal matrix $\Lambda_{ii}=\lambda_i$，且 $\lambda_i \ne \lambda_j, i\ne j$，$Q$ 為各個 Column 由各個 Eigenvalue $\lambda_i$ 所對應的 Eigenvector $x_i$ 組成的 Matrix $\left[\begin{matrix} x_1, \dots, x_N \end{matrix}\right]$。則根據 $Ax=\lambda x$，得到：</p><script type="math/tex; mode=display">\begin{align*}AQ &= Q\Lambda\\A &= Q\Lambda Q^{-1}\end{align*}</script><p>由於 $A$ 為 Symmetric matrix，因此由 Eigenvectors 組成的 $Q$ 為 Orthonomal matrix，Orthonomal matrix 有特性 $Q^{-1}=Q^T$，因此得到：</p><script type="math/tex; mode=display">A = Q\Lambda Q^T</script><p>接著假設任意 Vector $x$ 屬於 Vector space，且不為 $\mathbf{0}$，則：</p><script type="math/tex; mode=display">x^TAx=\underbrace{x^TQ}_{y^T}\Lambda \underbrace{Q^Tx}_{y}=y^T\Lambda y=\sum_{i=0}^N \lambda_i {y_i}^2 >0</script><p>由於 $A$ 為 Symmetric matrix，$\lambda_i>0$ 且 $y_i^2>0$ 因此得到 $x^TAx>0$，得證。</p><p>$(\Leftarrow)$ 假設 Symmetric positive definite matrix $A$ 的任意 Eigenvalue $\lambda$，以及對應的 Eigenvector $x\ne \mathbf{0}$，有 $Ax=\lambda x$。兩邊同乘 $x^T$：</p><script type="math/tex; mode=display">\begin{align*}x^TAx &= \lambda x^Tx\\\lambda &= \frac{x^TAx}{x^Tx} > 0\end{align*}</script><p>由於 $A$ 為 Symmetric positive definite，因此 $x^TAx>0$。且 $x\ne\mathbf{0}$，因此 ${x^Tx} > 0$。得證。</p>
          </div>
<p>基於這些特性，Symmetric positive definite matrix $A$ 可以定義內積。</p>
<p>利用 Symmetric positive definite matrix $A$ 定義一個基於 Vector space $\mathbb{R}^N$ 的內積空間 (Inner product space)，內積定義為：</p>
<script type="math/tex; mode=display">
\langle x,y\rangle_A=x^TAy</script><p>由於 Matrix $A$ 為 Symmetric，我們可以知道這個內積滿足 Inner product 的第一個條件 (Conjugate symmetry)，也就是 $x^TAy=y^TAx$。由於 $x^TAy$ 是線性的，滿足 Inner product 的第二個條件 (Linearity)，由於 $A$ 為 Positive definite，滿足 Inner product 的第三個條件 (Positive-definiteness)。因此，$ \langle \cdot,\cdot\rangle_A $ 是一個合法的內積。</p>
<p>產生 Symmetric positive definite matrix 非常簡單，設一個 $m\times n, m < n$ Matrix $A$，為 Full row rank，$AA^T$ 就會是 Symmetric positive definite matrix。</p>
<div class="note info">
            <p>Prove $AA^T$ 是 Symmetric positive definite matrix：</p><p>假設 Matrix $M=AA^T$，$AA^T$ 為 Symmetric matrix。假設任意 Vector $x\in \mathbb{R}^m$ 且 $x\ne\mathbf{0}$，則：</p><script type="math/tex; mode=display">x^TMx=x^TAA^Tx=\|A^Tx\|>0</script><p>故得證。</p>
          </div>
<h3 id="1-3-Steepest-Descent" class="heading-control"><a href="#1-3-Steepest-Descent" class="headerlink" title="1-3. Steepest Descent"></a>1-3. Steepest Descent<a class="heading-anchor" href="#1-3-Steepest-Descent" aria-hidden="true"></a></h3><ul>
<li><a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a></li>
</ul>
<p>設 Linear system</p>
<script type="math/tex; mode=display">
Ax=b \tag{1.3.1}</script><p>其中 $A\in\mathbb{R}^{N\times N}$ 為 Symmetric positive definite matrix， $b\in \mathbb{R}^N$ 為已知的 Vector。若要求解 vector $x$，根據傳統的解法 (高中、大學教的方法)是，首先先移項 $A$，使用高斯法求出 $A$ 的 Inverse matrix $A^{-1}$ 後，求出 $x$：</p>
<script type="math/tex; mode=display">
x=A^{-1}b</script><p>然而，對於 Large-scale sparse matrix 而言 ($N$ 非常巨大)，求 Inverse matrix 是非常困難的，因此另一種解法就是使用 Numerical method，將問題變成能夠使用 Iterative 趨近的方式來求出近似解。首先將 [式 1.3.1] 移項 $Ax-b=\mathbf{0}$，接著假設 Function $f(x)$，並令其一階微分：</p>
<script type="math/tex; mode=display">
f'(x)=Ax-b \tag{1.3.2}</script><p>根據一階微分，我們能夠得出 Quadratic form：</p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{2}x^TAx-b^Tx+c \tag{1.3.3}</script><p>則目標變成：找到 $x$ 使得 $f’(x)=\mathbf{0}$。也就是說，找到 $x$，使得 $f(x)$ 為極值，且若 $A$ 為 Symmetric positive definite matrix，則該極值一定會是最小值。</p>
<p>證明方法非常簡單，假設 $x^* $ 滿足 $Ax^* = b$，我們只需證明添加上任意 Error $e\in \mathbb{R}^{N}\setminus {\mathbf{0}}$，一定滿足 $f(x^* + e) > f(x^*)$：</p>
<script type="math/tex; mode=display">
\begin{align*}
f(x^* + e) 
    &= \frac{1}{2}(x^* + e)^TA(x^* + e)-b^T(x^* + e)+c\\
    &= \frac{1}{2}(x^*)^T A x^* + e^T\underbrace{Ax^*}_{=b}+\frac{1}{2}e^TAe-b^Tx^*-b^Te+c\\
    &= \underbrace{\left(\frac{1}{2}(x^*)^TAx^*-b^Tx^* + c\right)}_{=f(x^*)}+e^Tb+\frac{1}{2}e^TAe-b^Te\\
    &= f(x^*)+\frac{1}{2}e^TAe > f(x^*)
\end{align*}</script><p>由於 $A$ 是 Positive definite，且 $e\ne \mathbf{0}$，因此最後一項 $\frac{1}{2}e^TAe > 0$，則可以得證當 $Ax^* =b$ 時，$f(x^*)$ 一定為最小值。因此我們將一個 Linear system 的問題變成了 Minimization problem：找到 $x$ 使得 minimize $f(x)$。</p>
<p>接著是要如何解這個 Minimization problem，一種方法就是使用 <strong>Steepest descent method</strong>。</p>
<div class="note info">
            <p>這邊列出一些常用的 term：</p><ul><li>Error: $e_i=x_i-x^*$，寫成 Iterative form，$e_{i+1}=e_i+\alpha_i r_i$</li><li>Residual: $r_i=b-Ax_i=-f’(x_i)$</li></ul><p>Residual 與 Error 的關係式：</p><script type="math/tex; mode=display">r_i=b-Ax_i=Ax^*-Ax_i=A(x^*-x_i)=-Ae_i</script>
          </div>
<p>首先從任意一點 $x_0$ 開始，選擇負梯度方向 $-f’(x_0)=r_0$ 前進適當步長 $\alpha_0$，這個步長必須能夠在負梯度方向上最小化下一個 Step 的值 $f(x_1)$，也就是 $\alpha_0=\arg\min f(x_1)=\arg\min f(x_{0}+\alpha_0 r_0)$，通常會使用 Line search 的方式求得。求得步長後，移動到下一個位置 $x_1=x_0 + \alpha_0 r_0$，求出下一個步長 $\alpha_1$，再移動到下一個位置 $x_2=x_1+\alpha_1 r_1$ …，重複動作直到找到最佳解為止。<br><img src="https://i.imgur.com/OK3pGcQ.png" alt></p>
<p>至於求解步長 $\alpha_i$ 可以使用求極值 (導數為 $0$) 的方式求得：</p>
<script type="math/tex; mode=display">
\frac{d}{d\alpha}f(x_{i+1})=f'(x_{i+1})^T\frac{d}{d\alpha}x_{i+1}=f'(x_{i+1})^Tr_i=0</script><p>因為 $x_{i+1}=x_i+\alpha_i r_i$。由於 $f’(x_{i+1})=-r_{i+1}$，求得 $\alpha_i$：</p>
<script type="math/tex; mode=display">
\begin{align*}
r^T_{i+1}r_i&=0\\
(b-Ax_{i+1})^Tr_i&=0\\
(b-A(x_i+\alpha_i r_i))^Tr_i&=0\\
(b-Ax_i)^Tr_i-\alpha_i(Ar_i)^Tr_i &= 0\\
(b-Ax_i)^Tr_i &= \alpha_i(Ar_i)^Tr_i\\
r^T_ir_i &= \alpha_i r^T_i(Ar_i)\\
\alpha_i&=\frac{r^T_ir_i}{r^T_iAr_i}
\end{align*}</script><p>最後就可以得到 Steepest descent method：</p>
<script type="math/tex; mode=display">
\begin{gathered}
r_i=b-Ax_i\\
\alpha_i=\frac{r^T_ir_i}{r^T_iAr_i}\\
x_{i+1}=x_i+\alpha_ir_i
\end{gathered}</script><div class="note info">
            <p>Gradient descent：往 Negative gradient 方向前進任意步長 $\alpha$，此步長可以使用固定值，也可以使用估計的方式，例如：SGD 等等。</p><p>Steepest descent：往 Negative gradient 方向前進適當步長 $\alpha$，此步長必須使得下一個值在這個方向上是最小值。</p>
          </div>
<h2 id="2-Methodology" class="heading-control"><a href="#2-Methodology" class="headerlink" title="2. Methodology"></a>2. Methodology<a class="heading-anchor" href="#2-Methodology" aria-hidden="true"></a></h2><ul>
<li><a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a></li>
</ul>
<h3 id="2-1-Conjugate-Gradient" class="heading-control"><a href="#2-1-Conjugate-Gradient" class="headerlink" title="2-1. Conjugate Gradient"></a>2-1. Conjugate Gradient<a class="heading-anchor" href="#2-1-Conjugate-Gradient" aria-hidden="true"></a></h3><p>Steepest descent 有個缺點在於，經常發生重複搜索同一方向 (如 Figure 8) 的情況，導致收斂速度變慢，為了解決這項問題，有人提出了 Conjugate gradient。首先假設一個 Vector set $\{p_0,p_1,\dots,p_{N-1}\}$ 代表 Search directions，且這些 Vector 兩兩互相垂直 (Orthoginal)。因此可以將每個 Step 的 $x$ 列為：</p>
<script type="math/tex; mode=display">
x_{i+1}=x_i+\alpha_i p_{i} \tag{2.1.1}</script><p>且</p>
<script type="math/tex; mode=display">
x^*=x_0+\sum_{i=0}^{N-1}\alpha_i p_{i} \tag{2.1.2}</script><p>這樣就能夠確保每個搜索方向只需要搜尋一次就能夠找到最佳。由於每個 Step $i$ 的 Search direction $p_i$ 一定會與 Error $e_{i+1}$ 垂直，因此：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^T_ie_{i+1} &= 0\\
p^T(e_i+\alpha_ip_i) &= 0\\
\end{align*}</script><div class="note info">
            <p>Prove 每個 Step $i$ 的 Search direction $p_i$ 一定會與 Error $e_{i+1}$ 垂直：</p><p>因為 $e_{i+1}=x_{i+1}-x^*$ 且 $x^*=x_{i+1}+\sum_{j=i+1}^{N-1}\alpha_jp_j$，因此 $e_{i+1}=-\sum_{j=i+1}^{N-1}\alpha_jp_j$，也就是說，$e_{i+1}$ 是 $\{p_{i+1}, \dots, p_{N-1}\}$ 的 Linear combination，而 $p_{i}$ 與所有$\{p_{i+1}, \dots, p_{N-1}\}$ 互為 Orthogonal，因此可以得證 $p_{i}$ 與 $e_i$ 一定是 Orthogonal。</p>
          </div>
<p>如上根據 Error 的 Iterative form 展開後，可以算出：</p>
<script type="math/tex; mode=display">
\alpha_i=-\frac{p^T_ie_i}{p^T_ip_i}</script><p>然而，尷尬的是我們沒有辦法求出 $e_i$，因為 $e_i=x_i-x^*$，如果可以求得 $e_i$ 那麼 $x^*$ 就已經算出來了。避免掉這個窘境的方式是改變假設 Search directions 兩兩互為 $A$-orthogonal ($A$-conjugate)，而非 Orthogonal：</p>
<script type="math/tex; mode=display">
p^T_iAp_j=0, \quad i\ne j</script><p>這樣假設的原因是因為，如果 $e_i$ 算不出來，只需要對 $e_i$ 乘上 $A$，就會得到 $Ae_i=Ax_i-Ax^*=Ax_i-b$，如此就可以避免掉 $x^*$ 的問題。又因 $Ae_i=-r_i$，因此同 Steepest descent 求 $\alpha$ 可以得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{d}{d\alpha}f(x_{i+1}) &= 0\\
f'(x_{i+1})^T\frac{d}{d\alpha}x_{i+1} &= 0\\
-r^T_{i+1}p_i &= 0\\
(Ae_{i+1})^Tp_i=d_i^TAe_{i+1}&=0
\end{align*}</script><p>因 $e_{i+1}=e_i+\alpha_ip_i$，因此：</p>
<script type="math/tex; mode=display">
d^T_iA(e_i+\alpha_ip_i) = 0</script><p>移項整理後求得 $\alpha_i$：</p>
<script type="math/tex; mode=display">
\begin{align*}
\alpha_i &= -\frac{p^T_iAe_i}{p^T_iAp_i}\\
&=\frac{p^T_ir_i}{p^T_iAp_i} \tag{2.1.3}
\end{align*}</script><p>(能夠想到這種方法的人肯定腦力發揮 100%)</p>
<p>接下來的問題在於，要如何產生 Mutually $A$-orthogonal vectors $\{p_0,\dots,p_{N-1}\}$，可以使用 Gram-Schmidt process，來 Iteratively 產生互相 Orthogonal 的向量。</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process">Gram–Schmidt process - wiki</a></li>
</ul>
<p>假設一組 Linearly independent set $\{u_0, \dots, u_{N-1}\}$，Gram-Schmidt process 利用 Projection 的方式來製造出兩兩互相 Orthogonal 的 Vectors，原理非常簡單：</p>
<p><img src="https://i.imgur.com/bjvovNI.png" width="300px"></p>
<p>首先兩個 Linearly independent vectors $u_0, u_1$，先將 $u_1$ Project 到 $u_0$ 上算出投影向量 $\text{proj}_{u_0}(u_1)$，接著再將 $u_1$ 扣掉 $\text{proj}_{u_0}(u_1)$ 得到的就會是與 $u_0$ Orthogonal 的 Vector $u_1-\text{proj}_{u_0}(u_1)$。此時我們只需要令 $p_0=u_0$、$p_1=u_1-\text{proj}_{d_0}(u_1)$ 就可以得到兩個 Mutually orthogonal vectors $p_0, p_1$ 以同樣方式推廣至 $i$：</p>
<script type="math/tex; mode=display">
\begin{align*}
&p_0=u_0\\
&p_1=u_1-\text{proj}_{p_0}(u_1)\\
&p_2=u_2-\text{proj}_{p_0}(u_2)-\text{proj}_{p_1}(u_2)\\
&p_i=u_i-\sum_{j=0}^{i-1}\text{proj}_{p_j}(u_i) \tag{2.1.4}
\end{align*}</script><p>以此類推就可以得到一組 Mutually orthogonal vectors $\{p_0,\dots,p_{N-1}\}$，然而我們需要的是 $A$-orthogonal 的 vectors，因此可以定義 Projection 為 $A$ 內積空間的 Projection：</p>
<script type="math/tex; mode=display">
\text{proj}_{p_j}(u_i)=\frac{\langle u_i, p_j\rangle_A}{\langle p_j, p_j\rangle_A}p_j = \frac{u_i^TAp_j}{p_jAp_j}p_j \tag{2.1.5}</script><p>至於要如何找到 Linearly independent set $\{u_0, u_1, \dots, u_{N-1}\}$ 一個很直接的方式就是使用現成的 Residual $r$。</p>
<p>我們知道 Residual 與 Error 的關係式 $r_j=-Ae_j$，而 $e_j=x_j-x^*$，$x_j=x_0+\sum_{k=0}^{j-1}\alpha_kp_k$ 且 $x^*=x_0+\sum_{k=0}^{N-1}\alpha_kp_k$，全部展開來後得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
r_j &= -A\left( (x_0+\sum_{k=0}^{j-1}\alpha_kp_k)-(x_0+\sum_{k=0}^{N-1}\alpha_kp_k) \right)\\
    &= -A(-\sum_{k=j}^{N-1}\alpha_kp_k)\\
    &= \sum_{k=j}^{N-1}\alpha_kAp_k
\end{align*}</script><p>由此可以看出，Residual $r_i$ 是由 $\{Ap_{i+1}, \dots, Ap_{N-1}\}$ 組成的 Linear combination，而 $\{Ap_{i+1}, \dots, Ap_{N-1}\}$ 是 Linearly independent set，因此可以知道 $\{r_0, \dots, r_{N-1}\}$ 也一定會是 Linearly independent set。</p>
<p>此外，這邊如果對兩邊同時乘上 $p_i$ 就會發生一件很有趣的事情，其中 $i<j$ ，由於 $p_i$ 與其他 $p_j, \dots, p_{N-1}$ 為 $A$-orthogonal，因此得到：</p>
<script type="math/tex; mode=display">
p_i^Tr_j=\sum_{k=j}^{N-1}\alpha_kp_i^TAp_k=0, \quad i<j \tag{2.1.6}</script><p>使用 Residual $r$ 代替 $u$ 後，令 $\beta$ 為 Projection 的 Coefficient 項：</p>
<script type="math/tex; mode=display">
\beta_{ij}=-\frac{r^T_iAp_j}{p^T_jAp_j}</script><p>因此 [式 2.1.4] 變成：</p>
<script type="math/tex; mode=display">
p_i=r_i+\sum_{j=0}^{i-1}\beta_{ij}p_j \tag{2.1.7}</script><p>將 [式 2.1.7] 兩邊同乘 $r_j$，得到：</p>
<script type="math/tex; mode=display">
p^T_ir_j=r_i^Tr_j+\sum_{k=0}^{i-1}\beta_{ik}p_k^Tr_j</script><p>由 [式 2.1.6] 可以知道當 $i<j$ 時 $p^T_ir_j=0$ 且 $p_k^Tr_j=0$ 因為 $k\le i-1<i<j$，因此：</p>
<script type="math/tex; mode=display">
r_i^Tr_j=0, \quad i\ne j \tag{2.1.8}</script><p>可以得出一個結論：Residual $r$ 兩兩互為 Orthogonal。</p>
<p>除此之外，若 $i=j$ 時，$p^T_ir_i \ne 0$ 但 $p_k^Tr_i=0$ 因為 $k\le i-1<i$，因此得到：</p>
<script type="math/tex; mode=display">
p^T_ir_i=r_i^Tr_i</script><p>代入到 [式 2.1.3] 的 $\alpha_i$，得到新的 $\alpha_i$：</p>
<script type="math/tex; mode=display">
\alpha_i = \frac{p^T_ir_i}{p^T_iAp_i} = \frac{r^T_ir_i}{p^T_iAp_i} \tag{2.1.9}</script><p>已知 $r_{j+1}=r_j-\alpha_jAp_j$，對等號兩邊乘上 $r_i$，整理後同除 $\alpha_j$：</p>
<script type="math/tex; mode=display">
\begin{align*}
r^T_i r_{j+1} &= r^T_ir_j-\alpha_jr^T_iAp_j\\
\alpha_jr^T_iAp_j &= r^T_ir_j - r^T_i r_{j+1}\\
r^T_iAp_j &= \frac{1}{\alpha_j}(r^T_ir_j - r^T_i r_{j+1})
\end{align*}</script><p>根據 [式 2.1.8]，可以得到，只有兩個狀況下 $r^T_iAp_j$ 會有值：</p>
<script type="math/tex; mode=display">
r^T_iAp_j=\begin{cases}
\frac{1}{\alpha_i}r^T_ir_j, \quad& i=j\\
-\frac{1}{\alpha_{i-1}}r^T_i r_{i}, \quad& i=j+1\\
0, \quad&\text{otherwise}
\end{cases}</script><p>將兩邊同除以 $-p_j^TAp_j$，得到 $\beta_{ij}$，由於在 Gram-Schmidt process 中 $\beta_{ij}$ 只存在 $i<j$ 這個 Case，不存在 $i=j$ 的 Case，因此只剩下一種 Case $i=j+1$，剩下其他 Case 都為 $0$：</p>
<script type="math/tex; mode=display">
\beta_{ij}=-\frac{r^T_iAp_j}{p_j^TAp_j}=\begin{cases}
\frac{1}{\alpha_{i-1}}\frac{r^T_ir_i}{p_{i-1}^TAp_{i-1}}, \quad &i=j+1\\
0, \quad&\text{otherwise}
\end{cases}</script><p>將 [式 2.1.9] 的 $\alpha_{i-1}$ 代入，最後得到新的 $\beta$：</p>
<script type="math/tex; mode=display">
\begin{align*}
\beta_i &= \frac{p^T_{i-1}Ap_{i-1}}{r^T_{i-1}r_{i-1}}\frac{r^T_ir_i}{p_{i-1}^TAp_{i-1}}\\
    &= \frac{r^T_ir_i}{r^T_{i-1}r_{i-1}} \tag{2.1.10}
\end{align*}</script><p>結果 $p_i$ 一整個不見了，全部變成 Residual $r$。完全出乎意料。<br>寫到這邊為止的我：</p>
<p><img src="https://i.imgur.com/29vI4jy.png" width="500px"></p>
<p>最後將全部放在一起得到完整的 Conjugate Gradient：</p>
<script type="math/tex; mode=display">
\begin{gathered}
p_0=r_0=b-Ax_0\\
\alpha_i = \frac{r^T_ir_i}{p^T_iAp_i}\\
x_{i+1}=x_i+\alpha_ip_i\\
r_{i+1}=r_i-\alpha_iAp_i\\
\beta_{i+1}=\frac{r^T_{i+1}r_{i+1}}{r^T_ir_i}\\
p_{i+1}=r_{i+1}+\beta_{i+1}p_i
\end{gathered}</script><p>第一行 $p_0=r_0=b-Ax_0$ 是 Initial condition，之後就是持續 Iterate 直到 Residual $|r_{i+1}|\to 0$ 就結束 Iteration。通常可以另外設定一個 Tolerance 參數 $\epsilon \to 0$，當 $|r_{i+1}| < \epsilon$ 時結束 Conjugate gradient。</p>
<p>接下來就是 Convergence Analysis 與 Complexity 分析的部分，雖然這部分才是真正 Numerical Analysis 要做的事情，但礙於篇幅且不是很重要，在這邊就直接忽略，有興趣的可以自行研究 <a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">Ch. 9 Convergence Analysis / Ch. 10 Complexity</a>。這本書真的寫得很棒，有機會一定要看完！(我在說我)</p>
<h3 id="2-2-Preconditioned-Conjugate-Gradient" class="heading-control"><a href="#2-2-Preconditioned-Conjugate-Gradient" class="headerlink" title="2-2. Preconditioned Conjugate Gradient"></a>2-2. Preconditioned Conjugate Gradient<a class="heading-anchor" href="#2-2-Preconditioned-Conjugate-Gradient" aria-hidden="true"></a></h3><p>如果有看 <a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">Ch. 9 Convergence Analysis / Ch. 10 Complexity</a> 的話應該就會知道，Conjugate gradient 的收歛性取決於 Matrix $A$ 的 Condition number $\kappa(A)$ (Iteration 與 $\sqrt{\kappa(A)}$ 成正比)，$\kappa(A)$ 小則容易收斂，$\kappa(A)$ 大則不容易收斂。而 Condition number 定義為 $\kappa(A)=\frac{\lambda_\text{max}}{\lambda_\text{min}}\ge 1$，其中 $\lambda$ 為 $A$ 的 Eigenvalue。(等我學會了 Numerical Analysis 再來補充)</p>
<p>在某些情況下，我們會希望 Conjugate Gradient 能夠再收斂的更快，例如：對於 $N$ 為 Million 等級的 Matrix $A$，我們不希望 Conjugate Gradient 執行 100 萬次才收斂到解答，而是希望在 1000 次內就可以得到近似解。這種情況下我們就會需要對原本的 Linear system 進行 Preconditioning，使得新的 Linear system 能夠更快收斂。</p>
<p>假設 Linear system 不容易收斂 (也有人稱做 ill-conditioned system) $Ax=b$，我們希望可以找到新的 Linear system $\hat{A}\hat{x}=\hat{b}$，且新的 Linear system 能夠收斂的更快。換句話說，希望可以找到一個 $\hat{A}$，滿足 $\kappa(\hat{A})\ll \kappa(A)$。一種方法就是選擇一個 Non-singular matrix $M$，並套用到 Linear system：</p>
<script type="math/tex; mode=display">
M^{-1}Ax=M^{-1}b</script><p>使得 $\kappa(M^{-1}A)\ll \kappa(A)$，同時 $M^{-1}A$ 必須維持 Symmetric positive definite matrix，$M$ 就稱為 <strong>Preconditioner</strong>。若要維持 Symmetric positive definite matrix，最簡單的方式就是 $M$ 同為 Symmetric positive definite matrix，為了確保 $M$ 為 Symmetric positive definite matrix，我們也可以利用另一個 Non-singular matrix $E$，且令 $M=EE^T$ 來表示 $M$，如此就可以確保 $M$ 一定是 Symmetric positive definite matrix (證明請看 [Sec 1-2])</p>
<div class="note info">
            <p>Non-singular matrix $M$ 表示 $M$ 是 Invertible，且具有 Inverse matrix $M^{-1}$。</p>
          </div>
<p>至於要去哪裡找到如此好的 $M$，聰明如你，最直覺的方式就是選擇 $M=A$，如此一來，$M^{-1}A=A^{-1}A=I$，我們就可以確保 $\kappa(M^{-1}A)=\kappa(I)=1\ll\kappa(A)$，同時 $I$ 也是 Symmetric positive definite matrix。然而，事實卻沒有這麼簡單，原因是因為，如果取 $M=A$ 的話，勢必要先計算出 $A^{-1}$ 才能套用進新的 Linear system $A^{-1}Ax=A^{-1}b$，但問題就在於，就是因為 $A$ 的 Inverse matrix $A^{-1}$ 很難求才會需要使用 Conjugate gradient 的方式來求解，要不然 Linear system $Ax=b$ 早就解完了。因此，只能限定 $M\approx A$，且 $M$ 必須更容易計算 Inverse。</p>
<p>最終歸納出 $M$ 必須具有以下條件，只要 $M$ 具有以下條件，就可以大幅縮減 Condition number：</p>
<ul>
<li>$\kappa(M^{-1}A)\ll\kappa(A)$，有一說是 $M^{-1}A$ 的 Eigenvalues 必須要更 Clustered，因為 $\kappa=\frac{\lambda_\text{max}}{\lambda_\text{min}}$</li>
<li>$M^{-1}A$ 是 Symmetric positive definite matrix</li>
<li>$M\approx A \Longrightarrow M^{-1}A\approx A^{-1}A= I$</li>
<li>$M$ Invertible 且比 $A$ 更容易計算 Inverse matrix</li>
</ul>
<p>接著就是如何求解新的 Linear system，一種方式是直接計算出 $M^{-1}$ 後，將新的 $\hat{A}=M^{-1}A$，$\hat{b}=M^{-1}b$ 直接帶入 Conjugate gradient。另一種方式是將 $M$ 分解 $M=EE^T$ 且 $E$ 可能是比 $M$ 還要更容易計算 Inverse 的 Matrix (例如: Triangular matrix)。如果使用第二種 Case 的話，可以將 Linear system 拆解並轉換成新的 Linear system $\hat{A}\hat{x} = \hat{b}$：</p>
<script type="math/tex; mode=display">
\begin{align*}
M^{-1}Ax &= M^{-1}b\\
E^{-1}AE^{-T}x &= E^{-1}E^{-T}b\\
(E^{-1}AE^{-T})(E^Tx) &= (E^{-T}E^T)(E^{-1}b)\\
\hat{A}\hat{x} &= \hat{b}
\end{align*}</script><p>其中 $\hat{A}=E^{-1}AE^{-T}$，$\hat{x}=E^Tx$，$\hat{b}=E^{-1}b$。帶入 Conjugate gradient 後得到新的 Conjugate gradient (其實就是全部加上 Hat)：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\hat{p}_0=\hat{r}_0=\hat{b}-\hat{A}\hat{x}_0\\
\hat{\alpha}_i = \frac{\hat{r}^T_i\hat{r}_i}{\hat{p}^T_i\hat{A}\hat{p}_i}\\
\hat{x}_{i+1}=\hat{x}_i+\hat{\alpha}_i\hat{p}_i\\
\hat{r}_{i+1}=\hat{r}_i-\hat{\alpha}_i\hat{A}\hat{p}_i\\
\hat{\beta}_{i+1}=\frac{\hat{r}^T_{i+1}\hat{r}_{i+1}}{\hat{r}^T_i\hat{r}_i}\\
\hat{p}_{i+1}=\hat{r}_{i+1}+\hat{\beta}_{i+1}\hat{p}_i
\end{gathered}</script><p>接下來就是將 Hat 都展開來。計算出新的 Residual $\hat{r}_i$ 與舊 Residual $r_i$ 的關係：</p>
<script type="math/tex; mode=display">
\hat{r}_i= \hat{b}-\hat{A}\hat{x}_i=E^{-1}b-(E^{-1}AE^{-T})(E^Tx)=E^{-1}r_i</script><p>再由 [式 2.1.2] 可以得到：</p>
<script type="math/tex; mode=display">
\hat{x}=E^Tx=E^T(x_0+\sum_{i=0}^{N-1}\alpha_ip_i)=E^Tx_0+\sum_{i=0}^{N-1}\alpha_iE^Tp_i</script><p>因此定義 $\hat{p}_i=E^Tp_i$，則 $\hat{\alpha}_i$ 可以展開：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{\alpha}_i &= \frac{\hat{r}^T_i\hat{r}_i}{\hat{p}^T_i\hat{A}\hat{p}_i}\\
    &= \frac{(E^{-1}r_i)^T(E^{-1}r_i)}{(E^Tp_i)^T(E^{-1}AE^{-T})(E^Tp_i)}\\
    &= \frac{r_i^T(E^{-T}E^{-1})r_i}{p_i^T(EE^{-1})A(E^{-T}E^T)p_i}\\
    &= \frac{r_i^TM^{-1}r_i}{p_i^TAp_i}
\end{align*}</script><p>接下來將 $\hat{x}_{i+1} = \hat{x}_i+\hat{\alpha}_i\hat{p}_i$ 展開得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{x}_{i+1} &= \hat{x}_i+\hat{\alpha}_i\hat{p}_i\\
E^Tx_{i+1} &= E^Tx_i + \hat{\alpha}_iE^Tp_i\\
E^Tx_{i+1} &= E^T(x_i + \hat{\alpha}_ip_i)\\
x_{i+1} &= x_i + \hat{\alpha}_ip_i
\end{align*}</script><p>接下來 $\hat{r}_{i+1}=\hat{r}_i-\hat{\alpha}_i\hat{A}\hat{p}_i$ 也展開：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{r}_{i+1} &= \hat{r}_i-\hat{\alpha}_i\hat{A}\hat{p}_i\\
E^{-1}r_{i+1} &= E^{-1}r_i-\hat{\alpha}_i(E^{-1}AE^{-T})(E^Tp_i)\\
E^{-1}r_{i+1} &= E^{-1}r_i-\hat{\alpha}_iE^{-1}A(E^{-T}E^T)p_i\\
E^{-1}r_{i+1} &= E^{-1}(r_i-\hat{\alpha}_iAp_i)\\
r_{i+1} &= r_i-\hat{\alpha}_iAp_i
\end{align*}</script><p>接下來展開 $\hat{\beta}_{i+1}$：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{\beta}_{i+1} &= \frac{\hat{r}^T_{i+1}\hat{r}_{i+1}}{\hat{r}^T_i\hat{r}_i}\\
    &= \frac{(E^{-1}r_{i+1})^T(E^{-1}r_{i+1})}{(E^{-1}r_i)^T(E^{-1}r_i)}\\
    &= \frac{r^T_{i+1}(E^{-T}E^{-1})r_{i+1}}{r^T_{i}(E^{-T}E^{-1})r_{i}}\\
    &= \frac{r^T_{i+1}M^{-1}r_{i+1}}{r^T_{i}M^{-1}r_{i}}
\end{align*}</script><p>接下來展開 $\hat{p}_{i+1}=\hat{r}_{i+1}+\hat{\beta}_{i+1}\hat{p}_i$：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{p}_{i+1} &= \hat{r}_{i+1}+\hat{\beta}_{i+1}\hat{p}_i\\
E^Tp_{i+1} &= E^{-1}r_{i+1}+\hat{\beta}_{i+1}E^Tp_i\\
E^{-T}E^Tp_{i+1} &= (E^{-T}E^{-1})r_{i+1} + \hat{\beta}_{i+1}E^{-T}E^Tp_i\\
p_{i+1} &= M^{-1}r_{i+1} + \hat{\beta}_{i+1}p_i
\end{align*}</script><p>最後一件最重要的事情就是展開 $\hat{p}_0=\hat{r}_0$：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{p}_0 &= \hat{r}_0\\
E^Tp_0 &= E^{-1} r_0\\
p_0 &= E^{-T}E^{-1} r_0\\
p_0 &= M^{-1}r_0
\end{align*}</script><p>全部整理起來得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
r_0&=b-Ax_0\\
p_0 &= M^{-1}r_0\\
\alpha_i &= \frac{r_i^TM^{-1}r_i}{p_i^TAp_i}\\
x_{i+1} &= x_i + \alpha_ip_i\\
r_{i+1} &= r_i-\alpha_iAp_i\\
\beta_{i+1} &= \frac{r^T_{i+1}M^{-1}r_{i+1}}{r^T_{i}M^{-1}r_{i}}\\
p_{i+1} &= M^{-1}r_{i+1} + \hat{\beta}_{i+1}p_i
\end{align*}</script><p>接著令 $z_i=M^{-1}r_i$ 得到 Preconditioned conjugate gradient：</p>
<script type="math/tex; mode=display">
\begin{align*}
r_0 &= b-Ax_0\\
z_0 &= M^{-1}r_0\\
p_0 &= z_0\\
\alpha_i &= \frac{r_i^Tz_i}{p_i^TAp_i}\\
x_{i+1} &= x_i + \alpha_ip_i\\
r_{i+1} &= r_i-\alpha_iAp_i\\
z_{i+1} &= M^{-1}r_{i+1}\\
\beta_{i+1} &= \frac{r^T_{i+1}z_{i+1}}{r^T_{i}z_{i}}\\
p_{i+1} &= z_{i+1} + \hat{\beta}_{i+1}p_i
\end{align*}</script><p>結果發現其實根本不需要 $E$ 的我：<br><img src="https://i.imgur.com/db33QIH.png" width="300px"></p>
<h3 id="2-3-Incomplete-Cholesky-Preconditioned-Conjugate-Gradient-ICCG" class="heading-control"><a href="#2-3-Incomplete-Cholesky-Preconditioned-Conjugate-Gradient-ICCG" class="headerlink" title="2-3. Incomplete-Cholesky Preconditioned Conjugate Gradient (ICCG)"></a>2-3. Incomplete-Cholesky Preconditioned Conjugate Gradient (ICCG)<a class="heading-anchor" href="#2-3-Incomplete-Cholesky-Preconditioned-Conjugate-Gradient-ICCG" aria-hidden="true"></a></h3><ul>
<li><a href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Positive_semidefinite_matrices">Cholesky decomposition - wiki</a></li>
<li><a href="https://en.wikipedia.org/wiki/Incomplete_Cholesky_factorization">Incomplete Cholesky factorization - wiki</a></li>
</ul>
<p>Preconditioner $M$ 可以有很多種，在上一節中我們討論出 $M$ 必須具有以下特性：</p>
<ul>
<li>$\kappa(M^{-1}A)\ll\kappa(A)$</li>
<li>$M^{-1}A$ 是 Symmetric positive definite matrix</li>
<li>$M\approx A \Longrightarrow M^{-1}A\approx A^{-1}A=I$</li>
<li>$M$ Invertible 且比 $A$ 更容易計算 Inverse matrix</li>
</ul>
<p>但根據 Preconditioned conjugate gradient 計算 $z_{i+1}=M^{-1}r_{i+1}$，其實可以發現不一定要計算出 $M^{-1}$。假若 $M$ 可以被分解為 $M=EE^T$，帶入公式後得到</p>
<script type="math/tex; mode=display">
\begin{align*}
z_{i+1}&=(EE^T)^{-1}r_{i+1}\\
z_{i+1}&= E^{-1}\underbrace{(E^{-T} r_{i+1})}_y\\
z_{i+1}&= E^{-1}y
\end{align*}</script><p>令 $y=E^{-T}r_{i+1}$，原本需要計算出 Inverse matrix $M^{-1}$ 變成只須計算 $E^{-1}$ 並分別求解 $y=E^{-T}r_{i+1}$ 與 $z_{i+1} = E^{-1}y$，就能能夠計算出 $z_{i+1}$。因此若有辦法將 $M$ 分解成更容易計算 Inverse 的 Matrix $E$ 那將會一大福音。正好，Cholesky 認為任何 Symmetric (Hermitian) positive definite matrix $M$ 都能夠分解為 Lower triangular matrix $L$ 及其 Transpose $L^T$ (若非 Real matrix 則為 Conjugate transpose)，也就是 $M=LL^T$，這個方法稱作 Cholesky factorization。而剛好 Triangular matrix 非常好求 Inverse。</p>
<div class="note info">
            <p>Prove: 若 $M$ 為 Symmetric positive definite matrix，則存在唯一的 Cholesky factorization $LL^T$，其中 $L$ 是 Lower triangular matrix，且 Diagonal element 皆為正。</p><p>(等我想到再補充)</p>
          </div>
<p>然而，為了滿足 $M\approx A$ 而直接取 $M=A$ 計算 Cholesky factorization 還是會遇到同樣 $A^{-1}$ 的問題，除此之外，對於 Sparse matrix ($n\ll N\times N$，其中 $n$ 為非零項的數量) 而言， Cholesky factorization 是一大問題：經過分解後會爆出一堆非零項，使得 Sparse matrix 變得不是 Sparse。因此，解決方法就是使用 Incomplete-Cholesky factorization，來維持 Matrix 的 Sparsity。Incomplete-Cholesky factorization 在對 $A$ 計算 Cholesky factorization 的時候會將原本 $A$ 為 $0$ 的項維持為 $0$，只針對非零項計算，因此最後的結果會與原先的 Matrix 擁有相同的 Sparsity，同時可以得到一個很好的 Approximation $M=LL^T\approx A$。</p>
<h3 id="2-4-ICCG-Algorithm" class="heading-control"><a href="#2-4-ICCG-Algorithm" class="headerlink" title="2-4. ICCG Algorithm"></a>2-4. ICCG Algorithm<a class="heading-anchor" href="#2-4-ICCG-Algorithm" aria-hidden="true"></a></h3><p>Recall 前幾節的結果，Incomplete-Cholesky preconditioned conjugate gradient：</p>
<script type="math/tex; mode=display">
\begin{align*}
r_0 &= b-Ax_0\\
L &= \text{Preconditioning}(A)\\
z_0 &= L^{-1}L^{-T}r_0\\
p_0 &= z_0\\
\alpha_i &= \frac{r_i^Tz_i}{p_i^TAp_i}\\
x_{i+1} &= x_i + \alpha_ip_i\\
r_{i+1} &= r_i-\alpha_iAp_i\\
z_{i+1} &= L^{-1}L^{-T}r_{i+1}\\
\beta_{i+1} &= \frac{r^T_{i+1}z_{i+1}}{r^T_{i}z_{i}}\\
p_{i+1} &= z_{i+1} + \hat{\beta}_{i+1}p_i
\end{align*}</script><p>假設 Initial guess $x_0=\mathbf{0}$，可以得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
x_0 &= 0\\
r_0 &= b
\end{align*}</script><p>轉換成演算法：</p>
<p><img src="https://i.imgur.com/alrj8hn.png" alt></p>
<p>基本上這個演算法是一步一步根據推出來的公式計算，最標準的流程。維基百科也是使用這個順序。但為了後續方便使用 cuBLAS/cuSPARSE 實做，我比較習慣稍微調動一些順序變成下圖：</p>
<p><img src="https://i.imgur.com/hL1FiZD.png" alt></p>
<p>但基本上這兩個演算法做的事情是完全相同的。</p>
<h2 id="3-cuBLAS-amp-cuSPARSE" class="heading-control"><a href="#3-cuBLAS-amp-cuSPARSE" class="headerlink" title="3. cuBLAS & cuSPARSE"></a>3. cuBLAS & cuSPARSE<a class="heading-anchor" href="#3-cuBLAS-amp-cuSPARSE" aria-hidden="true"></a></h2><p>cuBLAS 與 cuSPARSE 都是 CUDA 內建的 Library，但是不同版本 CUDA 的 cuBLAS 與 cuSPARSE API 或多或少會有一些差異，這篇將以 CUDA 11.0 版本為主，此外也會加入一些 CUDA 8.0 的資料作為補充。</p>
<h3 id="3-1-cuBLAS-Introduction" class="heading-control"><a href="#3-1-cuBLAS-Introduction" class="headerlink" title="3-1. cuBLAS Introduction"></a>3-1. cuBLAS Introduction<a class="heading-anchor" href="#3-1-cuBLAS-Introduction" aria-hidden="true"></a></h3><p>cuBLAS 是以 CUDA 實作的 BLAS (Basic Linear Algebra Subprograms) library。提供了一些常用計算 Matrix/Vector 相關的 High-level API。例如：Matrix/vector copy、sort、dot product、multiplication 等等，另外也有提供對於特殊類型矩陣 (symmetric、triangular、hemitian) 做過優化的 API。</p>
<p>cuBLAS 使用 <strong>column-major</strong>，所以在儲存 Array 時需要注意順序。此外，在 Fortran 中是使用 1-based indexing，但在 C/C++ 中是使用 0-based indexing。</p>
<p><img src="https://i.imgur.com/EprrqSz.jpg" alt></p>
<p>使用 cuBLAS 時首先需要在程式碼中 Include <code>cublas_v2.h</code>，後面的 <code>v2</code> 代表是新的 API，而舊的 API <code>cublas.h</code> 則是 CUDA 4.0 以前的 API。<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><cublas_v2.h></span></span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>編譯時需要加上 Library 的 Link</p>
<ul>
<li>使用 <code>nvcc</code> Compile 需要加上 <code>-lcublas</code>：<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">nvcc hello_cublas.cu -o hello_cublas -lcublas</span><br></pre></td></tr></tbody></table></figure></li>
<li>使用 Visual Studio 則是從 <code>Property</code> 加：<ul>
<li>首先到 <code>CUDA C/C++</code> 的地方，在 <code>Additional Include Directories</code> 地方加上 <code>$(CudaToolkitIncludeDir)</code>。這是為了讓 Visual Studio 找的到 cuBLAS 的 header file。<img src="https://i.imgur.com/RgflLlW.png" alt></li>
<li>接著到 <code>CUDA Linker</code> 的地方，在 <code>Additional Librbary Directories</code> 地方加上 <code>$(CudaToolkitLibDir)</code>，並在 Additional Dependencies 地方加上 <code>cublas.lib</code><br><img src="https://i.imgur.com/QvGf09v.png" alt></li>
</ul>
</li>
</ul>
<p>這樣就可以正常編譯 cuBLAS。<br></p><div class="note warning">
            <p>需要注意的是 Visual Studio 的 <code>CUDA C/C++</code> 與 <code>CUDA Linker</code> 頁籤是對應 <code>.cu</code> File，如果想要在一般 <code>.c</code> 或 <code>.cpp</code> File 使用 cuBLAS 則必須在 <code>VC++ Directories</code> 與 <code>Linker</code> 下的 <code>Input</code> 頁籤做相同的處理，否則會出現 Link error。<br><img src="https://i.imgur.com/eUDlQzz.png" alt><br><img src="https://i.imgur.com/nWBcylz.png" alt></p>
          </div><p></p>
<h3 id="3-2-cuBLAS-APIs" class="heading-control"><a href="#3-2-cuBLAS-APIs" class="headerlink" title="3-2. cuBLAS APIs"></a>3-2. cuBLAS APIs<a class="heading-anchor" href="#3-2-cuBLAS-APIs" aria-hidden="true"></a></h3><ul>
<li><a href="https://docs.nvidia.com/cuda/cublas/index.html">cuBLAS :: CUDA Toolkit Documentation</a></li>
</ul>
<p>在使用 cuBLAS 的 API 前，需要先呼叫 <code>cublasCreate</code> 建立 cuBLAS 的 Handle，有 Handle 才能正常使用 cuBLAS API，且在呼叫 cuBLAS API 時，Handle 必須傳入 Function。最後程式結束時必須呼叫 <code>cublasDestroy</code> 來刪除 cuBLAS Handle。如下：</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    cublasHandle_t cubHandle;</span><br><span class="line">    cublasStatus_t cubStat;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Initialize cuBLAS</span></span><br><span class="line">    cubStat = cublasCreate( &cubHandle );</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Destroy cuBLAS handle</span></span><br><span class="line">    cubStat = cublasDestroy(cubHandle);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p><code>cubStat</code> 用來呼叫的 Function call 是否有正常執行，如果 API 有發生錯誤的話，Error code 會儲存在內，詳細可以參考官方的 Document，每個 Function call 底下都會寫有可能回傳的 Error code。</p>
<p>接下來是建立 Vector，使用 <code>cudaMalloc</code> 在 GPU 建立空間後，將資料複製到 GPU 有兩種方式：</p>
<ol>
<li>第一種是使用 CUDA 原生的 <code>cudaMemcpy</code> 複製到 GPU 上</li>
<li>第二種是使用 cuBLAS API <code>cublasSetVector</code>/<code>cublasGetVector</code> 來存取 GPU 上的空間</li>
</ol>
<p>不管使用哪個 API，最後都必須 Free memory。</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> vec[] = {<span class="comment">/*...*/</span>}; <span class="comment">// Vector in Host memory </span></span><br><span class="line"><span class="keyword">double</span> *d_vec;  <span class="comment">// Device pointer</span></span><br><span class="line">cudaMalloc(d_vec, N*<span class="keyword">sizeof</span>(<span class="keyword">double</span>));</span><br><span class="line"><span class="comment">// Set vector</span></span><br><span class="line">cudaMemcpy(d_vec, vec, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>),</span><br><span class="line">                  cudaMemcpyHostToDevice);</span><br><span class="line">cublasSetVector(N, <span class="keyword">sizeof</span>(<span class="keyword">double</span>), vec, <span class="number">1</span>, d_vec, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get vector</span></span><br><span class="line">cudaMemcpy(vec, d_vec, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>),</span><br><span class="line">                  cudaMemcpyDeviceToHost);</span><br><span class="line">cublasGetVector(N, <span class="keyword">sizeof</span>(<span class="keyword">double</span>), d_vec, <span class="number">1</span>, vec, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free memory</span></span><br><span class="line">cudaFree(d_vec);</span><br></pre></td></tr></tbody></table></figure>
<p>使用 cuBLAS API <code>cublasSetVector</code>/<code>cublasGetVector</code> 的好處在於，他有提供使用者設定 increment 的功能，也就是複製每個資料間要間隔多少資料，這在從一個 Matrix 中複製出 Column vector 或 Row vector 時非常實用。</p>
<p><img src="https://i.imgur.com/i702v6y.png" width="500px"><br><img src="https://i.imgur.com/zuEdAOU.png" width="500px"></p>
<p>接下來是 Matrix type，同樣使用 <code>cudaMalloc</code> 方式建立 GPU 空間，同 Vector，可以使用 <code>cudaMemcpy</code> 或是使用 cuBLAS API <code>cublasSetMatrix</code>/<code>cublasGetMatrix</code> 存取 Matrix。最重要的是，他是 <strong>Column-major</strong>！因此 Host memory 與 Device memory 都必須儲存成 Column-major 的形式。</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line">cudaMalloc(&d_A, N*M*<span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// copy Matrix A from host to device</span></span><br><span class="line">cublasSetMatrix(M, N, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), A, M, d_A, M);</span><br><span class="line"></span><br><span class="line"><span class="comment">// copy from device to host</span></span><br><span class="line">cublasGetMatrix(M, N, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), d_A, M, A, M);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free memory</span></span><br><span class="line">cudaFree(d_A);</span><br></pre></td></tr></tbody></table></figure>
<p>cuBLAS API <code>cublasSetMatrix</code>/<code>cublasGetMatrix</code> 有兩個參數 <code>lda</code> 與 <code>ldb</code>，分別代表 Source 與 Destination Matrix 的 Leading dimension。</p>
<p>接下來是介紹 cuBLAS 的 Operation API，cuBLAS 將 API 分成 3 個 Level：</p>
<ul>
<li>Level-1：與 Vector 相關，或是 Vector-vector operations。例如，<code>min</code>、<code>max</code>、<code>sum</code>、<code>copy</code>、<code>dot product</code>、<code>euclidean norm</code> 等等。</li>
<li>Level-2：Matrix-vector operations。例如，Matrix-vector multiplication 等等。</li>
<li>Level-3：Matrix-matrix operations。例如，Matrix-matrix multiplication 等等。</li>
</ul>
<p>根據不同的資料型態，API 的名稱也會不一樣：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Type</th>
<th>Notation <code><t></code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>float</code></td>
<td>‘s’ or ‘S’</td>
<td>Real single-precision</td>
</tr>
<tr>
<td><code>double</code></td>
<td>‘d’ or ‘D’</td>
<td>Real double-precision</td>
</tr>
<tr>
<td><code>cuComplex</code></td>
<td>‘c’ or ‘C’</td>
<td>Complex single-precision</td>
</tr>
<tr>
<td><code>cuDoubleComplex</code></td>
<td>‘z’ or ‘Z’</td>
<td>Complex double-precision</td>
</tr>
</tbody>
</table>
</div>
<p>舉例來說， Level-1 的 Function <code>cublas<t>axpy</code>，計算 Vector $x$ 與 Vector $y$ 的加法，$y\gets\alpha x+y$</p>
<p><img src="https://i.imgur.com/QKCou7y.png" alt></p>
<p>Level-2 的 Function <code>cublas<t>gemv</code>，計算 Matrix $A$ 與 Vector $x$ 的乘法，$y\gets\alpha \text{OP}(A)x+\beta y$</p>
<p><img src="https://i.imgur.com/oTeomLr.png" alt></p>
<p>其中 $\text{OP}$ 是在計算前，cuBLAS 會對 Matrix $A$ 套用的 Operation：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>OP</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUBLAS_OP_N</code></td>
<td>The non-transpose operation is selected</td>
</tr>
<tr>
<td><code>CUBLAS_OP_T</code></td>
<td>The transpose oeration is selected</td>
</tr>
<tr>
<td><code>CUBLAS_OP_C</code></td>
<td>The conjugate transpose operation is selected</td>
</tr>
</tbody>
</table>
</div>
<p>Level-3 的 Function <code>cublas<t>gemm</code> 計算 Matrix $A$ 與 Matrix $B$ 的矩陣乘法，$C\gets\alpha\text{OP}(A)\text{OP}(B)+\beta C$</p>
<p><img src="https://i.imgur.com/4KbJWH4.png" alt></p>
<p>Level-3 除了一般的 Matrix-matrix multiplication 之外，cuBLAS 還有提供額外的 API 給特殊的 Matrix 型態使用：<br><code>cublasFillMode_t</code> 用來指定是 Upper triangle 還是 Lower triangle。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cublasFillMode_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUBLAS_FILL_MODE_LOWER</code></td>
<td>The lower part of the matrix is filled</td>
</tr>
<tr>
<td><code>CUBLAS_FILE_MODE_UPPER</code></td>
<td>The upper part of the matrix is filled</td>
</tr>
</tbody>
</table>
</div>
<p>例如 <code>cublas<t>symm</code>，計算 Symmetric matrix-matrix multiplication，如果你的 Matrix 是 Symmetric 就可以使用這個 API 來進行更有效率的計算，則 cuBLAS 在運算時其實只需要取一半的值 (Upper triangle 或是 Lower triangle) 出來就可以了，另外一半是對稱的，這樣在 GPU memory 的存取上會比較有效率。</p>
<p><img src="https://i.imgur.com/cXQpf2O.png" alt></p>
<p>另外還有像是 <code>cublas<t>trmm</code> 計算 Triangle matrix-matrix multiplication。</p>
<p><code>cublasDiagType_t</code> 代表 Matrix 的對角線是不是 Unit (Unit matrix 的那個 unit)，如果是的話，cuBLAS 在運算時就會忽略對角線。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cublasDiagType_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUBLAS_DIAG_NON_UNIT</code></td>
<td>The matrix diagonal has non-unit elements</td>
</tr>
<tr>
<td><code>CUBLAS_DIAG_UNIT</code></td>
<td>The matrix diagonal has unit elements</td>
</tr>
</tbody>
</table>
</div>
<p>除此之外，cuBLAS 還有提供 <code>Stream</code> API 可以進行 Asynchronous 的計算，詳細請看 document。</p>
<h3 id="3-3-Hello-cuBLAS" class="heading-control"><a href="#3-3-Hello-cuBLAS" class="headerlink" title="3-3. Hello cuBLAS"></a>3-3. Hello cuBLAS<a class="heading-anchor" href="#3-3-Hello-cuBLAS" aria-hidden="true"></a></h3><p>這節將會使用 cuBLAS 示範 Matrix-matrix multiplication。首先寫好基本的 Framework：</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><iostream></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><cuda.h></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><cublas_v2.h></span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// You can find the source code here:</span></span><br><span class="line"><span class="comment">// https://gist.github.com/Ending2015a/4eb30e7665d91debc723d9c73afec821</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"error_helper.hpp"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    cublasHandle_t cubHandle;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Initialize cuBLAS</span></span><br><span class="line">    error_check(cublasCreate( &cubHandle ));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Get cuBLAS version information</span></span><br><span class="line">    <span class="keyword">int</span> major_version, minor_version;</span><br><span class="line">    </span><br><span class="line">    error_check(cublasGetProperty( MAJOR_VERSION, &major_version ));</span><br><span class="line">    error_check(cublasGetProperty( MINOR_VERSION, &minor_version ));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Say hello to cuBLAS</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"Hello cuBLAS!"</span> << <span class="built_in">std</span>::<span class="built_in">endl</span></span><br><span class="line">              << <span class="string">"* major version: "</span> << major_version << <span class="built_in">std</span>::<span class="built_in">endl</span></span><br><span class="line">              << <span class="string">"* minor version: "</span> << minor_version << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> Matrix-matrix multiplication</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Destroy cuBLAS handle</span></span><br><span class="line">    error_check(cublasDestroy(cubHandle));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>這段 Code 在一開始建立了 <code>cubHandle</code>，接著向 cuBLAS 打招呼，順便印出 cuBLAS 的版本資訊，最後將 <code>cubHandle</code> 摧毀，結束程序。</p>
<p>接下來是撰寫 Matrix-matrix multiplication，$AB=C$，其中 $A\in\mathbb{R}^{N\times M}$，$B\in\mathbb{R}^{M\times P}$，$C\in\mathbb{R}^{N\times P}$。<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// <span class="doctag">TODO:</span> Matrix-matrix multiplication</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> M = <span class="number">4</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> P = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">float</span> a[N*M];</span><br><span class="line"><span class="keyword">float</span> b[M*P];</span><br><span class="line"><span class="keyword">float</span> c[N*P];</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create matrix A, B in row-major order</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i<N*M;++i)</span><br><span class="line">{</span><br><span class="line">    a[i] = (<span class="keyword">float</span>)(i+<span class="number">1</span>); <span class="comment">//1, 2, 3, 4, ~</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i<M*P;++i)</span><br><span class="line">{</span><br><span class="line">    b[i] = (<span class="keyword">float</span>)(M*P-i); <span class="comment">//8, 7, 6, 5, ~</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>宣告 Dimension，宣告 Host memory 上的 matrix，需要注意的是<strong>在這邊我是使用 row-major 的方式初始化 Matrix</strong>，後面有點小 Trick。印出來看：</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_matrix</span><span class="params">(<span class="keyword">float</span> *mat, <span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">int</span> M)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i<N*M;++i)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> << mat[i];</span><br><span class="line">        <span class="keyword">if</span>((i+<span class="number">1</span>)%M==<span class="number">0</span>)</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">", "</span>;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">//print matrix</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"A: "</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">print_matrix(a, N, M);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"B: "</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">print_matrix(b, M, P);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></tbody></table></figure>
<p>接著是宣告、分配 Device memory 空間，使用 cuBLAS API 複製 Matrix data 到 device：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">float</span> *device_a;</span><br><span class="line"><span class="keyword">float</span> *device_b;</span><br><span class="line"><span class="keyword">float</span> *device_c;</span><br><span class="line"></span><br><span class="line">error_check(cudaMalloc(&device_a, N*M*<span class="keyword">sizeof</span>(<span class="keyword">float</span>)));</span><br><span class="line">error_check(cudaMalloc(&device_b, M*P*<span class="keyword">sizeof</span>(<span class="keyword">float</span>)));</span><br><span class="line">error_check(cudaMalloc(&device_c, N*P*<span class="keyword">sizeof</span>(<span class="keyword">float</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">// copy host matrix 'a' to device matrix 'device_a'</span></span><br><span class="line">error_check(cublasSetMatrix(M, N, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), a, M, device_a, M));</span><br><span class="line">error_check(cublasSetMatrix(P, M, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), b, P, device_b, P));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊因為之後打算使用 <code>cublasSgemm</code> 這個 API 計算矩陣乘法：</p>
<script type="math/tex; mode=display">
C\gets\alpha\text{OP}(A)\text{OP}(B) + \beta C</script><p><code>cublasSgemm</code> 會要求兩個常數 $\alpha$ 與 $\beta$：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">float</span> alpha = <span class="number">1.0f</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">float</span> beta = <span class="number">0.0f</span>;</span><br></pre></td></tr></tbody></table></figure><br>直接分別設為 $1$ 與 $0$，計算就會 reduce 成 $C\gets\text{OP}(A)\text{OP}(B)$。<p></p>
<p>接下來就是 Tricky 的地方。做 Matrix-matrix multiplication：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line">error_check(cublasSgemm(cubHandle, CUBLAS_OP_N, CUBLAS_OP_N, P, N, M,</span><br><span class="line">                        &alpha, device_b, P, device_a, M,</span><br><span class="line">                        &beta, device_c, P));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊要注意 Tricky 的地方。因為 Row-major 與 Column-major 只差在一次 Transpose，因此根據 $(AB)^T=B^TA^T$ 的特性，Row-major 的 matrix <code>b</code> 與 matrix <code>a</code> 在 Column-major 下必須倒過來相乘。在 row-major 下，我們的 Matrix <code>a</code> 與 <code>b</code> 會長這樣：</p>
<script type="math/tex; mode=display">
a=
\left[\begin{matrix}
1 & 2 & 3 & 4\\
5 & 6 & 7 & 8\\
9 & 10 & 11 & 12\\
\end{matrix}\right], \quad
b=
\left[\begin{matrix}
8 & 7 \\
6 & 5 \\
4 & 3 \\
2 & 1 \\
\end{matrix}\right]</script><p>但是 cuBLAS 是 column-major，因此在 cuBLAS 看來矩陣其實長這樣：</p>
<script type="math/tex; mode=display">
a=
\left[\begin{matrix}
1 & 5 & 9 \\
2 & 6 & 10 \\
3 & 7 & 11 \\
4 & 8 & 12\\
\end{matrix}\right], \quad
b=
\left[\begin{matrix}
8 & 6 & 4 & 2\\
7 & 5 & 3 & 1\\
\end{matrix}\right]</script><p>所以在計算 $AB$ 時，必須將 $A$ 與 $B$ 對調，變成 $BA$，如此一來就會計算出 Transpose 過的 $C$</p>
<script type="math/tex; mode=display">
c=\left[\begin{matrix}
8 & 6 & 4 & 2\\
7 & 5 & 3 & 1\\
\end{matrix}\right] \left[\begin{matrix}
1 & 5 & 9 \\
2 & 6 & 10 \\
3 & 7 & 11 \\
4 & 8 & 12\\
\end{matrix}\right] = \left[\begin{matrix}
40 & 120 & 200\\
30 & 94 & 158\\
\end{matrix}\right]</script><p>因為 cuBLAS 是 column-major，因此直接將 <code>device_c</code> copy 回 <code>c</code>，在 row-major 下就會 Transpose 回來，得到正確的值。</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// device to host</span></span><br><span class="line">error_check(cublasGetMatrix(P, N, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), device_c, P, c, P));</span><br><span class="line"></span><br><span class="line"><span class="comment">// print answer</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"C: "</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">print_matrix(c, N, P);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></tbody></table></figure>
<p>在 row-major 下，<code>c</code> 為：</p>
<script type="math/tex; mode=display">
c= \left[\begin{matrix}
40 & 30 \\
120 & 94 \\
200 & 158\\
\end{matrix}\right]</script><p>最後不要忘記 Free memory：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Free memory</span></span><br><span class="line">cudaFree(device_a);</span><br><span class="line">cudaFree(device_b);</span><br><span class="line">cudaFree(device_c);</span><br></pre></td></tr></tbody></table></figure><p></p>
<p><del>如果能夠做到這種程度，代表你對 row-major/column-major 關係很熟了</del></p>
<h3 id="3-4-cuSPARSE-Introduction" class="heading-control"><a href="#3-4-cuSPARSE-Introduction" class="headerlink" title="3-4. cuSPARSE Introduction"></a>3-4. cuSPARSE Introduction<a class="heading-anchor" href="#3-4-cuSPARSE-Introduction" aria-hidden="true"></a></h3><p>cuSPARSE 是 cuBLAS 的 Sparse version。在 cuBLAS 中使用的都是 Dense vector/matrix，而 cuSPARSE 則提供了一系列 API 用來從事 Sparse vector/matrix 相關的計算。且除了 Matrix/Vector operation 之外，cuSPARSE 也有提供許多求解 Sparse linear system 相關的 API。</p>
<p>Sparse matrix 有一個特點就是矩陣非常大，其中包含了許多 $0$ 項，如果在 GPU 上儲存這些 $0$ 項其實是非常浪費空間的，因此 cuSPARSE 提供了幾種 Data format 來儲存 Sparse matrix，節約 Memory 使用量：</p>
<p><del>這邊我懶得重寫，直接貼我的 Slide</del></p>
<ul>
<li>COO (Coordinate Format)<br>這是最為直接的表示方法，儲存矩陣中每個非 $0$ 項的位置：<br><img src="https://i.imgur.com/ZCJtDi1.png" alt></li>
<li>CSR (Compressed Sparse Row Format)<br>這個表示法對 Row 方向的資料進行了壓縮，他紀錄的是 Row 開始的 Entry 的 index。<br><img src="https://i.imgur.com/16gxonJ.png" alt></li>
<li>CSC (Compressed Sparse Column Format)<br>這個表示法則是對 Col 進行壓縮<br><img src="https://i.imgur.com/A87jBrY.png" alt></li>
<li>另外還有 BSR 與 BSRX 但由於不常使用，就不多介紹</li>
</ul>
<p>Sparse vector 使用兩個 Array 來表示，第一個 Array 儲存 Non-zero term，第二個 Array 則儲存 Non-zero term 的 Index。</p>
<p>使用 cuSPARSE 的方式與 cuBLAS 類似，先 Include <code>cusparse.h</code> (這邊不用 <code>v2</code>)，Compile 時同樣要加上 Link：</p>
<ul>
<li>使用 <code>nvcc</code>：<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">nvcc hello_cusparse.cu -o hello_cusparse -lcusparse</span><br></pre></td></tr></tbody></table></figure></li>
<li>使用 Visual Studio，在 <code>Additional Dependencies</code> 加上 <code>cusparse.lib</code></li>
</ul>
<p>就可以正常 Compile。</p>
<h3 id="3-5-cuSPARSE-API" class="heading-control"><a href="#3-5-cuSPARSE-API" class="headerlink" title="3-5. cuSPARSE API"></a>3-5. cuSPARSE API<a class="heading-anchor" href="#3-5-cuSPARSE-API" aria-hidden="true"></a></h3><ul>
<li><a href="https://docs.nvidia.com/cuda/cusparse/index.html">cuSPARSE :: CUDA Toolkit Documentation</a></li>
</ul>
<p>在使用 cuSPARSE 時，需要呼叫 <code>cusparseCreate</code> 建立 cuSPARSE 的 Handle <code>cusparseHandle_t</code>，在最後程式結束前則需呼叫 <code>cusparseDestroy</code> Destroy：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    cusparseHandle_t cusHandle;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Initialize cuSPARSE</span></span><br><span class="line">    cusparseCreate( &cusHandle );</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Destroy cuSPARSE handle</span></span><br><span class="line">    cusparseDestroy(cusHandle);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>建立 Sparse matrix 時，則需要根據 Data format (COO、CSR、CSC) 建立對應的 Data，如：COO format 需要建立三個 Array：</p>
<ul>
<li><code>cooValA</code> 儲存 Non-zero term</li>
<li><code>cooRowIndA</code> 儲存 Non-zero term 的 row index</li>
<li><code>cooColIndA</code> 儲存 Non-zero term 的 column index。</li>
</ul>
<p>如果想要建立 <code>CSR</code> 或 <code>CSC</code> format 但是不會 (或是不方便) 建立 Row/Column index 話，可以先建立 <code>COO</code> format，使用 cuSPARSE 提供的 Format Conversion API <code>cusparse<t>coo2csr</code> 做 Data format 的轉換。也可以直接從 Dense matrix，使用 <code>cusparse<t>dense2csc</code> / <code>cusparse<t>dense2csr</code> 轉換。</p>
<p>與 cuBLAS 不同的地方在於，cuSPARSE 在宣告 Matrix 時，必須建立該 Matrix 的 Descriptor <code>cusparseMatDescr_t</code>。<code>cusparseMatDescr_t</code> 包含了大略以下資料：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line">typeof <span class="class"><span class="keyword">struct</span> {</span></span><br><span class="line">    cusparseMatrixType_t MatrixType;</span><br><span class="line">    cusparseFillMode_t FillMode;</span><br><span class="line">    cusparseDiagType_t DiagType;</span><br><span class="line">    cusparseIndexBase_t IndexBase;</span><br><span class="line">} cusparseMatDescr_t;</span><br></pre></td></tr></tbody></table></figure><p></p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cusparseDiagType_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUSPARSE_DIAG_TYPE_NON_UNIT</code></td>
<td>The matrix diagonal has non-unit elements.</td>
</tr>
<tr>
<td><code>CUSPARSE_DIAG_TYPE_UNIT</code></td>
<td>The matrix diagonal has unit elements.</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cusparseFillMode_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUSPARSE_FILL_MODE_LOWER</code></td>
<td>The lower triangular part is stored.</td>
</tr>
<tr>
<td><code>CUSPARSE_FILL_MODE_UPPER</code></td>
<td>The upper triangular part is stored.</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cusparseIndexBase_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUSPARSE_INDEX_BASE_ZERO</code></td>
<td>The base index is zero.</td>
</tr>
<tr>
<td><code>CUSPARSE_INDEX_BASE_ONE</code></td>
<td>The base index is one.</td>
</tr>
</tbody>
</table>
</div>
<p>在過去的版本中 (CUDA 8.0) 為了節約 Memory，針對特殊的 Matrix (Symmetric、Triangular、Hermitian) 可以在 <code>cusparseMatrixType_t</code> 指定。如果有指定特殊的 Matrix 的話，cuSPARSE 會只使用 Lower triangle 或 Upper triangle 半邊的 Data 做計算，端看 <code>cusparseFillMode_t</code> 是指定 Lower 還是 Upper。如此一來，使用者可以只需要儲存半邊的 Matrix。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cusparseMatrixType_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUSPARSE_MATRIX_TYPE_GENERAL</code></td>
<td>The matrix is general.</td>
</tr>
<tr>
<td><code>CUSPARSE_MATRIX_TYPE_SYMMETRIC</code></td>
<td>The matrix is symmetric. (<em>Deprecated</em>)</td>
</tr>
<tr>
<td><code>CUSPARSE_MATRIX_TYPE_HERMITIAN</code></td>
<td>The matrix is Hermitian. (<em>Deprecated</em>)</td>
</tr>
<tr>
<td><code>CUSPARSE_MATRIX_TYPE_TRIANGULAR</code></td>
<td>The matrix is triangular. (<em>Deprecated</em>)</td>
</tr>
</tbody>
</table>
</div>
<div class="note warning">
            <p>然而在 CUDA 11.0 中，<strong>官方不再鼓勵使用者使用 General 以外的 Matrix Type</strong>，原因是在許多計算中 (Matrix-vector multiplication、Preconditioners、Lienar system solvers 等等)，使用<strong>非 General type </strong>的計算耗費時間會是 General type 的 10 倍慢**，因此在 CUDA 11.0 中，大部分的 API 不再支援非 General type 的 Matrix。因此使用者在指定 Matrix type 時直接填寫 <code>CUSPARSE_MATRIX_TYPE_GENERAL</code> 即可。</p>
          </div>
<p>以下示範如何宣告一個 <code>cusparseMatDescr_t</code>：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Create descriptor for the matrix A</span></span><br><span class="line">cusparseMatDescr_t descr_A = <span class="number">0</span>;</span><br><span class="line">cusparseCreateMatDescr(&descr_A);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Set properties of the matrix A</span></span><br><span class="line">cusparseSetMatType(descr_A, CUSPARSE_MATRIX_TYPE_GENERAL);</span><br><span class="line">cusparseSetMatIndexBase(descr_A, CUSPARSE_INDEX_BASE_ZERO);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free descriptor</span></span><br><span class="line">cusparseDestroyMatDescr(descr_A);</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>cuSPARSE 同樣有提供 3 個 Level 的 API：</p>
<ul>
<li>Level-1：Sparse vector operation (在 CUDA 11.0 中已列為 <em>Deprecated</em>)</li>
<li>Level-2：Sparse matrix-dense/sparse vector operation 以及求解 Matrix-vector sparse linear system 相關 API</li>
<li>Level-3：Sparse matrix-dense matrix operation 以及求解 Matrix-matrix sparse linear system</li>
</ul>
<p>需要注意的是 Level-2 與 Level-3 求解 Sparse linear system 的 API 有限定 Matrix 必須是 Triangular，如果矩陣非 Triangular 則只會使用 Lower triangle 的 Data，Upper triangle 忽略。</p>
<p>注意到，Level-3 是 Sparse matrix 對 dense matrix operation，Sparse matrix 對 sparse matrix 的 operation 則是放在 Extra API，但其中也有一些 API 已經列為 <em>Deprecated</em>。而多數 API 被列為 Deprecated 的原因是因為，CUDA 11.0 決定要將一些常用的 Multiplication operation 做整合，因此提供了另一種型態的 API —- Generic API。</p>
<p>使用 Generic API 的方法與 cuBLAS 大不相同。首先需要建立對應 Data format 的 Descriptor (不同於 <code>cusparseMatDescr_t</code>)。例如，如果要建立 <code>CSR</code> format 的 Sparse matrix 則必須呼叫對應的 API 來建立 Descriptor：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line">cusparseSpMatDescr_t mat_A;</span><br><span class="line">cusparseCreateCsr(&mat_A, N, N, nnz, d_rowPtr, d_colIdx, d_A, </span><br><span class="line">                 CUSPARSE_INDEX_32I, <span class="comment">// row index data type (int)</span></span><br><span class="line">                 CUSPARSE_INDEX_32I, <span class="comment">// col index data type (int) </span></span><br><span class="line">                 CUSPARSE_INDEX_BASE_ZERO, <span class="comment">// 0-based index</span></span><br><span class="line">                 CUDA_R_64F);  <span class="comment">// Data type (real double-floating point)</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>建立 Dense vector：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line">cusparseDnVecDescr_t vec_x;</span><br><span class="line">cusparseCreateDnVec(&vec_x, N, d_x, CUDA_R_64F);</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>建立完後，呼叫 Generic API 時都是使用 Descriptor 來代表 Vector/Matrix。例如 <code>cusparseSpMV</code> 計算 Sparse-matrix 對 dense-vector 的 Multiplication：</p>
<script type="math/tex; mode=display">
y\gets \alpha \text{OP}(A)x+\beta y</script><p><img src="https://i.imgur.com/zxsshA5.png" alt></p>
<p>計算分成兩個階段，首先使用者必須自行呼叫 <code>cusparseSpMV_bufferSize</code> 估算 <code>cusparseSPMV</code> 所需的額外 Buffer 空間大小，之後自行呼叫 <code>cudaMalloc</code> 建立後，再呼叫 <code>cusparseSpMV</code> 換成計算。<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> alpha = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">double</span> beta = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">size_t</span> buf_size = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">double</span> *d_buf;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Compute buffer size in computing matrix-vector multiplocation</span></span><br><span class="line">cusparseSpMV_bufferSize(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE, </span><br><span class="line">                        &alpha, mat_A, vec_x, </span><br><span class="line">                        &beta, vec_y, </span><br><span class="line">                        CUDA_R_64F, </span><br><span class="line">                        CUSPARSE_CSRMV_ALG1, </span><br><span class="line">                        &buf_size);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Allocate buffer memory</span></span><br><span class="line">cudaMalloc(&d_buf, buf_size);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Perform matrix-vector multiplocation</span></span><br><span class="line">cusparseSpMV(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE, </span><br><span class="line">             &alpha, mat_A, vec_x, </span><br><span class="line">             &beta, vec_y, </span><br><span class="line">             CUDA_R_64F, </span><br><span class="line">             CUSPARSE_CSRMV_ALG1, </span><br><span class="line">             d_buf));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free buffer</span></span><br><span class="line">cudaFree(d_buf);</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>最後需要呼叫對應的 API 來銷毀 Generic API 的 descriptor<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Destroy descriptor</span></span><br><span class="line">cusparseDestroySpMat(mat_A);</span><br><span class="line">cusparseDestroyDnVec(vec_x);</span><br><span class="line">cusparseDestroyDnVec(vec_y);</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>求解 Sparse linear system 在 CUDA 11.0 中也有大幅度的變更，原先在 CUDA 8.0 中，求解 Sparse linear system 時 cuSPARSE 會自己建立刪除額外的 Buffer，但從 CUDA 11.0 開始也需要使用者自行處理 Buffer。除此之外，Sparse linear system 只支援 Triangular matrix，若 Matrix 不是 Triangular，cuSPARSE 會忽略除了 Lower triangle 以外的 Data。</p>
<p>求解 Sparse linear system 前需要先建立對應 Data format 的 Infomation object，接著分成兩個階段 (Phase)，首先先執行 Analysis phase 分析 Matrix 型態，之後再執行 Solve phase 解出 Linear system。以下示範使用 <code>cusparse<t>csrsv2</code> 求解 Sparse linear system $\text{OP}(A)y=\alpha x$：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Create infomation object</span></span><br><span class="line">csrsv2Info_t info_A;</span><br><span class="line">cusparseCreateCsrsv2Info(&info_A);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> buf_size;</span><br><span class="line"><span class="keyword">double</span> *d_buf;</span><br><span class="line"><span class="comment">// Compute buffer size for solving linear system</span></span><br><span class="line">cusparseDcsrsv2_bufferSize(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE,</span><br><span class="line">                          N, nnz, descr_A,  <span class="comment">//descr_A: cusparseMatDescr_t</span></span><br><span class="line">                          d_A, d_rowIdx, d_colIdx,</span><br><span class="line">                          info_A, &buf_size);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Allocate buffer memory</span></span><br><span class="line">cudaMalloc(&d_buf, buf_size);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Analysis phase</span></span><br><span class="line">cusparseDcsrsv2_analysis(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE,</span><br><span class="line">                        N, nnz, descr_A,  <span class="comment">//descr_A: cusparseMatDescr_t</span></span><br><span class="line">                        d_A, d_rowIdx, d_colIdx,</span><br><span class="line">                        info_A, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf);</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> alpha = <span class="number">1</span>;</span><br><span class="line"><span class="comment">// Solve phase</span></span><br><span class="line">cusparseDcsrsv2_solve(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE,</span><br><span class="line">                     N, nnz, &alpha, descr_A, <span class="comment">//descr_A: cusparseMatDescr_t</span></span><br><span class="line">                     d_A, d_rowIdx, d_colIdx,</span><br><span class="line">                     info_A, d_x, d_y, </span><br><span class="line">                     CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free buffer</span></span><br><span class="line">cudaFree(d_buf);</span><br><span class="line"><span class="comment">// Destroy information object</span></span><br><span class="line">cusparseDestroyCsrsv2Info(info_A);</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>求解 Linear system 這類 API 在 CUDA 11.0 時有新增一個新的參數<code>cusparseSolvePolicy_t</code>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cusparseSolvePolicy_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUSPARSE_SOLVE_POLICY_NO_LEVEL</code></td>
<td>No level information is generated and used.</td>
</tr>
<tr>
<td><code>CUSPARSE_SOLVE_POLICY_USE_LEVEL</code></td>
<td>Generate and use level information.</td>
</tr>
</tbody>
</table>
</div>
<p>在 Document 裡，每個 API 底下的解說都有寫這個參數的用途，但並沒有說的很詳細，我也不清楚這個參數實際的用途。簡而言之，這個參數能夠提升某些運算的效率，本身並不影響計算的結果。</p>
<p>除了這些 API 之外，cuSPARSE 還有提供其他額外的 API，例如 Preconditioner、Format conversion 等等，詳細請看 Document。</p>
<h3 id="3-6-Hello-cuSPARSE" class="heading-control"><a href="#3-6-Hello-cuSPARSE" class="headerlink" title="3-6. Hello cuSPARSE"></a>3-6. Hello cuSPARSE<a class="heading-anchor" href="#3-6-Hello-cuSPARSE" aria-hidden="true"></a></h3><p>這邊範例將會介紹如何使用 cuSPARSE 計算 Sparse matrix dense vector multiplication。首先建立基礎 Framework。</p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><iostream></span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><cuda_runtime.h></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><cusparse.h></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><cublas_v2.h></span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// You can find the source code here:</span></span><br><span class="line"><span class="comment">// https://gist.github.com/Ending2015a/4eb30e7665d91debc723d9c73afec821</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"error_helper.hpp"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="comment">// Initialize cuBLAS / cuSPARSE</span></span><br><span class="line">    cublasHandle_t cubHandle = <span class="number">0</span>;</span><br><span class="line">    error_check(cublasCreate(&cubHandle));</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">cusparsehandle_t</span> cusHandle = <span class="number">0</span>;</span><br><span class="line">    error_check(cusparseCreate(&cusHandle));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> Sparse matrix dense vector multiplication</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Destroy handles</span></span><br><span class="line">    cusparseDestroy(cusHandle);</span><br><span class="line">    cublasDestroy(cubhandle);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>接下來在 Host 建立一個 Sparse matrix 與一個 Dense vector，這邊隨便寫一些 Hash 函數來產生亂數：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// random hash (2D to 1D)</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">hash21</span><span class="params">(<span class="keyword">double</span> x, <span class="keyword">double</span> y)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    x = (x*<span class="number">24.36854</span> + y*<span class="number">15.75427</span> + <span class="number">43.454614</span>);</span><br><span class="line">    y = (x*<span class="number">57.5654</span> + y*<span class="number">21.1435</span> + <span class="number">37.159636</span>);</span><br><span class="line">    x = <span class="built_in">sin</span>(x+y)*<span class="number">516.918738574</span>;</span><br><span class="line">    <span class="keyword">return</span> ((<span class="keyword">int</span>)((<span class="number">-1.</span> + <span class="number">2.</span> * <span class="built_in">fabs</span>(x - (<span class="keyword">long</span>)x))*<span class="number">100.</span>))*<span class="number">0.1</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// create tridiagonal matrix in column-major order</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">createDenseMatrix</span><span class="params">(<span class="keyword">double</span> **o_mat, <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">double</span> *mat = <span class="keyword">new</span> <span class="keyword">double</span>[N*N]{};</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> x=<span class="number">0</span>;x<N;++x)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> y=<span class="number">0</span>;y<N;++y)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(x == y)</span><br><span class="line">                mat[y+x*N] = hash21(x, y);</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>( <span class="built_in">abs</span>(x-y) == <span class="number">1</span> )</span><br><span class="line">                mat[y+x*N] = hash21(x, y);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    *o_mat = mat;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// create random vector</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">createDenseVector</span><span class="params">(<span class="keyword">double</span> **o_vec, <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">double</span> *vec = <span class="keyword">new</span> <span class="keyword">double</span>[N]{};</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> x=<span class="number">0</span>;x<N;++x)</span><br><span class="line">    {</span><br><span class="line">        vec[x] = hash21(x, -x+<span class="number">10</span>);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    *o_vec = vec;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// print matrix</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_matrix</span><span class="params">(<span class="keyword">double</span> *mat, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i<N;++i)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j<N;++j)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(j != <span class="number">0</span>) <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">", "</span>;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="built_in">std</span>::fixed << <span class="built_in">std</span>::setprecision(<span class="number">2</span>) << mat[i + j*N];</span><br><span class="line">        }</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// print vector</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_vector</span><span class="params">(<span class="keyword">double</span> *vec, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i<N;++i)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span>(i != <span class="number">0</span>) <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">", "</span>;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="built_in">std</span>::fixed << <span class="built_in">std</span>::setprecision(<span class="number">2</span>) << vec[i];</span><br><span class="line">    }</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊不需要太在意寫了甚麼，基本上就是用盡各種辦法建立了一個 Sparse matrix 跟 Dense vector。</p>
<p>接著回到 <code>TODO</code> 的地方繼續完成 Matrix vector multiplication。建立 Matrix 跟 Vector，copy 到 GPU 上：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// <span class="doctag">TODO:</span> Sparse matrix dense vector multiplication</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate random dense tridiagonal matrix A (column-major)</span></span><br><span class="line"><span class="keyword">int</span> N = <span class="number">5</span>;</span><br><span class="line"><span class="keyword">double</span> *A = <span class="number">0</span>;</span><br><span class="line">createDenseMatrix(&A, N);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print matrix</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"My matrix 'A': "</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">print_matrix(A, N);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate random dense vector b</span></span><br><span class="line"><span class="keyword">double</span> *b = <span class="number">0</span>;</span><br><span class="line">createDenseVector(&b, N);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print matrix</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"My vector 'b': "</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">print_vector(b, N);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Device pointer (Dense)</span></span><br><span class="line"><span class="keyword">double</span> *d_A = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">double</span> *d_b = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">double</span> *d_c = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Allocate GPU memory</span></span><br><span class="line">error_check(cudaMalloc(&d_A, <span class="keyword">sizeof</span>(<span class="keyword">double</span>) * N * N));</span><br><span class="line">error_check(cudaMalloc(&d_b, <span class="keyword">sizeof</span>(<span class="keyword">double</span>) * N));</span><br><span class="line">error_check(cudaMalloc(&d_c, <span class="keyword">sizeof</span>(<span class="keyword">double</span>) * N));</span><br><span class="line"></span><br><span class="line"><span class="comment">// you can also use cudaMemcpy/cudaMemcpy2D as well.</span></span><br><span class="line">error_check(cublasSetMatrix(N, N, <span class="keyword">sizeof</span>(<span class="keyword">double</span>), A, N, d_A, N));</span><br><span class="line">error_check(cublasSetVector(N, <span class="keyword">sizeof</span>(<span class="keyword">double</span>), b, <span class="number">1</span>, d_b, <span class="number">1</span>));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>建立 Matrix A 的 Description：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Create descriptor of the matrix A</span></span><br><span class="line">cusparseMatDescr_t descr_A = <span class="number">0</span>;</span><br><span class="line">error_check(cusparseCreateMatDescr(&descr_A));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Set properties of the matrix A</span></span><br><span class="line">error_check(cusparseSetMatType(descr_A, CUSPARSE_MATRIX_TYPE_GENERAL));</span><br><span class="line">error_check(cusparseSetMatIndexBase(descr_A, CUSPARSE_INDEX_BASE_ZERO));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>接下來要將 Dense matrix A 轉換成 Sparse 的 Format，因此要先計算 Matrix A 的 Non-zero term 數量，這個地方可以使用 cuSPARSE API <code>cusparseDnnz</code> 計算，之後再使用 <code>cusparseDdense2csr</code> 將 Dense format 的 Matrix A 轉換成 CSR format。<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Count non-zero terms</span></span><br><span class="line"><span class="keyword">int</span> nnz = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> *d_nnz_perRow = <span class="number">0</span>;</span><br><span class="line">error_check(cudaMalloc(&d_nnz_perRow, N * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">error_check(cusparseDnnz(cusHandle, CUSPARSE_DIRECTION_ROW, N, </span><br><span class="line">                        N, descr_A, d_A, </span><br><span class="line">                        N, d_nnz_perRow, &nnz));</span><br><span class="line"><span class="comment">// Print message</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"Total number of non-zero terms in dense matrix A = "</span> << nnz << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert dense format to csr format</span></span><br><span class="line"><span class="keyword">double</span> *d_csrValA = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> *d_rowPtrA = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> *d_colIdxA = <span class="number">0</span>;</span><br><span class="line">error_check(cudaMalloc(&d_csrValA, nnz * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">error_check(cudaMalloc(&d_rowPtrA, (N+<span class="number">1</span>) * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">error_check(cudaMalloc(&d_colIdxA, nnz * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">error_check(cusparseDdense2csr(cusHandle, N, N, </span><br><span class="line">                            descr_A, d_A, </span><br><span class="line">                            N, d_nnz_perRow,</span><br><span class="line">                            d_csrValA, d_rowPtrA, d_colIdxA));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>接著因為要使用 Generic API <code>cusparseSpMV</code> 計算 Matrix vector multiplication，因此要先建立 Descriptors：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Create Generic API descriptor</span></span><br><span class="line">cusparseSpMatDescr_t mat_A;</span><br><span class="line">error_check(cusparseCreateCsr(&mat_A, N, N, nnz,</span><br><span class="line">    d_rowPtrA, d_colIdxA, d_csrValA,</span><br><span class="line">    CUSPARSE_INDEX_32I, CUSPARSE_INDEX_32I,</span><br><span class="line">    CUSPARSE_INDEX_BASE_ZERO, CUDA_R_64F));</span><br><span class="line"></span><br><span class="line">cusparseDnVecDescr_t vec_b;</span><br><span class="line">error_check(cusparseCreateDnVec(&vec_b, N, d_b, CUDA_R_64F));</span><br><span class="line"></span><br><span class="line">cusparseDnVecDescr_t vec_c;</span><br><span class="line">error_check(cusparseCreateDnVec(&vec_c, N, d_c, CUDA_R_64F));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>接下來就是計算 Matrix vector multiplication，先呼叫 <code>cusparseSpMV_bufferSize</code> 計算計算所需的 Buffer 空間，接著呼叫 <code>cusparseSpMV</code> 完成計算：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">size_t</span> buf_size;</span><br><span class="line"><span class="keyword">double</span> *d_buf;</span><br><span class="line"><span class="keyword">double</span> alpha = <span class="number">1.0</span>;</span><br><span class="line"><span class="keyword">double</span> beta = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line">error_check(cusparseSpMV_bufferSize(cusHandle, </span><br><span class="line">                                    CUSPARSE_OPERATION_NON_TRANSPOSE,</span><br><span class="line">                                    &alpha, mat_A, vec_b, </span><br><span class="line">                                    &beta, vec_c, CUDA_R_64F, </span><br><span class="line">                                    CUSPARSE_CSRMV_ALG1, &buf_size));</span><br><span class="line"><span class="comment">// Allocate buffer</span></span><br><span class="line">cudaMalloc(&d_buf, buf_size);</span><br><span class="line"></span><br><span class="line">error_check(cusparseSpMV(cusHandle, </span><br><span class="line">                         CUSPARSE_OPERATION_NON_TRANSPOSE,</span><br><span class="line">                         &alpha, mat_A, vec_b, </span><br><span class="line">                         &beta, vec_c, CUDA_R_64F, </span><br><span class="line">                         CUSPARSE_CSRMV_ALG1, d_buf));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>最後將解答從 GPU memory 複製到 Host，並印出答案：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// device to host</span></span><br><span class="line"><span class="keyword">double</span> *c = <span class="keyword">new</span> <span class="keyword">double</span>[N]{};</span><br><span class="line">error_check(cublasGetVector(N, <span class="keyword">sizeof</span>(<span class="keyword">double</span>), d_c, <span class="number">1</span>, c, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// print answer</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"Answer: "</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">print_vector(c, N);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>程式最後將所有資源 Free 掉：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span>[] A;</span><br><span class="line"><span class="keyword">delete</span>[] b;</span><br><span class="line"><span class="keyword">delete</span>[] c;</span><br><span class="line">cudaFree(d_A);</span><br><span class="line">cudaFree(d_b);</span><br><span class="line">cudaFree(d_c);</span><br><span class="line">cudaFree(d_nnz_perRow);</span><br><span class="line">cudaFree(d_csrValA);</span><br><span class="line">cudaFree(d_rowPtrA);</span><br><span class="line">cudaFree(d_colIdxA);</span><br><span class="line">cudaFree(d_buf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free handles, descriptors, </span></span><br><span class="line">cusparseDestroyMatDescr(descr_A);</span><br><span class="line">cusparseDestroySpMat(mat_A);</span><br><span class="line">cusparseDestroyDnVec(vec_b);</span><br><span class="line">cusparseDestroyDnVec(vec_c);</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>成功的的話就會算出答案：<br><code>-33.48, 52.97, -47.62, 82.27, 3.97</code></p>
<h2 id="4-Implementation" class="heading-control"><a href="#4-Implementation" class="headerlink" title="4. Implementation"></a>4. Implementation<a class="heading-anchor" href="#4-Implementation" aria-hidden="true"></a></h2><p>這章節將解說如何使用 cuBLAS/cuSPARSE 實做 Incomplete-Cholesky preconditioned conjugate gradient。</p>
<h3 id="4-1-Frameworks" class="heading-control"><a href="#4-1-Frameworks" class="headerlink" title="4-1. Frameworks"></a>4-1. Frameworks<a class="heading-anchor" href="#4-1-Frameworks" aria-hidden="true"></a></h3><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><iostream></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><iomanip></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><fstream></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><cmath></span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><cuda_runtime.h></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><cublas_v2.h></span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string"><cusparse.h></span></span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> inputPath = <span class="string">"testcase/size1M/case_1M.in"</span>; <span class="comment">// Input file path</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> answerPath = <span class="string">"testcase/size1M/case_1M.out"</span>; <span class="comment">// Answer file path</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* TODO */</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>首先要處理的就是 Matrix 的讀寫。</p>
<ul>
<li>Input：使用 COO 格式<ul>
<li>$N$ (32-bit int)：表示 Matrix $A$ 大小為 $N\times N$，Vector $b$ 維度為 $N$</li>
<li>$\text{nz}$ (32-bit int)：表示矩陣 $A$ 具有的 Non-zero term 數量</li>
<li>3-tuple 有 $\text{nz}$ 個：<ul>
<li>$i$ (32-bit int)：表示 Row index (0-based)</li>
<li>$j$ (32-bit int)：表示 Column index (0-based)</li>
<li>$A_{ij}$ (64-bit float)：表示 Element $A_{ij}$ 的值</li>
</ul>
</li>
<li>$N$ 個 64-bit float 代表 Vector $b$</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/dt0WMYA.png" width="500px"></p>
<ul>
<li>Output：<ul>
<li>$N$ (32-bit int)：表示 Vector $x$ 的維度</li>
<li>$N$ 個 64-bit float 代表 Vector $x$</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/gmrsO1x.png" width="240px"></p>
<p>因此讀檔的部份：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Read testcase</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">read</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span> filePath,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> *pN, <span class="keyword">int</span> *pnz,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">double</span> **cooVal,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> **cooRowIdx, <span class="keyword">int</span> **cooColIdx,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">double</span> **b)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::ifstream <span class="title">in</span><span class="params">(filePath, <span class="built_in">std</span>::ios::binary)</span></span>;</span><br><span class="line"></span><br><span class="line">    in.read((<span class="keyword">char</span>*)pN, <span class="keyword">sizeof</span>(<span class="keyword">int</span>));  <span class="comment">// read N</span></span><br><span class="line">    in.read((<span class="keyword">char</span>*)pnz, <span class="keyword">sizeof</span>(<span class="keyword">int</span>)); <span class="comment">// read nz</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create array</span></span><br><span class="line">    *cooVal = <span class="keyword">new</span> <span class="keyword">double</span>[*pnz]{};</span><br><span class="line">    *cooRowIdx = <span class="keyword">new</span> <span class="keyword">int</span>[*pnz]{};</span><br><span class="line">    *cooColIdx = <span class="keyword">new</span> <span class="keyword">int</span>[*pnz]{};</span><br><span class="line">    *b = <span class="keyword">new</span> <span class="keyword">double</span>[*pN]{};</span><br><span class="line"></span><br><span class="line">    <span class="comment">// read each element Aij</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i < *pnz; ++i)</span><br><span class="line">    {</span><br><span class="line">        in.read((<span class="keyword">char</span>*)&(*cooRowIdx)[i], <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">        in.read((<span class="keyword">char</span>*)&(*cooColIdx)[i], <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">        in.read((<span class="keyword">char</span>*)&(*cooVal)[i], <span class="keyword">sizeof</span>(<span class="keyword">double</span>));</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// read b</span></span><br><span class="line">    in.read((<span class="keyword">char</span>*)(*b), <span class="keyword">sizeof</span>(<span class="keyword">double</span>)*(*pN));</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read answer</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">readAnswer</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span> filePath,</span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">int</span> *pN, <span class="keyword">double</span> **x)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::ifstream <span class="title">in</span><span class="params">(filePath, <span class="built_in">std</span>::ios::binary)</span></span>;</span><br><span class="line"></span><br><span class="line">    in.read((<span class="keyword">char</span>*)pN, <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line"></span><br><span class="line">    *x = <span class="keyword">new</span> <span class="keyword">double</span>[*pN]{};</span><br><span class="line"></span><br><span class="line">    in.read((<span class="keyword">char</span>*)(*x), <span class="keyword">sizeof</span>(<span class="keyword">double</span>)*(*pN));</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>讀檔這邊因為會需要更改傳入的參數，因此使用 Call by reference。接下來回到 <code>main</code>：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// in `main` function</span></span><br><span class="line"><span class="keyword">int</span> N;</span><br><span class="line"><span class="keyword">int</span> nz;</span><br><span class="line"><span class="keyword">double</span> *A;</span><br><span class="line"><span class="keyword">int</span> *rowIdxA;</span><br><span class="line"><span class="keyword">int</span> *colIdxA;</span><br><span class="line"><span class="keyword">double</span> *b;</span><br><span class="line">read(inputPath, &N, &nz, &A, &rowIdxA, &colIdxA, &b);</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> *ans_x;</span><br><span class="line">readAnswer(answerPath, &N, &ans_x);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print message</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"N = "</span> << N << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"nz = "</span> << nz << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>讀檔到這邊就完成了，由於讀進來的 Matrix $A$ 是 COO，需要轉換成 CSR，因此需要先初始化 cuBLAS/cuSPARSE：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Create handles</span></span><br><span class="line">cublasHandle_t cubHandle;</span><br><span class="line">cusparseHandle_t cusHandle;</span><br><span class="line"></span><br><span class="line">error_check(cublasCreate(&cubHandle));</span><br><span class="line">error_check(cusparseCreate(&cusHandle));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>接著分配 Matrix $A$ 在 GPU 上的 Memory，並複製一份過去：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Allocate GPU memory & copy matrix/vector to device</span></span><br><span class="line"><span class="keyword">double</span> *d_A;</span><br><span class="line"><span class="keyword">int</span> *d_rowIdxA; <span class="comment">// COO</span></span><br><span class="line"><span class="keyword">int</span> *d_rowPtrA; <span class="comment">// CSR</span></span><br><span class="line"><span class="keyword">int</span> *d_colIdxA;</span><br><span class="line"><span class="keyword">double</span> *d_b;</span><br><span class="line"></span><br><span class="line">error_check(cudaMalloc(&d_A, nz * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">error_check(cudaMalloc(&d_rowIdxA, nz * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">error_check(cudaMalloc(&d_rowPtrA, (N + <span class="number">1</span>) * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));  <span class="comment">// (N+1) !!!!</span></span><br><span class="line">error_check(cudaMalloc(&d_colIdxA, nz * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">error_check(cudaMalloc(&d_b, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line"></span><br><span class="line">error_check(cudaMemcpy(d_A, A, nz * <span class="keyword">sizeof</span>(<span class="keyword">double</span>), cudaMemcpyHostToDevice));</span><br><span class="line">error_check(cudaMemcpy(d_rowIdxA, rowIdxA, nz * <span class="keyword">sizeof</span>(<span class="keyword">int</span>), cudaMemcpyHostToDevice));</span><br><span class="line">error_check(cudaMemcpy(d_colIdxA, colIdxA, nz * <span class="keyword">sizeof</span>(<span class="keyword">int</span>), cudaMemcpyHostToDevice));</span><br><span class="line">error_check(cudaMemcpy(d_b, b, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>), cudaMemcpyHostToDevice));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊需要注意 CSR 格式是針對 RowIdx 做壓縮，壓縮後的大小為 $N+1$，在 Document 上有寫。接著就是 Call <a href="https://docs.nvidia.com/cuda/cusparse/index.html#coo2csr"><code>cusparseXcoo2csr</code></a> 做轉換：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Convert matrix A from COO format to CSR format</span></span><br><span class="line">error_check(cusparseXcoo2csr(cusHandle, d_rowIdxA, nz, N,</span><br><span class="line">                    d_rowPtrA, CUSPARSE_INDEX_BASE_ZERO));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>接著就是建立 ICCGsolver 跟 Call solve，內容的部份會在 4-2 節實作：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Create conjugate gradient solver</span></span><br><span class="line"><span class="keyword">int</span> max_iter = <span class="number">1000</span>;</span><br><span class="line"><span class="keyword">double</span> tolerance = <span class="number">1e-12</span>;</span><br><span class="line"></span><br><span class="line"><span class="function">ICCGsolver <span class="title">solver</span><span class="params">(max_iter, tolerance, cubHandle, cusHandle)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print message</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"Solving linear system..."</span> << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">bool</span> res = solver.solve(N, nz, d_A, d_rowPtrA, d_colIdxA, d_b);</span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << (res ? <span class="string">"Converged!"</span>: <span class="string">"Failed to converge"</span>) << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊我讓 <code>solver.solve</code> 回傳 <code>true</code> 如果 <code>solver</code> 成功在 <code>max_iter</code> Iteration 內收斂 (小於 <code>tolerance</code>) 的話，否則回傳 <code>false</code>。接下來就是將解答從 GPU 摳回 Host memory，並驗證結果：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> *x = <span class="keyword">new</span> <span class="keyword">double</span>[N] {};</span><br><span class="line">error_check(cudaMemcpy(x, solver.x_ptr(), N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>), cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> tol = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i < N; ++i)</span><br><span class="line">{</span><br><span class="line">    tol += x[i] - ans_x[i];</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// print message</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"Solved in "</span> << solver.iter_count() << <span class="string">" iterations, final norm(r) = "</span></span><br><span class="line">        << <span class="built_in">std</span>::scientific << solver.err() << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> << <span class="string">"Total error (compared with ans_x): "</span> << tol << <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>最後要記得把 Memory 都 Free 掉：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Free Host memory</span></span><br><span class="line"><span class="keyword">delete</span>[] A;</span><br><span class="line"><span class="keyword">delete</span>[] rowIdxA;</span><br><span class="line"><span class="keyword">delete</span>[] colIdxA;</span><br><span class="line"><span class="keyword">delete</span>[] b;</span><br><span class="line"><span class="keyword">delete</span>[] ans_x;</span><br><span class="line"><span class="keyword">delete</span>[] x;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free Device memory</span></span><br><span class="line">cudaFree(d_A);</span><br><span class="line">cudaFree(d_rowIdxA);</span><br><span class="line">cudaFree(d_rowPtrA);</span><br><span class="line">cudaFree(d_colIdxA);</span><br><span class="line">cudaFree(d_b);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free handles</span></span><br><span class="line">cublasDestroy(cubHandle);</span><br><span class="line">cusparseDestroy(cusHandle);</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>到這邊 <code>main</code> 就完成了，下一節就是實作 <code>ICCGsolver</code>。</p>
<h3 id="4-2-Main-Algorithm" class="heading-control"><a href="#4-2-Main-Algorithm" class="headerlink" title="4-2. Main Algorithm"></a>4-2. Main Algorithm<a class="heading-anchor" href="#4-2-Main-Algorithm" aria-hidden="true"></a></h3><p>先定義 <code>class ICCGsolver</code>：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ICCGsolver</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    cublasHandle_t cubHandle;</span><br><span class="line">    cusparseHandle_t cusHandle;</span><br><span class="line">    </span><br><span class="line">    cusparseMatDescr_t descr_A;</span><br><span class="line">    cusparseMatDescr_t descr_L;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// host data</span></span><br><span class="line">    <span class="keyword">int</span> N = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> nz = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> max_iter;</span><br><span class="line">    <span class="keyword">int</span> k;  <span class="comment">// k iteration</span></span><br><span class="line">    <span class="keyword">double</span> tolerance;</span><br><span class="line">    <span class="keyword">double</span> alpha;</span><br><span class="line">    <span class="keyword">double</span> beta;</span><br><span class="line">    <span class="keyword">double</span> rTr;</span><br><span class="line">    <span class="keyword">double</span> pTq;</span><br><span class="line">    <span class="keyword">double</span> rho;    <span class="comment">//rho{k}</span></span><br><span class="line">    <span class="keyword">double</span> <span class="keyword">rho_t</span>;  <span class="comment">//rho{k-1}</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span> one = <span class="number">1.0</span>;  <span class="comment">// constant</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span> zero = <span class="number">0.0</span>;   <span class="comment">// constant</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// device data</span></span><br><span class="line">    <span class="keyword">double</span> *d_ic = <span class="literal">nullptr</span>;  <span class="comment">// Factorized L</span></span><br><span class="line">    <span class="keyword">double</span> *d_x = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_y = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_z = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_r = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_rt = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_xt = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_q = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_p = <span class="literal">nullptr</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">bool</span> release_cusHandle = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">bool</span> release_cubHandle = <span class="literal">false</span>;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    ICCGsolver(<span class="keyword">int</span> max_iter = <span class="number">1000</span>, <span class="keyword">double</span> tol = <span class="number">1e-12</span>,</span><br><span class="line">        cublasHandle_t cub_handle = <span class="literal">NULL</span>,</span><br><span class="line">        cusparseHandle_t cus_handle = <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    ~ICCGsolver();</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">solve</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">int</span> nz,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">double</span> *d_A, <span class="keyword">int</span> *d_rowIdx, <span class="keyword">int</span> *d_colIdx,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">double</span> *d_b)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">double</span> *<span class="title">x_ptr</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">err</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">iter_count</span><span class="params">()</span></span>;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">allocate_nz_memory</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">allocate_N_memory</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">free_nz_memory</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">free_N_memory</span><span class="params">()</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">check_and_resize</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">int</span> nz)</span></span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><br>只要有前綴 <code>d_*</code> 表示他是指向 GPU device 的 Pointer。<p></p>
<ul>
<li>首先是 Line <code>4</code>、<code>5</code> 宣告 Handle，接著是因為我們會用到兩個 Matrix 分別為 $A$ 與 Factorized matrix $L$，因此宣告兩個 Descriptor。</li>
<li><code>11</code>~<code>23</code> 是計算時會用到的變數。</li>
<li><code>25</code>~<code>34</code> 則是 GPU 計算實用到的變數。</li>
<li><code>36</code>~<code>37</code> 用來紀錄 Handler 需要需要自動釋放，如果 Handler 是使用者宣告 <code>ICCGsolver</code> 時傳入的則使用者要自行釋放，如果不是則自動釋放。</li>
<li><code>40</code>~<code>42</code> 是 Constructor，第一個參數 <code>max_iter</code> 表示 Iteration 的上限，<code>tol</code> 表示 Tolerance，<code>cub_handle</code> 與 <code>cus_handle</code> 可傳入可不傳入。</li>
<li><code>44</code> 是 Destructor，要 Free 掉所有 GPU memory。</li>
<li><code>46</code>~<code>48</code> 是解方程的 API，<code>N</code>、<code>nz</code> 分別表示 Matrix 大小以及 Non-zero term 數量，<code>d_A</code>、<code>d_rowIdx</code>、<code>d_colIdx</code> 為 CSR format 的 Matrix $A$，<code>d_b</code> 是 Vector $b$。</li>
<li><code>50</code> 用來取得 <code>d_x</code> Pointer，可以利用這個 Pointer 將答案複製出來。</li>
<li><code>51</code> 用來取得誤差值 $|r_k|$。</li>
<li><code>52</code> 用來取得 Iteration count $k$。</li>
<li><code>55</code>~<code>60</code> 用來分配、釋放 GPU memory。</li>
</ul>
<p>首先是完成 Constructor 與 Destructor：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line">ICCGsolver::ICCGsolver(<span class="keyword">int</span> max_iter, <span class="keyword">double</span> tol,</span><br><span class="line">    cublasHandle_t cub_handle, cusparseHandle_t cus_handle) </span><br><span class="line">    : max_iter(max_iter), tolerance(tol), </span><br><span class="line">      cubHandle(cub_handle), cusHandle(cus_handle)</span><br><span class="line">{</span><br><span class="line">    <span class="comment">// create cuBLAS handle</span></span><br><span class="line">    <span class="keyword">if</span> (cubHandle == <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        error_check(cublasCreate(&cubHandle));</span><br><span class="line">        release_cubHandle = <span class="literal">true</span>;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// create cuSPARSE handle</span></span><br><span class="line">    <span class="keyword">if</span> (cusHandle == <span class="literal">NULL</span>)</span><br><span class="line">    {</span><br><span class="line">        error_check(cusparseCreate(&cusHandle));</span><br><span class="line">        release_cusHandle = <span class="literal">true</span>;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create descriptor for matrix A</span></span><br><span class="line">    error_check(cusparseCreateMatDescr(&descr_A));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initialize properties of matrix A</span></span><br><span class="line">    error_check(cusparseSetMatType(descr_A, CUSPARSE_MATRIX_TYPE_GENERAL));</span><br><span class="line">    error_check(cusparseSetMatFillMode(descr_A, CUSPARSE_FILL_MODE_LOWER));</span><br><span class="line">    error_check(cusparseSetMatDiagType(descr_A, CUSPARSE_DIAG_TYPE_NON_UNIT));</span><br><span class="line">    error_check(cusparseSetMatIndexBase(descr_A, CUSPARSE_INDEX_BASE_ZERO));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create descriptor for matrix L</span></span><br><span class="line">    error_check(cusparseCreateMatDescr(&descr_L));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initialize properties of matrix L</span></span><br><span class="line">    error_check(cusparseSetMatType(descr_L, CUSPARSE_MATRIX_TYPE_GENERAL));</span><br><span class="line">    error_check(cusparseSetMatFillMode(descr_L, CUSPARSE_FILL_MODE_LOWER));</span><br><span class="line">    error_check(cusparseSetMatIndexBase(descr_L, CUSPARSE_INDEX_BASE_ZERO));</span><br><span class="line">    error_check(cusparseSetMatDiagType(descr_L, CUSPARSE_DIAG_TYPE_NON_UNIT));</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>Constructor 做的事情很簡單，初始化傳入得參數後，建立 Matrix $A$ 與 $L$ 的 Descriptor，這邊需要注意的是在 CUDA 11.0 <code>cusparseMatDescr_t</code> 已經有快要 <em>Deprecate</em> 的傾向，因此這邊設定 Properties 也沒什麼選項可選，首先是 <code>cusparseSetMatType</code>，目前除了 <code>CUSPARSE_MATRIX_TYPE_GENERAL</code> 選項以外其他都 <em>Deprecate</em> 因此直接勇敢的設成 <code>CUSPARSE_MATRIX_TYPE_GENERAL</code> 就可以了，剩下就是除了 <code>cusparseSetMatIndexBase</code> 需要照實填寫外，其他都會被忽略，並不是很重要。</p>
<p>接下來是 Destructor：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line">ICCGsolver::~ICCGsolver()</span><br><span class="line">{</span><br><span class="line">    <span class="comment">// free data</span></span><br><span class="line">    free_nz_memory();</span><br><span class="line">    free_N_memory();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// release descriptor</span></span><br><span class="line">    cusparseDestroyMatDescr(descr_A);</span><br><span class="line">    cusparseDestroyMatDescr(descr_L);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// release handles</span></span><br><span class="line">    <span class="keyword">if</span> (release_cubHandle)</span><br><span class="line">    {</span><br><span class="line">        cublasDestroy(cubHandle);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (release_cusHandle)</span><br><span class="line">    {</span><br><span class="line">        cusparseDestroy(cusHandle);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>Destructor 做的事情更簡單，就是將 GPU memory 都 Free 掉，Descriptor 也 Free 掉，Handle 看是不是使用者傳入的，如果不是，就一起毀掉，如果是就不用毀掉。</p>
<p>接下來是除了 <code>solve</code> 以外的 Function：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">double</span> *<span class="title">ICCGsolver::x_ptr</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">return</span> d_x;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">ICCGsolver::err</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">return</span> rTr;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">ICCGsolver::iter_count</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">return</span> k;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p><code>x_ptr()</code> 用來取得答案的 GPU Pointer。<code>err()</code> 用來取得誤差值 $|r_k|=r^Tr$。<code>iter_count()</code>用來取得最後一次 <code>solve</code> 的 Iteration count。接下來 Memory 相關：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ICCGsolver::check_and_resize</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">int</span> nz)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="comment">// allocate N</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>->N < N)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">this</span>->N = N;</span><br><span class="line">        free_N_memory();</span><br><span class="line">        allocate_N_memory();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>->nz < nz)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">this</span>->nz = nz;</span><br><span class="line">        free_nz_memory();</span><br><span class="line">        allocate_nz_memory();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p><code>check_and_resize</code> 用來檢查，呼叫 <code>solve</code> 時新傳進來的 $N$ 與 $\text{nz}$ 有沒有比目前分配的還大，如果比較大，就要 Free 掉並重新分配一個更大的空間來做計算。</p>
<p>因此釋放與分配空間的 Function：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ICCGsolver::allocate_N_memory</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    error_check(cudaMalloc(&d_x, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&d_y, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&d_z, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&d_r, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&d_rt, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&d_xt, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&d_q, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&d_p, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ICCGsolver::allocate_nz_memory</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    error_check(cudaMalloc(&d_ic, nz * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ICCGsolver::free_N_memory</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    cudaFree(d_x);</span><br><span class="line">    cudaFree(d_y);</span><br><span class="line">    cudaFree(d_z);</span><br><span class="line">    cudaFree(d_r);</span><br><span class="line">    cudaFree(d_rt);</span><br><span class="line">    cudaFree(d_xt);</span><br><span class="line">    cudaFree(d_q);</span><br><span class="line">    cudaFree(d_p);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ICCGsolver::free_nz_memory</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    cudaFree(d_ic);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>到這邊，週邊的 Function 全部完成了，總算可以進入正題：Preconditioned conjugate gradient。建議這邊實作的時候可以搭配 Algorithm 1。</p>
<p>Recall Algorithm 1：<br><img src="https://i.imgur.com/hL1FiZD.png" alt></p>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ICCGsolver::solve</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">int</span> nz,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> *d_A, <span class="keyword">int</span> *d_rowIdx, <span class="keyword">int</span> *d_colIdx,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> *d_b, <span class="keyword">double</span> *d_guess)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    check_and_resize(N, nz);</span><br><span class="line">     </span><br><span class="line">    <span class="comment">// --- 1. Create cuSPARSE generic API objects ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">     </span><br><span class="line">    <span class="comment">// --- 2. Perform incomplete cholesky factorization ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// --- 3. Prepare for performing conjugate gradient ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// --- 4. Perform conjugate gradient ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// --- 5. Finalize ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rTr < tolrance; <span class="comment">// return true if converged</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>首先一開始需要先檢查、分配 GPU 空間，因此 Call <code>check_and_resize</code>，接著我將分成 5 個部份分別實作。</p>
<p>先從第二部份開始，這邊會對應到 Algorithm 1 的 Line <code>4</code>：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// --- Perform incomplete cholesky factorization ---</span></span><br><span class="line">csric02Info_t icinfo_A;</span><br><span class="line"><span class="keyword">size_t</span> buf_size = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">size_t</span> u_temp_buf_size = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">size_t</span> u_temp_buf_size2 = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> i_temp_buf_size = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">void</span> *d_buf = <span class="literal">NULL</span>;</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>cuSPARSE 進行 Incomplete cholesky factorization 分成幾個步驟：</p>
<ul>
<li>Call <code>cusparseCreateCsric02Info</code> 建立 <code>csric02Info_t</code> object</li>
<li>Call <code>cusparseDcsric02_bufferSize</code> 讓 cuSPARSE 估計計算 Factorization 所需的 Buffer 大小，然後分配 Buffer <code>d_buf</code> 空間</li>
<li>Call <code>cusparseDcsric02_analysis</code> (analysis phase) 讓 cuSPARSE 分析 Matrix $A$ 的 Sparsity。</li>
<li>Call <code>cusparseDcsric02</code> (solve phase) 進行 Factorization</li>
</ul>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Create info object for incomplete-cholesky factorization</span></span><br><span class="line">error_check(cusparseCreateCsric02Info(&icinfo_A));</span><br><span class="line"><span class="comment">// Compute buffer size in computing ic factorization</span></span><br><span class="line">error_check(cusparseDcsric02_bufferSize(cusHandle, N, nz, </span><br><span class="line">    descr_A, d_A, d_rowIdx, d_colIdx, icinfo_A, &i_temp_buf_size));</span><br><span class="line">buf_size = i_temp_buf_size;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create buffer</span></span><br><span class="line">error_check(cudaMalloc(&d_buf, buf_size));</span><br><span class="line"><span class="comment">// Copy A</span></span><br><span class="line">error_check(cudaMemcpy(d_ic, d_A, nz * <span class="keyword">sizeof</span>(<span class="keyword">double</span>), cudaMemcpyDeviceToDevice));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Perform incomplete-choleskey factorization: analysis phase</span></span><br><span class="line">error_check(cusparseDcsric02_analysis(cusHandle, N, nz,</span><br><span class="line">    descr_A, d_ic, d_rowIdx, d_colIdx, icinfo_A, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Perform incomplete-choleskey factorization: solve phase</span></span><br><span class="line">error_check(cusparseDcsric02(cusHandle, N, nz,</span><br><span class="line">    descr_A, d_ic, d_rowIdx, d_colIdx, icinfo_A, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf));</span><br></pre></td></tr></tbody></table></figure>
<p>這邊我將 <code>d_A</code> 複製到 <code>d_ic</code> 是因為，cuSPARSE 在進行 Factorization 的時候會直接將結果覆寫到 Input array 上，因此這邊將 <code>d_A</code> 複製一份到 <code>d_ic</code>，將 Factorized 的 Lower triangular matrix 存到 <code>d_ic</code> 裡面。</p>
<p>另外，根據 Document 的說法，<code>CUSPARSE_SOLVE_POLICY_USE_LEVEL</code> 有時會對計算進行稍微的優化，有時不會，因此這邊要設定 <code>NO_LEVEL</code> 或 <code>USE_LEVEL</code> 都可以，但是 Analysis phase 與 Solve phase 的設定一定要一致。</p>
<p>接下來第三部份是估算 Conjugate gradient 所需的 Buffer 空間：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// --- Prepare for performing conjugate gradient ---</span></span><br><span class="line"><span class="comment">// Create info object for factorized matrix L, LT</span></span><br><span class="line">csrsv2Info_t info_L, info_U;</span><br><span class="line">error_check(cusparseCreateCsrsv2Info(&info_L));</span><br><span class="line">error_check(cusparseCreateCsrsv2Info(&info_U));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>在 Algorithm 1 Line <code>9</code> 跟 <code>10</code> 有計算到 $L$ 與 $L^T$ 的 Inverse，但實際上 cuSPARSE 並沒有求 Inverse matrix 的 API，因此可以將這兩行看做求解兩個 Triangular sparse linear system：$Ly=r_k$、求 $y$ 與 $L^Tz_k=y$ 求 $z_k$。而求解 Triancular sparse linear system 可以使用 <code>cusparseDcsrsv2</code> 這個 API。因此首先我們需要建立兩個 <code>csrsv2Info_t</code>：</p>
<ul>
<li>利用 <code>info_L</code> 代表 $L$ (Lower triangular matrix)</li>
<li>利用 <code>info_U</code> 代表 $L^T$ (Upper triangular matrix)</li>
</ul>
<p>接下來就是分別估算解 Triangular sparse linear system 所需的 Buffer 大小：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Compute buffer size in solving linear system</span></span><br><span class="line">error_check(cusparseDcsrsv2_bufferSize(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE, </span><br><span class="line">    N, nz, descr_L, d_ic, d_rowIdx, d_colIdx, info_L, &i_temp_buf_size));</span><br><span class="line"></span><br><span class="line">u_temp_buf_size = i_temp_buf_size;</span><br><span class="line"></span><br><span class="line">error_check(cusparseDcsrsv2_bufferSize(cusHandle, CUSPARSE_OPERATION_TRANSPOSE,</span><br><span class="line">    N, nz, descr_L, d_ic, d_rowIdx, d_colIdx, info_U, &i_temp_buf_size));</span><br><span class="line"></span><br><span class="line"><span class="comment">// check whether need more buffer</span></span><br><span class="line"><span class="keyword">if</span> (i_temp_buf_size > u_temp_buf_size)</span><br><span class="line">{</span><br><span class="line">    u_temp_buf_size = i_temp_buf_size;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>除此之外，Conjugate gradient 有發生一次 Sparse Matrix-dense vector multiplication 在 Algorithm 1 Line <code>17</code> $q\gets Ap_k$，這個需要使用 cuSPARSE 的 Generic API <code>cusparseSpMV</code>，因此先回到第一部份宣告 Generic API 所需的 Objects：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// --- 1. Create cuSPARSE generic API objects ---</span></span><br><span class="line">cusparseSpMatDescr_t smat_A;</span><br><span class="line">error_check(cusparseCreateCsr(&smat_A, N, N, nz, d_rowIdx, d_colIdx, d_A, CUSPARSE_INDEX_32I,</span><br><span class="line">    CUSPARSE_INDEX_32I, CUSPARSE_INDEX_BASE_ZERO, CUDA_R_64F));</span><br><span class="line"></span><br><span class="line">cusparseDnVecDescr_t dvec_p;</span><br><span class="line">error_check(cusparseCreateDnVec(&dvec_p, N, d_p, CUDA_R_64F));</span><br><span class="line"></span><br><span class="line">cusparseDnVecDescr_t dvec_q;</span><br><span class="line">error_check(cusparseCreateDnVec(&dvec_q, N, d_q, CUDA_R_64F));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>建立一個 CSR Matrix $A$ 的 Descriptor，建立 Dense vector $p_k$ 與 $q$ 的 Descriptor。再回到第三部份完成剩下的部份，估算 Sparse Matrix-dense vector multiplication 所需的 Buffer 數量：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Compute buffer size for matrix-vector multiplication</span></span><br><span class="line">error_check(cusparseSpMV_bufferSize(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE, &one, smat_A,</span><br><span class="line">    dvec_p, &zero, dvec_q, CUDA_R_64F, CUSPARSE_CSRMV_ALG1, &u_temp_buf_size2));</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (u_temp_buf_size2 > u_temp_buf_size)</span><br><span class="line">{</span><br><span class="line">    u_temp_buf_size = u_temp_buf_size2;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>然後分配空間：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// re-allocate buffer</span></span><br><span class="line"><span class="keyword">if</span> (u_temp_buf_size > buf_size)</span><br><span class="line">{</span><br><span class="line">    buf_size = u_temp_buf_size;</span><br><span class="line">    cudaFree(d_buf);</span><br><span class="line">    error_check(cudaMalloc(&d_buf, buf_size));</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊需要注意是，通常 Natrix-vector multiplication 會需要比 Incomplete cholesky factorization 與 Solving lienar system 還要更多的 Buffer。如果 Buffer 空間不夠時，cuSPARSE 會回傳 <code>CUSPARSE_STATUS_INTERNAL_ERROR</code>。</p>
<p>分配完 Buffer 後，就可以先對 Triangular sparse linear system 進行 Analysis phase：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// analysis phase</span></span><br><span class="line">error_check(cusparseDcsrsv2_analysis(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE,</span><br><span class="line">    N, nz, descr_L, d_ic, d_rowIdx, d_colIdx, info_L, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf));</span><br><span class="line">error_check(cusparseDcsrsv2_analysis(cusHandle, CUSPARSE_OPERATION_TRANSPOSE,</span><br><span class="line">    N, nz, descr_L, d_ic, d_rowIdx, d_colIdx, info_U, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊需要注意的是 <code>d_ic</code> 是 Lower triangular matrix，因此在分析 $L$ (Lower triangular matrix) 時要設定 Operation 為 <code>CUSPARSE_OPERATION_NON_TRANSPOSE</code>，但是在分析 $L^T$ (Upper triangular matrix) 時則要設為 <strong><code>CUSPARSE_OPERATION_TRANSPOSE</code></strong>，這個非常容易被忽略。</p>
<p>所以到目前為止，計算 Conjugate gradient 的事前準備都已經完成了：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ICCGsolver::solve</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">int</span> nz,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> *d_A, <span class="keyword">int</span> *d_rowIdx, <span class="keyword">int</span> *d_colIdx,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> *d_b, <span class="keyword">double</span> *d_guess)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    check_and_resize(N, nz);</span><br><span class="line">     </span><br><span class="line">    <span class="comment">// --- 1. Create cuSPARSE generic API objects ---</span></span><br><span class="line">    <span class="comment">// DONE</span></span><br><span class="line">     </span><br><span class="line">    <span class="comment">// --- 2. Perform incomplete cholesky factorization ---</span></span><br><span class="line">    <span class="comment">// DONE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// --- 3. Prepare for performing conjugate gradient ---</span></span><br><span class="line">    <span class="comment">// DONE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// --- 4. Perform conjugate gradient ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// --- 5. Finalize ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rTr < tolrance; <span class="comment">// return true if converged</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>接下來第四部份就是按照 Algorithm 1 一行一行刻：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// x = 0</span></span><br><span class="line">error_check(cudaMemset(d_x, <span class="number">0</span>, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line"><span class="comment">// r0 = b  (since x == 0, b - A*x = b)</span></span><br><span class="line">error_check(cudaMemcpy(d_r, d_b, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>), cudaMemcpyDeviceToDevice));</span><br></pre></td></tr></tbody></table></figure><br>Line <code>2</code>、<code>3</code> 初始化 $x_0$ 與 $r_0$。接下來就是 For 迴圈的內容：<br><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(k = <span class="number">0</span>; k < max_iter; ++k)</span><br><span class="line">{</span><br><span class="line">    <span class="comment">//TODO</span></span><br><span class="line">}<span class="comment">//EndFor</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>Algorithm 1 Line <code>6</code>~<code>8</code>，計算 $|r_k|$，由於 r_k$ 是 Dense vector，因此可以使用 cuBLAS 的 API <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-nrm2"><code>cublasDnrm2</code></a> 計算：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// if ||rk|| < tolerance</span></span><br><span class="line">error_check(cublasDnrm2(cubHandle, N, d_r, <span class="number">1</span>, &rTr));</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (rTr < tolerance)</span><br><span class="line">{</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><br>如果誤差 <code>rTr</code> 已經比設定的 <code>tolerance</code> 還小就結束 Conjugate gradient。<p></p>
<p>Algorithm 1 Line <code>9</code>、<code>10</code> 分別解出兩個 Triangular sparse linear system：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// Solve L*y = rk, find y</span></span><br><span class="line">error_check(cusparseDcsrsv2_solve(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE, N, nz, &one,</span><br><span class="line">    descr_L, d_ic, d_rowIdx, d_colIdx, info_L, d_r, d_y, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Solve L^T*zk = y, find zk</span></span><br><span class="line">error_check(cusparseDcsrsv2_solve(cusHandle, CUSPARSE_OPERATION_TRANSPOSE, N, nz, &one,</span><br><span class="line">    descr_L, d_ic, d_rowIdx, d_colIdx, info_U, d_y, d_z, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊同樣 Solve phase 與 Analysis phase 的參數 <code>CUSPARSE_SOLVE_POLICY_USE_LEVEL</code> 必須一致。</p>
<p>Algorithm 1 Line <code>11</code> 計算 $\rho_k=r^T_kz_k$，這邊是兩個 Dense vector 的內積，因此可以使用 cuBLAS 的 <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-dot"><code>cublasDdot</code></a>：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// rho_t = r{k-1} * z{k-1}</span></span><br><span class="line"><span class="keyword">rho_t</span> = rho;  </span><br><span class="line"><span class="comment">// rho = rk * zk</span></span><br><span class="line">error_check(cublasDdot(cubHandle, N, d_r, <span class="number">1</span>, d_z, <span class="number">1</span>, &rho));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>這邊還有一個重點就是，在計算 $\beta$ 的時候會需要 $\rho_{k-1}=r_{k-1}^Tz_{k-1}$，因此可以用一個變數 <code>rho_t</code> 把他暫存起來。</p>
<p>Algorithm 1 Line <code>12</code>~<code>17</code>，當 $k=0$ 時直接 Assign $p_k\gets z_k$，可以使用 <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-copy"><code>cublasDcopy</code></a>；否則，先計算 $\beta$，再更新 $p_k$：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (k == <span class="number">0</span>)</span><br><span class="line">{</span><br><span class="line">    <span class="comment">// pk = zk</span></span><br><span class="line">    error_check(cublasDcopy(cubHandle, N, d_z, <span class="number">1</span>, d_p, <span class="number">1</span>));</span><br><span class="line">}</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">{</span><br><span class="line">    <span class="comment">// beta = (rk*zk) / (r{k-1}*z{k-1})</span></span><br><span class="line">    beta = rho / <span class="keyword">rho_t</span>;</span><br><span class="line">    <span class="comment">// pk = zk + beta*p{k-1}</span></span><br><span class="line">    error_check(cublasDscal(cubHandle, N, &beta, d_p, <span class="number">1</span>));</span><br><span class="line">    error_check(cublasDaxpy(cubHandle, N, &one, d_z, <span class="number">1</span>, d_p, <span class="number">1</span>));</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>Line <code>8</code> 計算 $\beta$ 後，先 Call <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-scal"><code>cublasDscal</code></a> 對 $p_{k-1}$ 進行 Scale $p_{k-1}\gets \beta p_{k-1}$， 在 Call <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-axpy"><code>cublasDaxpy</code></a> 計算 $p_k\gets 1\cdot z_k+p_{k-1}$。</p>
<p>Algorithm 1 Line <code>18</code> 計算 Sparse matrix-dense vector multiplication，這邊需要使用 cuSPARSE 的 Generic API <a href="https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-spmv"><code>cusparseSpMV</code></a>：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// q = A*pk</span></span><br><span class="line">error_check(cusparseSpMV(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE, &one, smat_A,</span><br><span class="line">    dvec_p, &zero, dvec_q, CUDA_R_64F, CUSPARSE_MV_ALG_DEFAULT, d_buf));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>Algorithm 1 Line <code>19</code> 計算 $\alpha$，但在計算 $\alpha$ 前要先算出 $p_k^Tq$：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// alpha = (rk*zk) / (pk*q)</span></span><br><span class="line">error_check(cublasDdot(cubHandle, N, d_p, <span class="number">1</span>, d_q, <span class="number">1</span>, &pTq));</span><br><span class="line">alpha = rho / pTq;</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>因此首先用 <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-dot"><code>cublasDdot</code></a> 計算 $p_k^Tq$，再計算出 $\alpha$。</p>
<p>Algorithm 1 Line <code>20</code> 更新 $x_{k+1}$ 可以使用 <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-axpy"><code>cublasDaxpy</code></a> 計算 $x_{k+1}\gets \alpha p_k + x_k$<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// x{k+1} = xk + alpha*pk</span></span><br><span class="line">error_check(cublasDaxpy(cubHandle, N, &alpha, d_p, <span class="number">1</span>, d_x, <span class="number">1</span>));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>最後更新 Algorithm 1 Line <code>21</code> 更新 Residual $r_{k+1}$。這邊減法可以看作 $r_{k+1}\gets (-\alpha)q + r_k$，因此同樣使用 <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-axpy"><code>cublasDaxpy</code></a> 計算：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// r{k+1} = rk - alpha*q </span></span><br><span class="line"><span class="keyword">double</span> n_alpha = -alpha;</span><br><span class="line">error_check(cublasDaxpy(cubHandle, N, &n_alpha, d_q, <span class="number">1</span>, d_r, <span class="number">1</span>));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>到這邊第四部份就完成了。最後第五部份就是將 Buffer 之類的全部 Free 掉：<br></p><figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// free buffer</span></span><br><span class="line">cudaFree(d_buf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// free objects</span></span><br><span class="line">error_check(cusparseDestroySpMat(smat_A));</span><br><span class="line">error_check(cusparseDestroyDnVec(dvec_p));</span><br><span class="line">error_check(cusparseDestroyDnVec(dvec_q));</span><br><span class="line">error_check(cusparseDestroyDnVec(dvec_x));</span><br><span class="line">error_check(cusparseDestroyCsric02Info(icinfo_A));</span><br><span class="line">error_check(cusparseDestroyCsrsv2Info(info_L));</span><br><span class="line">error_check(cusparseDestroyCsrsv2Info(info_U));</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>就完成了 Preconditioned conjugate gradient。</p>
<p>試著執行了一下，結果 $N=1,000,000$ 的 Sparse linear system 只花了 $368$ 個 Iteration 誤差就收斂到小於 $10^{-12}$。</p>
<p><img src="https://i.imgur.com/iX7VgCC.png" alt></p>
<p>完整的 Code 可以在 Github 找到：<a href="https://github.com/Ending2015a/ICCG0_CUDA">Ending2015a/ICCG0_CUDA - github</a></p>
</body></html>]]></content>
      <categories>
        <category>Linear Algebra</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Programming</tag>
        <tag>教學</tag>
        <tag>Linear Algebra</tag>
        <tag>Numerical Analysis</tag>
        <tag>CUDA</tag>
        <tag>cuBLAS</tag>
        <tag>cuSPARSE</tag>
        <tag>燃燒吧 GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>[Note] NIPS2020 Reinforcement Learning - 163 Papers</title>
    <url>/Ending2015a/62651/</url>
    <content><![CDATA[<p><img src="https://i.imgur.com/CuyVsVZ.png" width="50%"></p>
<a id="more"></a>
<p><strong>Generalised Bayesian Filtering via Sequential Monte Carlo</strong></p>
<blockquote>
<p>ID 16887<br>Ayman Boustati (University of Warwick) · Omer Deniz Akyildiz (University of Warwick) · Theodoros Damoulas (University of Warwick &amp; The Alan Turing Institute) · Adam Johansen (University of Warwick)</p>
</blockquote>
<p>We introduce a framework for inference in general state-space hidden Markov models (HMMs) under likelihood misspecification. In particular, we leverage the loss-theoretic perspective of Generalized Bayesian Inference (GBI) to define generalised filtering recursions in HMMs, that can tackle the problem of inference under model misspecification. In doing so, we arrive at principled procedures for robust inference against observation contamination by utilising the $\beta$-divergence. Operationalising the proposed framework is made possible via sequential Monte Carlo methods (SMC), where the standard particle methods, and their associated convergence results, are readily adapted to the new setting. We demonstrate our approach to object tracking and Gaussian process regression problems, and observe improved performance over standard filtering algorithms.</p>
<p><strong>Softmax Deep Double Deterministic Policy Gradients</strong></p>
<blockquote>
<p>ID 16898<br>Ling Pan (Tsinghua University) · Qingpeng Cai (Alibaba Group) · Longbo Huang (IIIS, Tsinghua Univeristy)</p>
</blockquote>
<p>A widely-used actor-critic reinforcement learning algorithm for continuous control, Deep Deterministic Policy Gradients (DDPG), suffers from the overestimation problem, which can negatively affect the performance. Although the state-of-the-art Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm mitigates the overestimation issue, it can lead to a large underestimation bias. In this paper, we propose to use the Boltzmann softmax operator for value function estimation in continuous control. We first theoretically analyze the softmax operator in continuous action space. Then, we uncover an important property of the softmax operator in actor-critic algorithms, i.e., it helps to smooth the optimization landscape, which sheds new light on the benefits of the operator. We also design two new algorithms, Softmax Deep Deterministic Policy Gradients (SD2) and Softmax Deep Double Deterministic Policy Gradients (SD3), by building the softmax operator upon single and double estimators, which can effectively improve the overestimation and underestimation bias. We conduct extensive experiments on challenging continuous control tasks, and results show that SD3 outperforms state-of-the-art methods.</p>
<p><strong>Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model</strong></p>
<blockquote>
<p>ID 16917<br>Gen Li (Tsinghua University) · Yuting Wei (Carnegie Mellon University) · Yuejie Chi (CMU) · Yuantao Gu (Tsinghua University) · Yuxin Chen (Princeton University)</p>
</blockquote>
<p>We investigate the sample efficiency of reinforcement learning in a $\gamma$-discounted infinite-horizon Markov decision process (MDP) with state space S and action space A, assuming access to a generative model.  Despite a number of prior work tackling this problem, a complete picture of the trade-offs between sample complexity and statistical accuracy is yet to be determined. In particular, prior results suffer from a sample size barrier, in the sense that their claimed statistical guarantees hold only when the sample size exceeds at least $ |S| |A| / (1-\gamma)^2 $ (up to some log factor).  The current paper overcomes this barrier by certifying the minimax optimality of model-based reinforcement learning as soon as the sample size exceeds the order of $ |S| |A| / (1-\gamma) $ (modulo some log factor). More specifically, a perturbed model-based planning algorithm provably finds an $\epsilon$-optimal policy with an order of $ |S| |A| / ((1-\gamma)^3\epsilon^2 ) $ samples (up to log factor) for any $0&lt; \epsilon &lt; 1/(1-\gamma)$. Along the way, we derive improved (instance-dependent) guarantees for model-based policy evaluation. To the best of our knowledge, this work provides the first minimax-optimal guarantee in a generative model that accommodates the entire range of sample sizes (beyond which finding a meaningful policy is information theoretically impossible).</p>
<p><strong>Learning Multi-Agent Coordination for Enhancing Target Coverage in Directional Sensor Networks</strong></p>
<blockquote>
<p>ID 16920<br>Jing Xu (Peking University) · Fangwei Zhong (Peking University) · Yizhou Wang (Peking University)</p>
</blockquote>
<p>Maximum target coverage by adjusting the orientation of distributed sensors is an important problem in directional sensor networks (DSNs). This problem is challenging as the targets usually move randomly but the coverage range of sensors is limited in angle and distance. Thus, it is required to coordinate sensors to get ideal target coverage with low power consumption, e.g. no missing targets or reducing redundant coverage. To realize this, we propose a Hierarchical Target-oriented Multi-Agent Coordination (HiT-MAC), which decomposes the target coverage problem into two-level tasks: targets assignment by a coordinator and tracking assigned targets by executors. Specifically, the coordinator periodically monitors the environment globally and allocates targets to each executor. In turn, the executor only needs to track its assigned targets. To effectively learn the HiT-MAC by reinforcement learning, we further introduce a bunch of practical methods, including a self-attention module, marginal contribution approximation for the coordinator, goal-conditional observation filter for the executor, etc. Empirical results demonstrate the advantage of HiT-MAC in coverage rate, learning efficiency, and scalability, comparing to baselines. We also conduct an ablative analysis on the effectiveness of the introduced components in the framework.</p>
<p><strong>Off-Policy Imitation Learning from Observations</strong></p>
<blockquote>
<p>ID 16941<br>Zhuangdi Zhu (Michigan State University) · Kaixiang Lin (Michigan State University) · Bo Dai (Google Brain) · Jiayu Zhou (Michigan State University)</p>
</blockquote>
<p>Learning from Observations (LfO) is a practical reinforcement learning scenario from which many applications can benefit through the reuse of incomplete resources. Compared to conventional imitation learning (IL), LfO is more challenging because of the lack of expert action guidance. In both conventional IL and LfO, distribution matching is at the heart of their foundation. Traditional distribution matching approaches are sample-costly which depend on on-policy transitions for policy learning. Towards sample-efficiency, some off-policy solutions have been proposed, which, however, either lack comprehensive theoretical justifications or depend on the guidance of expert actions.In this work, we propose a sample-efficient LfO approach which enables off-policy optimization in a principled manner. To further accelerate the learning procedure, we regulate the policy update with an inverse action model, which assists distribution matching from the perspective of mode-covering. Extensive empirical results on challenging locomotion tasks indicate that our approach is comparable with state-of-the-art in terms of both sample-efficiency and asymptotic performance.</p>
<p><strong>Can Q-Learning with Graph Networks Learn a Generalizable Branching Heuristic for a SAT Solver?</strong></p>
<blockquote>
<p>ID 16954<br>Vitaly Kurin (University of Oxford) · Saad Godil (NVIDIA) · Shimon Whiteson (University of Oxford) · Bryan Catanzaro (NVIDIA)</p>
</blockquote>
<p>We present Graph-Q-SAT, a branching heuristic for a Boolean SAT solver trained with value-based reinforcement learning (RL) using Graph Neural Networks for function approximation. Solvers using Graph-Q-SAT are complete SAT solvers that either provide a satisfying assignment or proof of unsatisfiability, which is required for many SAT applications. The branching heuristics commonly used in SAT solvers make poor decisions during their warm-up period, whereas Graph-Q-SAT is trained to examine the structure of the particular problem instance to make better decisions early in the search. Training Graph-Q-SAT is data efficient and does not require elaborate dataset preparation or feature engineering. We train Graph-Q-SAT using RL interfacing with MiniSat solver and show that Graph-Q-SAT can reduce the number of iterations required to solve SAT problems by 2-3X. Furthermore, it generalizes to unsatisfiable SAT instances, as well as to problems with 5X more variables than it was trained on. We show that for larger problems, reductions in the number of iterations lead to wall clock time reductions, the ultimate goal when designing heuristics.We also show positive zero-shot transfer behavior when testing Graph-Q-SAT on a task family different from that used for training.While more work is needed to apply Graph-Q-SAT to reduce wall clock time in modern SAT solving settings, it is a compelling proof-of-concept showing that RL equipped with Graph Neural Networks can learn a generalizable branching heuristic for SAT search.</p>
<p><strong>DISK: Learning local features with policy gradient</strong></p>
<blockquote>
<p>ID 16979<br>MichaÅ‚ Tyszkiewicz (EPFL) · Pascal Fua (EPFL, Switzerland) · Eduard Trulls (Google)</p>
</blockquote>
<p>Local feature frameworks are difficult to learn in an end-to-end fashion due to the discreteness inherent to the selection and matching of sparse keypoints. We introduce DISK (DIScrete Keypoints), a novel method that overcomes these obstacles by leveraging principles from Reinforcement Learning (RL), optimizing end-to-end for a high number of correct feature matches. Our simple yet expressive probabilistic model lets us keep the training and inference regimes close, while maintaining good enough convergence properties to reliably train from scratch. Our features can be extracted very densely while remaining discriminative, challenging commonly held assumptions about what constitutes a good keypoint, as showcased in Fig. 1, and deliver state-of-the-art results on three public benchmarks.</p>
<p><strong>Learning Individually Inferred Communication for Multi-Agent Cooperation</strong></p>
<blockquote>
<p>ID 16981<br>Ziluo Ding (Peking University) · Tiejun Huang (Peking University) · Zongqing Lu (Peking University)</p>
</blockquote>
<p>Communication lays the foundation for human cooperation. It is also crucial for multi-agent cooperation. However, existing work focuses on broadcast communication, which is not only impractical but also leads to information redundancy that could even impair the learning process. To tackle these difficulties, we propose Individually Inferred Communication (I2C), a simple yet effective model to enable agents to learn a prior for agent-agent communication. The prior knowledge is learned via causal inference and realized by a feed-forward neural network that maps the agent’s local observation to a belief about who to communicate with. The influence of one agent on another is inferred via the joint action-value function in multi-agent reinforcement learning and quantified to label the necessity of agent-agent communication. Furthermore, the agent policy is regularized to better exploit communicated messages. Empirically, we show that I2C can not only reduce communication overhead but also improve the performance in a variety of multi-agent cooperative scenarios, comparing to existing methods.</p>
<p><strong>Lifelong Policy Gradient Learning of Factored Policies for Faster Training Without Forgetting</strong></p>
<blockquote>
<p>ID 16983<br>Jorge Mendez (University of Pennsylvania) · Boyu Wang (University of Western Ontario) · Eric Eaton (University of Pennsylvania)</p>
</blockquote>
<p>Policy gradient methods have shown success in learning control policies for high-dimensional dynamical systems. Their biggest downside is the amount of exploration they require before yielding high-performing policies. In a lifelong learning setting, in which an agent is faced with multiple consecutive tasks over its lifetime, reusing information from previously seen tasks can substantially accelerate the learning of new tasks. We provide a novel method for lifelong policy gradient learning that trains lifelong function approximators directly via policy gradients, allowing the agent to benefit from accumulated knowledge throughout the entire training process. We show empirically that our algorithm learns faster and converges to better policies than single-task and lifelong learning baselines, and completely avoids catastrophic forgetting on a variety of challenging domains.</p>
<p><strong>Fixed-Support Wasserstein Barycenters: Computational Hardness and Fast Algorithm</strong></p>
<blockquote>
<p>ID 16988<br>Tianyi Lin (UC Berkeley) · Nhat Ho (University of Texas at Austin) · Xi Chen (New York University) · Marco Cuturi (Google Brain  &amp;  CREST - ENSAE) · Michael Jordan (UC Berkeley)</p>
</blockquote>
<p>We study the fixed-support Wasserstein barycenter problem (FS-WBP), which consists in computing the Wasserstein barycenter of $m$ discrete probability measures supported on a finite metric space of size $n$. We show first that the constraint matrix arising from the standard linear programming (LP) representation of the FS-WBP is \textit{not totally unimodular} when $m \geq 3$ and $n \geq 3$. This result resolves an open question pertaining to the relationship between the FS-WBP and the minimum-cost flow (MCF) problem since it proves that the FS-WBP in the standard LP form is not an MCF problem when $m \geq 3$ and $n \geq 3$. We also develop a provably fast \textit{deterministic} variant of the celebrated iterative Bregman projection (IBP) algorithm, named \textsc{FastIBP}, with a complexity bound of $\tilde{O}(mn^{7/3}\varepsilon^{-4/3})$, where $\varepsilon \in (0, 1)$ is the desired tolerance. This complexity bound is better than the best known complexity bound of $\tilde{O}(mn^2\varepsilon^{-2})$ for the IBP algorithm in terms of $\varepsilon$, and that of $\tilde{O}(mn^{5/2}\varepsilon^{-1})$ from accelerated alternating minimization algorithm or accelerated primal-dual adaptive gradient algorithm in terms of $n$. Finally, we conduct extensive experiments with both synthetic data and real images and demonstrate the favorable performance of the \textsc{FastIBP} algorithm in practice.</p>
<p><strong>Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards</strong></p>
<blockquote>
<p>ID 16999<br>Yijie Guo (University of Michigan) · Jongwook Choi (University of Michigan) · Marcin Moczulski (Google Brain) · Shengyu Feng (University of Illinois Urbana Champaign) · Samy Bengio (Google Research, Brain Team) · Mohammad Norouzi (Google Brain) · Honglak Lee (Google / U. Michigan)</p>
</blockquote>
<p>Reinforcement learning with sparse rewards is challenging because an agent can rarely obtain non-zero rewards and hence, gradient-based optimization of parameterized policies can be incremental and slow. Recent work demonstrated that using a memory buffer of previous successful trajectories can result in more effective policies. However, existing methods may overly exploit past successful experiences, which can encourage the agent to adopt sub-optimal and myopic behaviors. In this work, instead of focusing on good experiences with limited diversity, we propose to learn a trajectory-conditioned policy to follow and expand diverse past trajectories from a memory buffer. Our method allows the agent to reach diverse regions in the state space and improve upon the past trajectories to reach new states. We empirically show that our approach significantly outperforms count-based exploration methods (parametric approach) and self-imitation learning (parametric approach with non-parametric memory) on various complex tasks with local optima. In particular, without using expert demonstrations or resetting to arbitrary states, we achieve the state-of-the-art scores under five billion number of frames, on challenging Atari games such as Montezuma’s Revenge and Pitfall.</p>
<p><strong>Almost Optimal Model-Free Reinforcement Learningvia Reference-Advantage Decomposition</strong></p>
<blockquote>
<p>ID 17025<br>Zihan Zhang (Tsinghua University) · Yuan Zhou (UIUC) · Xiangyang Ji (Tsinghua University)</p>
</blockquote>
<p>We  study  the  reinforcement  learning  problem  in  the  setting  of  finite-horizon1episodic Markov Decision Processes (MDPs) with S states, A actions, and episode length H. We propose a model-free algorithm UCB-ADVANTAGE and prove that it achieves  \tilde{O}(\sqrt{H^2 SAT}) regret where T=KH and K is the number of episodes to play.  Our regret bound improves upon the results of [Jin et al., 2018] and matches the best known model-based algorithms as well as the information theoretic lower bound up to logarithmic factors. We also show that UCB-ADVANTAGE achieves low local switching cost and applies to concurrent reinforcement learning, improving upon the recent results of [Bai et al., 2019].</p>
<p><strong>Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping</strong></p>
<blockquote>
<p>ID 17042<br>Yujing Hu (NetEase Fuxi AI Lab) · Weixun Wang (Tianjin University) · Hangtian Jia (Netease Fuxi AI Lab) · Yixiang Wang (University of Science and Technology of China) · Yingfeng Chen (NetEase Fuxi AI Lab) · Jianye Hao (Tianjin University) · Feng Wu (University of Science and Technology of China) · Changjie Fan (NetEase Fuxi AI Lab)</p>
</blockquote>
<p>Reward shaping is an effective technique for incorporating domain knowledge into reinforcement learning (RL). Existing approaches such as potential-based reward shaping normally make full use of a given shaping reward function. However, since the transformation of human knowledge into numeric reward values is often imperfect due to reasons such as human cognitive bias, completely utilizing the shaping reward function may fail to improve the performance of RL algorithms. In this paper, we consider the problem of adaptively utilizing a given shaping reward function. We formulate the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneficial shaping rewards, and meanwhile ignore unbeneficial shaping rewards or even transform them into beneficial ones.</p>
<p><strong>Effective Diversity in Population Based Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17049<br>Jack Parker-Holder (University of Oxford) · Aldo Pacchiano (UC Berkeley) · Krzysztof M Choromanski (Google Brain Robotics) · Stephen J Roberts (University of Oxford)</p>
</blockquote>
<p>Exploration is a key problem in reinforcement learning, since agents can only learn from data they acquire in the environment. With that in mind, maintaining a population of agents is an attractive method, as it allows data be collected with a diverse set of behaviors. This behavioral diversity is often boosted via multi-objective loss functions. However, those approaches typically leverage mean field updates based on pairwise distances, which makes them susceptible to cycling behaviors and increased redundancy. In addition, explicitly boosting diversity often has a detrimental impact on optimizing already fruitful behaviors for rewards. As such, the reward-diversity trade off typically relies on heuristics. Finally, such methods require behavioral representations, often handcrafted and domain specific. In this paper, we introduce an approach to optimize all members of a population simultaneously. Rather than using pairwise distance, we measure the volume of the entire population in a behavioral manifold, defined by task-agnostic behavioral embeddings. In addition, our algorithm Diversity via Determinants (DvD), adapts the degree of diversity during training using online learning techniques. We introduce both evolutionary and gradient-based instantiations of DvD and show they effectively improve exploration without reducing performance when better exploration is not required.</p>
<p><strong>A Boolean Task Algebra for Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17054<br>Geraud Nangue Tasse (University of the Witwatersrand) · Steven James (University of the Witwatersrand) · Benjamin Rosman (University of the Witwatersrand / CSIR)</p>
</blockquote>
<p>The ability to compose learned skills to solve new tasks is an important property for lifelong-learning agents. In this work we formalise the logical composition of tasks as a Boolean algebra. This allows us to formulate new tasks in terms of the negation, disjunction and conjunction of a set of base tasks. We then show that by learning goal-oriented value functions and restricting the transition dynamics of the tasks, an agent can solve these new tasks with no further learning. We prove that by composing these value functions in specific ways, we immediately recover the optimal policies for all tasks expressible under the Boolean algebra. We verify our approach in two domains—-including a high-dimensional video game environment requiring function approximation—-where an agent first learns a set of base skills, and then composes them to solve a super-exponential number of new tasks.</p>
<p><strong>A new convergent variant of Q-learning with linear function approximation</strong></p>
<blockquote>
<p>ID 17057<br>Diogo Carvalho (GAIPS, INESC-ID) · Francisco S. Melo (IST/INESC-ID) · Pedro A. Santos (Instituto Superior TÃ©cnico)</p>
</blockquote>
<p>In this work, we identify a novel set of conditions that ensure convergence with probability 1 of Q-learning with linear function approximation, by proposing a two time-scale variation thereof. In the faster time scale, the algorithm features an update similar to that of DQN, where the impact of bootstrapping is attenuated by using a Q-value estimate akin to that of the target network in DQN. The slower time-scale, in turn, can be seen as a modified target network update. We establish the convergence of our algorithm, provide an error bound and discuss our results in light of existing convergence results on reinforcement learning with function approximation. Finally, we illustrate the convergent behavior of our method in domains where standard Q-learning has previously been shown to diverge.</p>
<p><strong>Knowledge Transfer in Multi-Task Deep Reinforcement Learning for Continuous Control</strong></p>
<blockquote>
<p>ID 17070<br>Zhiyuan Xu (Syracuse University) · Kun Wu (Syracuse University) · Zhengping Che (DiDi AI Labs, Didi Chuxing) · Jian Tang (DiDi AI Labs, DiDi Chuxing) · Jieping Ye (Didi Chuxing)</p>
</blockquote>
<p>While Deep Reinforcement Learning (DRL) has emerged as a promising approachto many complex tasks, it remains challenging to train a single DRL agent that iscapable of undertaking multiple different continuous control tasks. In this paper,we present a Knowledge Transfer based Multi-task Deep Reinforcement Learningframework (KTM-DRL) for continuous control, which enables a single DRL agentto achieve expert-level performance in multiple different tasks by learning fromtask-specific teachers. In KTM-DRL, the multi-task agent first leverages an offlineknowledge transfer algorithm designed particularly for the actor-critic architectureto quickly learn a control policy from the experience of task-specific teachers, andthen it employs an online learning algorithm to further improve itself by learningfrom new online transition samples under the guidance of those teachers. Weperform a comprehensive empirical study with two commonly-used benchmarks inthe MuJoCo continuous control task suite. The experimental results well justifythe effectiveness of KTM-DRL and its knowledge transfer and online learningalgorithms, as well as its superiority over the state-of-the-art by a large margin.</p>
<p><strong>Multi-task Batch Reinforcement Learning with Metric Learning</strong></p>
<blockquote>
<p>ID 17080<br>Jiachen Li (University of California, San Diego) · Quan Vuong (University of California San Diego) · Shuang Liu (University of California, San Diego) · Minghua Liu (UCSD) · Kamil Ciosek (Microsoft) · Henrik Christensen (UC San Diego) · Hao Su (UCSD)</p>
</blockquote>
<p>We tackle the Multi-task Batch Reinforcement Learning problem. Given multiple datasets collected from different tasks, we train a multi-task policy to perform well in unseen tasks sampled from the same distribution. The task identities of the unseen tasks are not provided. To perform well, the policy must infer the task identity from collected transitions by modelling its dependency on states, actions and rewards. Because the different datasets may have state-action distributions with large divergence, the task inference module can learn to ignore the rewards and spuriously correlate \textit{only} state-action pairs to the task identity, leading to poor test time performance. To robustify task inference, we propose a novel application of the triplet loss. To mine hard negative examples, we relabel the transitions from the training tasks by approximating their reward functions. When we allow further training on the unseen tasks, using the trained policy as an initialization leads to significantly faster convergence compared to randomly initialized policies (up to 80% improvement and across 5 different Mujoco task distributions). We name our method \textbf{MBML} (\textbf{M}ulti-task \textbf{B}atch RL with \textbf{M}etric \textbf{L}earning).</p>
<p><strong>Demystifying Orthogonal Monte Carlo and Beyond</strong></p>
<blockquote>
<p>ID 17102<br>Han Lin (Columbia University) · Haoxian Chen (Columbia University) · Krzysztof M Choromanski (Google Brain Robotics) · Tianyi Zhang (Columbia University) · Clement Laroche (Columbia University)</p>
</blockquote>
<p>Orthogonal Monte Carlo (OMC) is a very effective sampling algorithm imposing structural geometric conditions (orthogonality) on samples for variance reduction. Due to its simplicity and superior performance as compared to its Quasi Monte Carlo counterparts, OMC is used in a wide spectrum of challenging machine learning applications ranging from scalable kernel methods to predictive recurrent neural networks, generative models and reinforcement learning.However theoretical understanding of the method remains very limited. In this paper we shed new light on the theoretical principles behind OMC, applying theory of negatively dependent random variables to obtain several new concentration results. As a corollary, we manage to obtain first uniform convergence results for OMCs and consequently, substantially strengthen best known downstream guarantees for kernel ridge regression via OMCs. We also propose novel extensions of the method leveraging theory of algebraic varieties over finite fields and particle algorithms, called Near-Orthogonal Monte Carlo (NOMC). We show that NOMC is the first algorithm consistently outperforming OMC in applications ranging from kernel methods to approximating distances in probabilistic metric spaces.</p>
<p><strong>On the Stability and Convergence of Robust Adversarial Reinforcement Learning: A Case Study on Linear Quadratic Systems</strong></p>
<blockquote>
<p>ID 17122<br>Kaiqing Zhang (University of Illinois at Urbana-Champaign (UIUC)) · Bin Hu (University of Illinois at Urbana-Champaign) · Tamer Basar (University of Illinois at Urbana-Champaign)</p>
</blockquote>
<p>Reinforcement learning (RL) algorithms can fail to generalize due to the gap between the simulation and the real world. One standard remedy is to use robust adversarial RL (RARL) that accounts for this gap during the policy training, by modeling the gap as an adversary against the training agent. In this work, we reexamine the effectiveness of RARL under a fundamental robust control setting: the linear quadratic (LQ) case. We first observe that the popular RARL scheme that greedily alternates agents’ updates can easily destabilize the system. Motivated by this, we propose several other policy-based RARL algorithms whose convergence behaviors are then studied both empirically and theoretically. We find: i) the conventional RARL framework (Pinto et al., 2017) can learn a destabilizing policy if the initial policy does not enjoy the robust stability property against the adversary; and ii) with robustly stabilizing initializations, our proposed double-loop RARL algorithm provably converges to the global optimal cost while maintaining robust stability on-the-fly. We also examine the stability and convergence issues of other variants of policy-based RARL algorithms, and then discuss several ways to learn robustly stabilizing initializations. From a robust control perspective, we aim to provide some new and critical angles about RARL, by identifying and addressing the stability issues in this fundamental LQ setting in continuous control. Our results make an initial attempt toward better theoretical understandings of policy-based RARL, the core approach in Pinto et al., 2017.</p>
<p><strong>Towards Playing Full MOBA Games with Deep Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17137<br>Deheng Ye (Tencent) · Guibin Chen (Tencent) · Wen Zhang (Tencent) · chen sheng (qq) · Bo Yuan (Tencent) · Bo Liu (Tencent) · Jia Chen (Tencent) · Hongsheng Yu (Tencent) · Zhao Liu (Tencent) · Fuhao Qiu (Tencent AI Lab) · Liang Wang (Tencent) · Tengfei Shi (Tencent) · Yinyuting Yin (Tencent) · Bei Shi (Tencent AI Lab) · Lanxiao Huang (Tencent) · qiang fu (Tencent AI Lab) · Wei Yang (Tencent AI Lab) · Wei Liu (Tencent AI Lab)</p>
</blockquote>
<p>MOBA games, e.g., Honor of Kings, League of Legends, and Dota 2, pose grand challenges to AI systems such as multi-agent, enormous state-action space, complex action control, etc. Developing AI for playing MOBA games has raised much attention accordingly. However, existing work falls short in handling the raw game complexity caused by the explosion of agent combinations, i.e., lineups, when expanding the hero pool in case that OpenAI’s Dota AI limits the play to a pool of only 17 heroes. As a result, full MOBA games without restrictions are far from being mastered by any existing AI system. In this paper, we propose a MOBA AI learning paradigm that methodologically enables playing full MOBA games with deep reinforcement learning. Specifically, we develop a combination of novel and existing learning techniques, including off-policy adaption, multi-head value estimation, curriculum self-play learning, policy distillation, and Monte-Carlo tree-search, in training and playing a large pool of heroes, meanwhile addressing the scalability issue skillfully. Tested on Honor of Kings, a popular MOBA game, we show how to build superhuman AI agents that can defeat top esports players. The superiority of our AI is demonstrated by the first large-scale performance test of MOBA AI agent in the literature.</p>
<p><strong>How to Learn a Useful Critic? Model-based Action-Gradient-Estimator Policy Optimization</strong></p>
<blockquote>
<p>ID 17157<br>Pierluca D’Oro (MILA) · Wojciech  JaÅ›kowski (NNAISENSE SA)</p>
</blockquote>
<p>Deterministic-policy actor-critic algorithms for continuous control improve the actor by plugging its actions into the critic and ascending the action-value gradient, which is obtained by chaining the actor’s Jacobian matrix with the gradient of the critic with respect to input actions. However, instead of gradients, the critic is, typically, only trained to accurately predict expected returns, which, on their own, are useless for policy optimization. In this paper, we propose MAGE, a model-based actor-critic algorithm, grounded in the theory of policy gradients, which explicitly learns the action-value gradient. MAGE backpropagates through the learned dynamics to compute gradient targets in temporal difference learning, leading to a critic tailored for policy improvement. On a set of MuJoCo continuous-control tasks, we demonstrate the efficiency of the algorithm in comparison to model-free and model-based state-of-the-art baselines.</p>
<p><strong>Reinforcement Learning in Factored MDPs: Oracle-Efficient Algorithms and Tighter Regret Bounds for the Non-Episodic Setting</strong></p>
<blockquote>
<p>ID 17165<br>Ziping Xu (University of Michigan) · Ambuj Tewari (University of Michigan)</p>
</blockquote>
<p>We study reinforcement learning in non-episodic factored Markov decision processes (FMDPs). We propose two near-optimal and oracle-efficient algorithms for FMDPs. Assuming oracle access to an FMDP planner, they enjoy a Bayesian and a frequentist regret bound respectively, both of which reduce to the near-optimal bound $O(DS\sqrt{AT})$ for standard non-factored MDPs. We propose a tighter connectivity measure, factored span, for FMDPs and prove a lower bound that depends on the factored span rather than the diameter $D$. In order to decrease the gap between lower and upper bounds, we propose an adaptation of the REGAL.C algorithm whose regret bound depends on the factored span. Our oracle-efficient algorithms outperform previously proposed near-optimal algorithms on computer network administration simulations.</p>
<p><strong>HiPPO: Recurrent Memory with Optimal Polynomial Projections</strong></p>
<blockquote>
<p>ID 17173<br>Albert Gu (Stanford) · Tri Dao (Stanford University) · Stefano Ermon (Stanford) · Atri Rudra (University at Buffalo, SUNY) · Christopher RÃ© (Stanford)</p>
</blockquote>
<p>A central problem in learning from sequential data is representing cumulativehistory in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.</p>
<p><strong>Promoting Coordination through Policy Regularization in Multi-Agent Deep Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17238<br>Julien Roy (Mila) · Paul Barde (Quebec AI institute - Ubisoft La Forge) · FÃ©lix G Harvey (Polytechnique MontrÃ©al) · Derek Nowrouzezahrai (McGill University) · Chris Pal (MILA, Polytechnique MontrÃ©al, Element AI)</p>
</blockquote>
<p>In multi-agent reinforcement learning, discovering successful collective behaviors is challenging as it requires exploring a joint action space that grows exponentially with the number of agents. While the tractability of independent agent-wise exploration is appealing, this approach fails on tasks that require elaborate group strategies. We argue that coordinating the agents’ policies can guide their exploration and we investigate techniques to promote such an inductive bias. We propose two policy regularization methods: TeamReg, which is based on inter-agent action predictability and CoachReg that relies on synchronized behavior selection. We evaluate each approach on four challenging continuous control tasks with sparse rewards that require varying levels of coordination as well as on the discrete action Google Research Football environment. Our experiments show improved performance across many cooperative multi-agent problems. Finally, we analyze the effects of our proposed methods on the policies that our agents learn and show that our methods successfully enforce the qualities that we propose as proxies for coordinated behaviors.</p>
<p><strong>Bias no more: high-probability data-dependent regret bounds for adversarial bandits and MDPs</strong></p>
<blockquote>
<p>ID 17245<br>Chung-Wei Lee (University of Southern California) · Haipeng Luo (University of Southern California) · Chen-Yu Wei (University of Southern California) · Mengxiao Zhang (University of Southern California)</p>
</blockquote>
<p>We develop a new approach to obtaining high probability regret bounds for online learning with bandit feedback against an adaptive adversary. While existing approaches all require carefully constructing optimistic and biased loss estimators,our approach uses standard unbiased estimators and relies on a simple increasing learning rate schedule, together with the help of logarithmically homogeneous self-concordant barriers and a strengthened Freedman’s inequality.Besides its simplicity, our approach enjoys several advantages. First, the obtained high-probability regret bounds are data-dependent and could be much smaller than the worst-case bounds, which resolves an open problem asked by Neu (2015). Second, resolving another open problem of Bartlett et al. (2008) and Abernethy and Rakhlin (2009), our approach leads to the first general and efficient algorithm with a high-probability regret bound for adversarial linear bandits, while previous methods are either inefficient or only applicable to specific action sets. Finally, our approach can also be applied to learning adversarial Markov Decision Processes and provides the first algorithm with a high-probability small-loss bound for this problem.</p>
<p><strong>Confounding-Robust Policy Evaluation in Infinite-Horizon Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17255<br>Nathan Kallus (Cornell University) · Angela Zhou (Cornell University)</p>
</blockquote>
<p>Off-policy evaluation of sequential decision policies from observational data is necessary in applications of batch reinforcement learning such as education and healthcare. In such settings, however, unobserved variables confound observed actions, rendering exact evaluation of new policies impossible, i.e, unidentifiable. We develop a robust approach that estimates sharp bounds on the (unidentifiable) value of a given policy in an infinite-horizon problem given data from another policy with unobserved confounding, subject to a sensitivity model. We consider stationary unobserved confounding and compute bounds by optimizing over the set of all stationary state-occupancy ratios that agree with a new partially identified estimating equation and the sensitivity model. We prove convergence to the sharp bounds as we collect more confounded data. Although checking set membership is a linear program, the support function is given by a difficult nonconvex optimization problem. We develop approximations based on nonconvex projected gradient descent and demonstrate the resulting bounds empirically.</p>
<p><strong>Simultaneously Learning Stochastic and Adversarial Episodic MDPs with Known Transition</strong></p>
<blockquote>
<p>ID 17260<br>Tiancheng Jin (University of Southern California) · Haipeng Luo (University of Southern California)</p>
</blockquote>
<p>This work studies the problem of learning episodic Markov Decision Processes with known transition and bandit feedback. We develop the first algorithm with a ``best-of-both-worlds’’ guarantee: it achieves O(log T) regret when the losses are stochastic, and simultaneously enjoys worst-case robustness with \tilde{O}(\sqrt{T}) regret even when the losses are adversarial, where T is the number of episodes. More generally, it achieves \tilde{O}(\sqrt{C}) regret in an intermediate setting where the losses are corrupted by a total amount of C.Our algorithm is based on the Follow-the-Regularized-Leader method from Zimin and Neu (2013), with a novel hybrid regularizer inspired by recent works of Zimmert et al. (2019a, 2019b) for the special case of multi-armed bandits. Crucially, our regularizer admits a non-diagonal Hessian with a highly complicated inverse. Analyzing such a regularizer and deriving a particular self-bounding regret guarantee is our key technical contribution and might be of independent interest.</p>
<p><strong>Learning Retrospective Knowledge with Reverse Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17276<br>Shangtong Zhang (University of Oxford) · Vivek Veeriah (University of Michigan) · Shimon Whiteson (University of Oxford)</p>
</blockquote>
<p>We present a Reverse Reinforcement Learning (Reverse RL) approach for representing retrospective knowledge. General Value Functions (GVFs) have enjoyed great success in representing predictive knowledge, i.e., answering questions about possible future outcomes such as “how much fuel will be consumed in expectation if we drive from A to B?”. GVFs, however, cannot answer questions like “how much fuel do we expect a car to have given it is at B at time t?”. To answer this question, we need to know when that car had a full tank and how that car came to B. Since such questions emphasize the influence of possible past events on the present, we refer to their answers as retrospective knowledge. In this paper, we show how to represent retrospective knowledge with Reverse GVFs, which are trained via Reverse RL. We demonstrate empirically the utility of Reverse GVFs in both representation learning and anomaly detection.</p>
<p><strong>Combining Deep Reinforcement Learning and Search for Imperfect-Information Games</strong></p>
<blockquote>
<p>ID 17281<br>Noam Brown (Facebook AI Research) · Anton Bakhtin (Facebook AI Research) · Adam Lerer (Facebook AI Research) · Qucheng Gong (Facebook AI Research)</p>
</blockquote>
<p>The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exemplified by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zero-sum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold’em poker, while using far less domain knowledge than any prior poker AI.</p>
<p><strong>POMO: Policy Optimization with Multiple Optima for Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17319<br>Yeong-Dae Kwon (Samsung SDS) · Jinho Choo (Samsung SDS) · Byoungjip Kim (Samsung SDS) · Iljoo Yoon (Samsung SDS) · Youngjune Gwon (Samsung SDS) · Seungjai Min (Samsung SDS)</p>
</blockquote>
<p>In neural combinatorial optimization (CO), reinforcement learning (RL) can turn a deep neural net into a fast, powerful heuristic solver of NP-hard problems. This approach has a great potential in practical applications because it allows near-optimal solutions to be found without expert guides armed with substantial domain knowledge. We introduce Policy Optimization with Multiple Optima (POMO), an end-to-end approach for building such a heuristic solver. POMO is applicable to a wide range of CO problems. It is designed to exploit the symmetries in the representation of a CO solution. POMO uses a modified REINFORCE algorithm that forces diverse rollouts towards all optimal solutions. Empirically, the low-variance baseline of POMO makes RL training fast and stable, and it is more resistant to local minima compared to previous approaches. We also introduce a new augmentation-based inference method, which accompanies POMO nicely. We demonstrate the effectiveness of POMO by solving three popular NP-hard problems, namely, traveling salesman (TSP), capacitated vehicle routing (CVRP), and 0-1 knapsack (KP). For all three, our solver based on POMO shows a significant improvement in performance over all recent learned heuristics. In particular, we achieve the optimality gap of 0.14% with TSP100 while reducing inference time by more than an order of magnitude.</p>
<p><strong>Mixed Hamiltonian Monte Carlo for Mixed Discrete and Continuous Variables</strong></p>
<blockquote>
<p>ID 17320<br>Guangyao Zhou (Vicarious AI)</p>
</blockquote>
<p>Hamiltonian Monte Carlo (HMC) has emerged as a powerful Markov Chain Monte Carlo (MCMC) method to sample from complex continuous distributions. However, a fundamental limitation of HMC is that it can not be applied to distributions with mixed discrete and continuous variables. In this paper, we propose mixed HMC (M-HMC) as a general framework to address this limitation. M-HMC is a novel family of MCMC algorithms that evolves the discrete and continuous variables in tandem, allowing more frequent updates of discrete variables while maintaining HMC’s ability to suppress random-walk behavior. We establish M-HMC’s theoretical properties, and present an efficient implementation with Laplace momentum that introduces minimal overhead compared to existing HMC methods. The superior performances of M-HMC over existing methods are demonstrated with numerical experiments on Gaussian mixture models (GMMs), variable selection in Bayesian logistic regression (BLR), and correlated topic models (CTMs).</p>
<p><strong>Self-Paced Deep Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17336<br>Pascal Klink (TU Darmstadt) · Carlo D’Eramo (TU Darmstadt) · Jan Peters (TU Darmstadt &amp; MPI Intelligent Systems) · Joni Pajarinen (TU Darmstadt)</p>
</blockquote>
<p>Curriculum reinforcement learning (CRL) improves the learning speed and stability of an agent by exposing it to a tailored series of tasks throughout learning. Despite empirical successes, an open question in CRL is how to automatically generate a curriculum for a given reinforcement learning (RL) agent, avoiding manual design. In this paper, we propose an answer by interpreting the curriculum generation as an inference problem, where distributions over tasks are progressively learned to approach the target task. This approach leads to an automatic curriculum generation, whose pace is controlled by the agent, with solid theoretical motivation and easily integrated with deep RL algorithms. In the conducted experiments, the curricula generated with the proposed algorithm significantly improve learning performance across several environments and deep RL algorithms, matching or outperforming state-of-the-art existing CRL algorithms.</p>
<p><strong>Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning</strong></p>
<blockquote>
<p>ID 17368<br>Sebastian Curi (ETH ZÃ¼rich) · Felix Berkenkamp (Bosch Center for Artificial Intelligence) · Andreas Krause (ETH Zurich)</p>
</blockquote>
<p>Model-based reinforcement learning algorithms with probabilistic dynamical models are amongst the most data-efficient learning methods. This is often attributed to their ability to distinguish between epistemic and aleatoric uncertainty. However, while most algorithms distinguish these two uncertainties for learning the model, they ignore it when optimizing the policy, which leads to greedy and insufficient exploration. At the same time, there are no practical solvers for optimistic exploration algorithms. In this paper, we propose a practical optimistic exploration algorithm (H-UCRL). H-UCRL reparameterizes the set of plausible models and hallucinates control directly on the epistemic uncertainty. By augmenting the input space with the hallucinated inputs, H-UCRL can be solved using standard greedy planners. Furthermore, we analyze H-UCRL and construct a general regret bound for well-calibrated models, which is provably sublinear in the case of Gaussian Process models. Based on this theoretical foundation, we show how optimistic exploration can be easily combined with state-of-the-art reinforcement learning algorithms and different probabilistic models. Our experiments demonstrate that optimistic exploration significantly speeds-up learning when there are penalties on actions, a setting that is notoriously difficult for existing model-based reinforcement learning algorithms.</p>
<p><strong>Doubly Robust Off-Policy Value and Gradient Estimation for Deterministic Policies</strong></p>
<blockquote>
<p>ID 17370<br>Nathan Kallus (Cornell University) · Masatoshi Uehara (Cornell University)</p>
</blockquote>
<p>Offline reinforcement learning, wherein one uses off-policy data logged by a fixed behavior policy to evaluate and learn new policies, is crucial in applications where experimentation is limited such as medicine. We study the estimation of policy value and gradient of a deterministic policy from off-policy data when actions are continuous. Targeting deterministic policies, for which action is a deterministic function of state, is crucial since optimal policies are always deterministic (up to ties). In this setting, standard importance sampling and doubly robust estimators for policy value and gradient fail because the density ratio does not exist. To circumvent this issue, we propose several new doubly robust estimators based on different kernelization approaches. We analyze the asymptotic mean-squared error of each of these under mild rate conditions for nuisance estimators. Specifically, we demonstrate how to obtain a rate that is independent of the horizon length.</p>
<p><strong>Off-Policy Evaluation and Learning for External Validity under a Covariate Shift</strong></p>
<blockquote>
<p>ID 17372<br>Masatoshi Uehara (Cornell University) · Masahiro Kato (The University of Tokyo) · Shota Yasui (Cyberagent)</p>
</blockquote>
<p>We consider the evaluation and training of a new policy for the evaluation data by using the historical data obtained from a different policy. The goal of off-policy evaluation (OPE) is to estimate the expected reward of a new policy over the evaluation data, and that of off-policy learning (OPL) is to find a new policy that maximizes the expected reward over the evaluation data. Although the standard OPE and OPL assume the same distribution of covariate between the historical and evaluation data, there often exists a problem of a covariate shift,i.e., the distribution of the covariate of the historical data is different from that of the evaluation data. In this paper, we derive the efficiency bound of OPE under a covariate shift. Then, we propose doubly robust and efficient estimators for OPE and OPL under a covariate shift by using an estimator of the density ratio between the distributions of the historical and evaluation data. We also discuss other possible estimators and compare their theoretical properties. Finally, we confirm the effectiveness of the proposed estimators through experiments.</p>
<p><strong>Improving Sample Complexity Bounds for (Natural) Actor-Critic Algorithms</strong></p>
<blockquote>
<p>ID 17382<br>Tengyu Xu (The Ohio State University) · Zhe Wang (Ohio State University) · Yingbin Liang (The Ohio State University)</p>
</blockquote>
<p>The actor-critic (AC) algorithm is a popular method to find an optimal policy in reinforcement learning. In the infinite horizon scenario, the finite-sample convergence rate for the AC and natural actor-critic (NAC) algorithms has been established recently, but under independent and identically distributed (i.i.d.) sampling and single-sample update at each iteration. In contrast, this paper characterizes the convergence rate and sample complexity of AC and NAC under Markovian sampling, with mini-batch data for each iteration, and with actor having general policy class approximation. We show that the overall sample complexity for a mini-batch AC to attain an $\epsilon$-accurate stationary point improves the best known sample complexity of AC by an order of $\mathcal{O}(\epsilon^{-1}\log(1/\epsilon))$, and the overall sample complexity for a mini-batch NAC to attain an $\epsilon$-accurate globally optimal point improves the existing sample complexity of NAC by an order of $\mathcal{O}(\epsilon^{-2}/\log(1/\epsilon))$. Moreover, the sample complexity of AC and NAC characterized in this work outperforms that of policy gradient (PG) and natural policy gradient (NPG) by a factor of $\mathcal{O}((1-\gamma)^{-3})$ and $\mathcal{O}((1-\gamma)^{-4}\epsilon^{-2}/\log(1/\epsilon))$, respectively. This is the first theoretical study establishing that AC and NAC attain orderwise performance improvement over PG and NPG under infinite horizon due to the incorporation of critic.</p>
<p><strong>Fast Epigraphical Projection-based Incremental Algorithms for Wasserstein Distributionally Robust Support Vector Machine</strong></p>
<blockquote>
<p>ID 17387<br>Jiajin Li (The Chinese University of Hong Kong) · Caihua Chen (Nanjing University) · Anthony Man-Cho So (CUHK)</p>
</blockquote>
<p>Wasserstein \textbf{D}istributionally \textbf{R}obust \textbf{O}ptimization (DRO) is concerned with finding decisions that perform well on data that are drawn from the worst probability distribution within a Wasserstein ball centered at a certain nominal distribution. In recent years, it has been shown that various DRO formulations of learning models admit tractable convex reformulations. However, most existing works propose to solve these convex reformulations by general-purpose solvers, which are not well-suited for tackling large-scale problems. In this paper, we focus on a family of Wasserstein distributionally robust support vector machine (DRSVM) problems and propose two novel epigraphical projection-based incremental algorithms to solve them. The updates in each iteration of these algorithms can be computed in a highly efficient manner. Moreover, we show that the DRSVM problems considered in this paper satisfy a Hölderian growth condition with explicitly determined growth exponents. Consequently, we are able to establish the convergence rates of the proposed incremental algorithms. Our numerical results indicate that the proposed methods are orders of magnitude faster than the state-of-the-art, and the performance gap grows considerably as the problem size increases.</p>
<p><strong>Off-policy Policy Evaluation For Sequential Decisions Under Unobserved Confounding</strong></p>
<blockquote>
<p>ID 17468<br>Hongseok Namkoong (Stanford University) · Ramtin Keramati (Stanford University) · Steve Yadlowsky (Stanford University) · Emma Brunskill (Stanford University)</p>
</blockquote>
<p>When observed decisions depend only on observed features, off-policy policy evaluation (OPE) methods for sequential decision problems can estimate the performance of evaluation policies before deploying them. However, this assumption is frequently violated due to unobserved confounders, unrecorded variables that impact both the decisions and their outcomes. We assess robustness of OPE methods under unobserved confounding by developing worst-case bounds on the performance of an evaluation policy. When unobserved confounders can affect every decision in an episode, we demonstrate that even small amounts of per-decision confounding can heavily bias OPE methods. Fortunately, in a number of important settings found in healthcare, policy-making, and technology, unobserved confounders may directly affect only one of the many decisions made, and influence future decisions/rewards only through the directly affected decision. Under this less pessimistic model of one-decision confounding, we propose an efficient loss-minimization-based procedure for computing worst-case bounds, and prove its statistical consistency. On simulated healthcare examples—-management of sepsis and interventions for autistic children—-where this is a reasonable model, we demonstrate that our method invalidates non-robust results and provides meaningful certificates of robustness, allowing reliable selection of policies under unobserved confounding.</p>
<p><strong>Self-Imitation Learning via Generalized Lower Bound Q-learning</strong></p>
<blockquote>
<p>ID 17477<br>Yunhao Tang (Columbia University)</p>
</blockquote>
<p>Self-imitation learning motivated by lower-bound Q-learning is a novel and effective approach for off-policy learning. In this work, we propose a n-step lower bound which generalizes the original return-based lower-bound Q-learning, and introduce a new family of self-imitation learning algorithms. To provide a formal motivation for the potential performance gains provided by self-imitation learning, we show that n-step lower bound Q-learning achieves a trade-off between fixed point bias and contraction rate, drawing close connections to the popular uncorrected n-step Q-learning. We finally show that n-step lower bound Q-learning is a more robust alternative to return-based self-imitation learning and uncorrected n-step, over a wide range of benchmark tasks.</p>
<p><strong>Weakly-Supervised Reinforcement Learning for Controllable Behavior</strong></p>
<blockquote>
<p>ID 17480<br>Lisa Lee (CMU / Google Brain / Stanford) · Ben Eysenbach (Carnegie Mellon University) · Russ Salakhutdinov (Carnegie Mellon University) · Shixiang (Shane) Gu (Google Brain) · Chelsea Finn (Stanford)</p>
</blockquote>
<p>Reinforcement learning (RL) is a powerful framework for learning to take actions to solve tasks. However, in many settings, an agent must winnow down the inconceivably large space of all possible tasks to the single task that it is currently being asked to solve. Can we instead constrain the space of tasks to those that are semantically meaningful? In this work, we introduce a framework for using weak supervision to automatically disentangle this semantically meaningful subspace of tasks from the enormous space of nonsensical “chaff” tasks. We show that this learned subspace enables efficient exploration and provides a representation that captures distance between states. On a variety of challenging, vision-based continuous control problems, our approach leads to substantial performance gains, particularly as the complexity of the environment grows.</p>
<p><strong>MOReL: Model-Based Offline Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17499<br>Rahul Kidambi (Cornell University) · Aravind Rajeswaran (University of Washington) · Praneeth Netrapalli (Microsoft Research) · Thorsten Joachims (Cornell)</p>
</blockquote>
<p>In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. This serves as an extreme test for an agent’s ability to effectively use historical data which is known to be critical for efficient RL. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP using the offline dataset; (b) learning a near-optimal policy in this pessimistic MDP. The design of the pessimistic MDP is such that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the pessimistic MDP. This enables the pessimistic MDP to serve as a good surrogate for purposes of policy evaluation and learning. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Empirically, MOReL matches or exceeds state-of-the-art results on widely used offline RL benchmarks. Overall, the modular design of MOReL enables translating advances in its components (for e.g., in model learning, planning etc.) to improvements in offline RL.</p>
<p><strong>Zap Q-Learning With Nonlinear Function Approximation</strong></p>
<blockquote>
<p>ID 17500<br>Shuhang Chen (University of Florida) · Adithya M Devraj (University of Florida) · Fan Lu (University of Florida) · Ana Busic (INRIA) · Sean Meyn (University of Florida)</p>
</blockquote>
<p>Zap Q-learning is a recent class of reinforcement learning algorithms, motivated primarily as a means to accelerate convergence.  Stability theory has been absent outside of two restrictive classes: the tabular setting, and optimal stopping.  This paper introduces a new framework for analysis of a more general class of recursive algorithms known as stochastic approximation. Based on this general theory, it is shown that Zap Q-learning is consistent under a non-degeneracy assumption, even when the function approximation architecture is nonlinear. Zap Q-learning with neural network function approximation emerges as a special case, and is tested on examples from OpenAI Gym.  Based on multiple experiments with a range of neural network sizes, it is found that the new algorithms converge quickly and are robust to choice of function approximation architecture.</p>
<p><strong>Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension</strong></p>
<blockquote>
<p>ID 17506<br>Ruosong Wang (Carnegie Mellon University) · Russ Salakhutdinov (Carnegie Mellon University) · Lin Yang (UCLA)</p>
</blockquote>
<p>Value function approximation has demonstrated phenomenal empirical success in reinforcement learning (RL). Nevertheless, despite a handful of recent progress on developing theory for RL with linear function approximation, the understanding of \emph{general} function approximation schemes largely remains missing. In this paper, we establish the first provably efficient RL algorithm with general value function approximation. We show that if the value functions admit an approximation with a function class $\mathcal{F}$, our algorithm achieves a regret bound of $\widetilde{O}(\mathrm{poly}(dH)\sqrt{T})$ where $d$ is a complexity measure of $\mathcal{F}$ that depends on the eluder dimension~[Russo and Van Roy, 2013] and log-covering numbers, $H$ is the planning horizon, and $T$ is the number interactions with the environment. Our theory generalizes the linear MDP assumption to general function classes. Moreover, our algorithm is model-free and provides a framework to justify the effectiveness of algorithms used in practice.</p>
<p><strong>Security Analysis of Safe and Seldonian Reinforcement Learning Algorithms</strong></p>
<blockquote>
<p>ID 17516<br>Pinar Ozisik (UMass Amherst) · Philip Thomas (University of Massachusetts Amherst)</p>
</blockquote>
<p>We analyze the extent to which existing methods rely on accurate training data for a specific class of reinforcement learning (RL) algorithms, known as Safe and Seldonian RL. We introduce a new measure of security to quantify the susceptibility to perturbations in training data by creating an attacker model that represents a worst-case analysis, and show that a couple of Seldonian RL methods are extremely sensitive to even a few data corruptions. We then introduce a new algorithm that is more robust against data corruptions, and demonstrate its usage in practice on some RL problems, including a grid-world and a diabetes treatment simulation.</p>
<p><strong>RepPoints v2: Verification Meets Regression for Object Detection</strong></p>
<blockquote>
<p>ID 17552<br>Yihong Chen (Peking University) · Zheng Zhang (MSRA) · Yue Cao (Microsoft Research) · Liwei Wang (Peking University) · Stephen Lin (Microsoft Research) · Han Hu (Microsoft Research Asia)</p>
</blockquote>
<p>Verification and regression are two general methodologies for prediction in neural networks. Each has its own strengths: verification can be easier to infer accurately, and regression is more efficient and applicable to continuous target variables. Hence, it is often beneficial to carefully combine them to take advantage of their benefits. In this paper, we take this philosophy to improve state-of-the-art object detection, specifically by RepPoints. Though RepPoints provides high performance, we find that its heavy reliance on regression for object localization leaves room for improvement. We introduce verification tasks into the localization prediction of RepPoints, producing RepPoints v2, which proves consistent improvements of about 2.0 mAP over the original RepPoints on COCO object detection benchmark using different backbones and training methods. RepPoints v2 also achieves 52.1 mAP on the COCO \texttt{test-dev} by a single model. Moreover, we show that the proposed approach can more generally elevate other object detection frameworks as well as applications such as instance segmentation.</p>
<p><strong>Belief-Dependent Macro-Action Discovery in POMDPs using the Value of Information</strong></p>
<blockquote>
<p>ID 17562<br>Genevieve E Flaspohler (Massachusetts Institute of Technology) · Nicholas Roy (MIT) · John W Fisher III (MIT)</p>
</blockquote>
<p>This work introduces macro-action discovery using value-of-information (VoI) for robust and efficient planning in partially observable Markov decision processes (POMDPs). POMDPs are a powerful framework for planning under uncertainty. Previous approaches have used high-level macro-actions within POMDP policies to reduce planning complexity. However, macro-action design is often heuristic and rarely comes with performance guarantees.  Here, we present a method for extracting belief-dependent, variable-length macro-actions directly from a low-level POMDP model.  We construct macro-actions by chaining sequences of open-loop actions together when the task-specific value of information (VoI) —- the change in expected task performance caused by observations in the current planning iteration —- is low. Importantly, we provide performance guarantees on the resulting VoI macro-action policies in the form of bounded regret relative to the optimal policy. In simulated tracking experiments, we achieve higher reward than both closed-loop and hand-coded macro-action baselines, selectively using VoI macro-actions to reduce planning complexity while maintaining near-optimal task performance.</p>
<p><strong>Bayesian Multi-type Mean Field Multi-agent Imitation Learning</strong></p>
<blockquote>
<p>ID 17579<br>Fan Yang (University at Buffalo) · Alina Vereshchaka (University at Buffalo) · Changyou Chen (University at Buffalo) · Wen Dong (University at Buffalo)</p>
</blockquote>
<p>Multi-agent Imitation learning (MAIL) refers to the problem that agents learn to perform a task interactively in a multi-agent system through observing and mimicking expert demonstrations, without any knowledge of a reward function from the environment. MAIL has received a lot of attention due to promising results achieved on synthesized tasks, with the potential to be applied to complex real-world multi-agent tasks. Key challenges for MAIL include sample efficiency and scalability. In this paper, we proposed Bayesian multi-type mean field multi-agent imitation learning (BM3IL). Our method improves sample efficiency through establishing a Bayesian formulation for MAIL, and enhances scalability through introducing a new multi-type mean field approximation. We demonstrate the performance of our algorithm through benchmarking with three state-of-the-art multi-agent imitation learning algorithms on several tasks, including solving a multi-agent traffic optimization problem in a real-world transportation network. Experimental results indicate that our algorithm significantly outperforms all other algorithms in all scenarios.</p>
<p><strong>Model-based Adversarial Meta-Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17592<br>Zichuan Lin (Tsinghua University) · Garrett W. Thomas (Stanford University) · Guangwen Yang (Tsinghua University) · Tengyu Ma (Stanford University)</p>
</blockquote>
<p>Meta-reinforcement learning (meta-RL) aims to learn from multiple training tasks the ability to adapt efficiently to unseen test tasks. Despite the success, existing meta-RL algorithms are known to be sensitive to the task distribution shift. When the test task distribution is different from the training task distribution, the performance may degrade significantly. To address this issue, this paper proposes \textit{Model-based Adversarial Meta-Reinforcement Learning} (AdMRL), where we aim to minimize the worst-case sub-optimality gap —- the difference between the optimal return and the return that the algorithm achieves after adaptation —-  across all tasks in a family of tasks, with a model-based approach. We propose a minimax objective and optimize it by alternating between learning the dynamics model on a fixed task and finding the \textit{adversarial} task for the current model —- the task for which the policy induced by the model is maximally suboptimal.  Assuming the family of tasks is parameterized, we derive a formula for the gradient of the suboptimality with respect to the task parameters via the implicit function theorem, and show how the gradient estimator can be efficiently implemented by the conjugate gradient method and a novel use of the REINFORCE estimator. We evaluate our approach on several continuous control benchmarks and demonstrate its efficacy in the worst-case performance over all tasks, the generalization power to out-of-distribution tasks, and in training and test time sample efficiency, over existing state-of-the-art meta-RL algorithms.</p>
<p><strong>Provably Efficient Neural GTD for Off-Policy Learning</strong></p>
<blockquote>
<p>ID 17599<br>Hoi-To Wai (The Chinese University of Hong Kong) · Zhuoran Yang (Princeton) · Zhaoran Wang (Northwestern University) · Mingyi Hong (University of Minnesota)</p>
</blockquote>
<p>This paper studies a gradient temporal difference (GTD) algorithm using neural network (NN) function approximators to minimize the mean squared Bellman error (MSBE). For off-policy learning, we show that the minimum MSBE problem can be recast into a min-max optimization involving a pair of over-parameterized primal-dual NNs. The resultant formulation can then be tackled using a neural GTD algorithm. We analyze the convergence of the proposed algorithm with a 2-layer ReLU NN architecture using $m$ neurons and prove that it computes an approximate optimal solution to the minimum MSBE problem as $m \rightarrow \infty$.</p>
<p><strong>A Randomized Algorithm to Reduce the Support of Discrete Measures</strong></p>
<blockquote>
<p>ID 17608<br>Francesco Cosentino (University of Oxford) · Harald Oberhauser (University of Oxford) · Alessandro Abate (University of Oxford)</p>
</blockquote>
<p>Given a discrete probability measure supported on $N$ atoms and a set of $n$ real-valued functions, there exists a probability measure that is supported on a subset of $n+1$ of the original $N$ atoms and has the same mean when integrated against each of the $n$ functions. If $ N \gg n$ this results in a huge reduction of complexity. We give a simple geometric characterization of barycenters via negative cones and derive a randomized algorithm that computes this new measure by ``greedy geometric sampling’’. We then study its properties, and benchmark it on synthetic and real-world data to show that it can be very beneficial in the $N\gg n$ regime. A Python implementation is available at \url{<a href="https://github.com/FraCose/Recombination_Random_Algos}">https://github.com/FraCose/Recombination_Random_Algos}</a>.</p>
<p><strong>Model Inversion Networks for Model-Based Optimization</strong></p>
<blockquote>
<p>ID 17613<br>Aviral Kumar (UC Berkeley) · Sergey Levine (UC Berkeley)</p>
</blockquote>
<p>This work addresses data-driven optimization problems, where the goal is to find an input that maximizes an unknown score or reward function given access to a dataset of inputs with corresponding scores. When the inputs are high-dimensional and valid inputs constitute a small subset of this space (e.g., valid protein sequences or valid natural images), such model-based optimization problems become exceptionally difficult, since the optimizer must avoid out-of-distribution and invalid inputs. We propose to address such problems with model inversion networks (MINs), which learn an inverse mapping from scores to inputs. MINs can scale to high-dimensional input spaces and leverage offline logged data for both contextual and non-contextual optimization problems. MINs can also handle both purely offline data sources and active data collection. We evaluate MINs on high- dimensional model-based optimization problems over images, protein designs, and neural network controller parameters, and bandit optimization from logged data.</p>
<p><strong>Safe Reinforcement Learning via Curriculum Induction</strong></p>
<blockquote>
<p>ID 17616<br>Matteo Turchetta (ETH Zurich) · Andrey Kolobov (Microsoft Research) · Shital Shah (Microsoft) · Andreas Krause (ETH Zurich) · Alekh Agarwal (Microsoft Research)</p>
</blockquote>
<p>In safety-critical applications, autonomous agents may need to learn in an environment where mistakes can be very costly. In such settings, the agent needs to behave safely not only after but also while learning. To achieve this, existing safe reinforcement learning methods make an agent rely on priors that let it avoid dangerous situations during exploration with high probability, but both the probabilistic guarantees and the smoothness assumptions inherent in the priors are not viable in many scenarios of interest such as autonomous driving. This paper presents an alternative approach inspired by human teaching, where an agent learns under the supervision of an automatic instructor that saves the agent from violating constraints during learning. In this model, we introduce the monitor that neither needs to know how to do well at the task the agent is learning nor needs to know how the environment works. Instead, it has a library of reset controllers that it activates when the agent starts behaving dangerously, preventing it from doing damage. Crucially, the choices of which reset controller to apply in which situation affect the speed of agent learning. Based on observing agents’ progress the teacher itself learns a policy for choosing the reset controllers, a curriculum, to optimize the agent’s final policy reward. Our experiments use this framework in two environments to induce curricula for safe and efficient learning.</p>
<p><strong>Conservative Q-Learning for Offline Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17621<br>Aviral Kumar (UC Berkeley) · Aurick Zhou (University of California, Berkeley) · George Tucker (Google Brain) · Sergey Levine (UC Berkeley)</p>
</blockquote>
<p>Effectively leveraging large, previously collected datasets in reinforcement learn- ing (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.</p>
<p><strong>SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection</strong></p>
<blockquote>
<p>ID 17628<br>Xiaoya Li (Shannon.AI) · Yuxian Meng (Shannon.AI) · Mingxin Zhou (Shannon.AI) · Qinghong  Han (Shannon.AI) · Fei Wu (Zhejiang University) · Jiwei Li (Shannon.AI)</p>
</blockquote>
<p>While the self-attention mechanism has been widely used in a wide variety of tasks, it has the unfortunate property of a quadratic cost with respect to the input  length, which makes it difficult to deal with long inputs. In this paper, we present a  method for accelerating and structuring self-attentions: Sparse Adaptive Connection (SAC). In SAC, we regard the input sequence as a graph and attention operations are performed between linked nodes. In contrast with previous self-attention models with pre-defined structures (edges), the model learns to construct attention edges to  improve task-specific performances. In this way, the model is able to select the most salient nodes and reduce the quadratic complexity regardless of the sequence length. Based on SAC, we show that previous variants of self-attention models are its special cases. Through extensive experiments on neural machine translation, language modeling, graph representation learning and image classification, we demonstrate SAC is competitive with state-of-the-art models while significantly reducing  memory cost.</p>
<p><strong>Variational Bayesian Monte Carlo with Noisy Likelihoods</strong></p>
<blockquote>
<p>ID 17637<br>Luigi Acerbi (University of Helsinki)</p>
</blockquote>
<p>Variational Bayesian Monte Carlo (VBMC) is a recently introduced framework that uses Gaussian process surrogates to perform approximate Bayesian inference in models with black-box, non-cheap likelihoods. In this work, we extend VBMC to deal with noisy log-likelihood evaluations, such as those arising from simulation-based models. We introduce new global’ acquisition functions, such as expected information gain (EIG) and variational interquantile range (VIQR), which are robust to noise and can be efficiently evaluated within the VBMC setting. In a novel, challenging, noisy-inference benchmark comprising of a variety of models with real datasets from computational and cognitive neuroscience, VBMC+VIQR achieves state-of-the-art performance in recovering the ground-truth posteriors and model evidence.In particular, our method vastly outperformslocal’ acquisition functions and other surrogate-based inference methods while keeping a small algorithmic cost. Our benchmark corroborates VBMC as a general-purpose technique for sample-efficient black-box Bayesian inference also with noisy models.</p>
<p><strong>Munchausen Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17663<br>Nino Vieillard (Google Brain) · Olivier Pietquin (Google Research    Brain Team) · Matthieu Geist (Google Brain)</p>
</blockquote>
<p>Bootstrapping is a core mechanism in Reinforcement Learning (RL). Most algorithms, based on temporal differences, replace the true value of a transiting state by their current estimate of this value. Yet, another estimate could be leveraged to bootstrap RL: the current policy. Our core contribution stands in a very simple idea: adding the scaled log-policy to the immediate reward. We show that, by slightly modifying Deep Q-Network (DQN) in that way provides an agent that is competitive with the state-of-the-art Rainbow on Atari games, without making use of distributional RL, n-step returns or prioritized replay. To demonstrate the versatility of this idea, we also use it together with an Implicit Quantile Network (IQN). The resulting agent outperforms Rainbow on Atari, installing a new State of the Art with very little modifications to the original algorithm. To add to this empirical study, we provide strong theoretical insights on what happens under the hood — implicit Kullback-Leibler regularization and increase of the action-gap.</p>
<p><strong>A Self-Tuning Actor-Critic Algorithm</strong></p>
<blockquote>
<p>ID 17681<br>Tom Zahavy (Technion) · Zhongwen Xu (DeepMind) · Vivek Veeriah (University of Michigan) · Matteo Hessel (Google DeepMind) · Junhyuk Oh (DeepMind) · Hado van Hasselt (DeepMind) · David Silver (DeepMind) · Satinder Singh (DeepMind)</p>
</blockquote>
<p>Reinforcement learning algorithms are highly sensitive to the choice of hyperparameters, typically requiring significant manual effort to identify hyperparameters that perform well on a new domain. In this paper, we take a step towards addressing this issue by using metagradients to automatically adapt hyperparameters online by meta-gradient descent (Xu et al., 2018). We apply our algorithm, Self-Tuning Actor-Critic (STAC), to self-tune all the differentiable hyperparameters of an actor-critic loss function, to discover auxiliary tasks, and to improve off-policy learning using a novel leaky V-trace operator. STAC is simple to use, sample efficient and does not require a significant increase in compute. Ablative studies show that the overall performance of STAC improved as we adapt more hyperparameters. When applied to the Arcade Learning Environment (Bellemare et al. 2012), STAC improved the median human normalized score in 200M steps from 243% to 364%. When applied to the DM Control suite (Tassa et al., 2018), STAC improved the mean score in 30M steps from 217 to 389 when learning with features, from 108 to 202 when learning from pixels, and from 195 to 295 in the Real-World Reinforcement Learning Challenge (Dulac-Arnold et al., 2020).</p>
<p><strong>Non-Crossing Quantile Regression for Distributional Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17686<br>Fan Zhou (Shanghai University of Finance and Economics) · Jianing Wang (Shanghai University of Finance and Economics) · Xingdong Feng (Shanghai University of Finance and Economics)</p>
</blockquote>
<p>Distributional reinforcement learning (DRL) estimates the distribution over future returns instead of the mean to more efficiently capture the intrinsic uncertainty of MDPs. However, batch-based DRL algorithms cannot guarantee the non-decreasing property of learned quantile curves especially at the early training stage, leading to abnormal distribution estimates and reduced model interpretability. To address these issues, we introduce a general DRL framework by using non-crossing quantile regression to ensure the monotonicity constraint within each sampled batch, which can be incorporated with any well-known DRL algorithm. We demonstrate the validity of our method from both the theory and model implementation perspectives. Experiments on Atari 2600 Games show that some state-of-art DRL algorithms with the non-crossing modification can significantly outperform their baselines in terms of faster convergence speeds and better testing performance. In particular, our method can effectively recover the distribution information and thus dramatically increase the exploration efficiency when the reward space is extremely sparse.</p>
<p><strong>Online Meta-Critic Learning for Off-Policy Actor-Critic Methods</strong></p>
<blockquote>
<p>ID 17697<br>Wei Zhou (National University of Defense Technology) · Yiying Li (National University of Defense Technology) · Yongxin Yang (University of Edinburgh ) · Huaimin Wang (National University of Defense Technology) · Timothy Hospedales (University of Edinburgh)</p>
</blockquote>
<p>Off-Policy Actor-Critic (OffP-AC) methods have proven successful in a variety of continuous control tasks. Normally, the critic’s action-value function is updated using temporal-difference, and the critic in turn provides a loss for the actor that trains it to take actions with higher expected return. In this paper, we introduce a flexible and augmented meta-critic that observes the learning process and meta-learns an additional loss for the actor that accelerates and improves actor-critic learning. Compared to existing meta-learning algorithms, meta-critic is rapidly learned online for a single task, rather than slowly over a family of tasks. Crucially, our meta-critic is designed for off-policy based learners, which currently provide state-of-the-art reinforcement learning sample efficiency. We demonstrate that online meta-critic learning benefits to a variety of continuous control tasks when combined with contemporary OffP-AC methods DDPG, TD3 and SAC.</p>
<p><strong>Online Decision Based Visual Tracking via Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17702<br>ke Song (Shandong university) · Wei Zhang (Shandong University) · Ran Song (School of Control Science and Engineering, Shandong University) · Yibin Li (Shandong University)</p>
</blockquote>
<p>A deep visual tracker is typically based on either object detection or template matching while each of them is only suitable for a particular group of scenes. It is straightforward to consider fusing them together to pursue more reliable tracking. However, this is not wise as they follow different tracking principles. Unlike previous fusion-based methods, we propose a novel ensemble framework, named DTNet, with an online decision mechanism for visual tracking based on hierarchical reinforcement learning. The decision mechanism substantiates an intelligent switching strategy where the detection and the template trackers have to compete with each other to conduct tracking within different scenes that they are adept in. Besides, we present a novel detection tracker which avoids the common issue of incorrect proposal. Extensive results show that our DTNet achieves state-of-the-art tracking performance as well as good balance between accuracy and efficiency. The project website is available at <a href="https://vsislab.github.io/DTNet/">https://vsislab.github.io/DTNet/</a>.</p>
<p><strong>Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization</strong></p>
<blockquote>
<p>ID 17703<br>Paul Barde (Quebec AI institute - Ubisoft La Forge) · Julien Roy (Mila) · Wonseok Jeon (MILA, McGill University) · Joelle Pineau (McGill University) · Chris Pal (MILA, Polytechnique MontrÃ©al, Element AI) · Derek Nowrouzezahrai (McGill University)</p>
</blockquote>
<p>Adversarial Imitation Learning alternates between learning a discriminator — which tells apart expert’s demonstrations from generated ones — and a generator’s policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator’s iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator’s policy. Consequently, our discriminator’s update solves the generator’s optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods.</p>
<p><strong>Discovering Reinforcement Learning Algorithms</strong></p>
<blockquote>
<p>ID 17706<br>Junhyuk Oh (DeepMind) · Matteo Hessel (Google DeepMind) · Wojciech Czarnecki (DeepMind) · Zhongwen Xu (DeepMind) · Hado van Hasselt (DeepMind) · Satinder Singh (DeepMind) · David Silver (DeepMind)</p>
</blockquote>
<p>Reinforcement learning (RL) algorithms update an agent’s parameters according to one of several possible rules, discovered manually through years of research. Automating the discovery of update rules from data could lead to more efficient algorithms, or algorithms that are better adapted to specific environments. Although there have been prior attempts at addressing this significant scientific challenge, it remains an open question whether it is feasible to discover alternatives to fundamental concepts of RL such as value functions and temporal-difference learning. This paper introduces a new meta-learning approach that discovers an entire update rule which includes both what to predict’ (e.g. value functions) andhow to learn from it’ (e.g. bootstrapping) by interacting with a set of environments. The output of this method is an RL algorithm that we call Learned Policy Gradient (LPG). Empirical results show that our method discovers its own alternative to the concept of value functions. Furthermore it discovers a bootstrapping mechanism to maintain and use its predictions. Surprisingly, when trained solely on toy environments, LPG generalises effectively to complex Atari games and achieves non-trivial performance. This shows the potential to discover general RL algorithms from data.</p>
<p><strong>Model-based Policy Optimization with Unsupervised Model Adaptation</strong></p>
<blockquote>
<p>ID 17720<br>Jian Shen (Shanghai Jiao Tong University) · Han Zhao (Carnegie Mellon University) · Weinan Zhang (Shanghai Jiao Tong University) · Yong Yu (Shanghai Jiao Tong Unviersity)</p>
</blockquote>
<p>Model-based reinforcement learning methods learn a dynamics model with real data sampled from the environment and leverage it to generate simulated data to derive an agent. However, due to the potential distribution mismatch between simulated data and real data, this could lead to degraded performance. Despite much effort being devoted to reducing this distribution mismatch, existing methods fail to solve it explicitly. In this paper, we investigate how to bridge the gap between real and simulated data due to inaccurate model estimation for better policy optimization. To begin with, we first derive a lower bound of the expected return, which naturally inspires a bound maximization algorithm by aligning the simulated and real data distributions. To this end, we propose a novel model-based reinforcement learning framework AMPO, which introduces unsupervised model adaptation to minimize the integral probability metric (IPM) between feature distributions from real and simulated data. Instantiating our framework with Wasserstein-1 distance gives a practical model-based approach. Empirically, our approach achieves state-of-the-art performance in terms of sample efficiency on a range of continuous control benchmark tasks.</p>
<p><strong>Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17727<br>Filippos Christianos (University of Edinburgh) · Lukas SchÃ¤fer (University of Edinburgh) · Stefano Albrecht (University of Edinburgh)</p>
</blockquote>
<p>Exploration in multi-agent reinforcement learning is a challenging problem, especially in environments with sparse rewards. We propose a general method for efficient exploration by sharing experience amongst agents.  Our proposed algorithm, called shared Experience Actor-Critic(SEAC), applies experience sharing in an actor-critic framework by combining the gradients of different agents. We evaluate SEAC in a collection of sparse-reward multi-agent environments and find that it consistently outperforms several baselines and state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environments, experience sharing makes the difference between learning to solve the task and not learning at all.</p>
<p><strong>The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17766<br>Harm Van Seijen (Microsoft Research) · Hadi Nekoei (MILA) · Evan Racah (Mila, UniversitÃ© de MontrÃ©al) · Sarath Chandar (Mila / Ã‰cole Polytechnique de MontrÃ©al)</p>
</blockquote>
<p>Deep model-based Reinforcement Learning (RL) has the potential to substantially improve the sample-efficiency of deep RL. While various challenges have long held it back, a number of papers have recently come out reporting success with deep model-based methods. This is a great development, but the lack of a consistent metric to evaluate such methods makes it difficult to compare various approaches. For example, the common single-task sample-efficiency metric conflates improvements due to model-based learning with various other aspects, such as representation learning, making it difficult to assess true progress on model-based RL. To address this, we introduce an experimental setup to evaluate model-based behavior of RL methods, inspired by work from neuroscience on detecting model-based behavior in humans and animals. Our metric based on this setup, the Local Change Adaptation (LoCA) regret, measures how quickly an RL method adapts to a local change in the environment. Our metric can identify model-based behavior, even if the method uses a poor representation and provides insight in how close a method’s behavior is from optimal model-based behavior. We use our setup to evaluate the model-based behavior of MuZero on a variation of the classic Mountain Car task.</p>
<p><strong>Deep Inverse Q-learning with Constraints</strong></p>
<blockquote>
<p>ID 17813<br>Gabriel Kalweit (University of Freiburg) · Maria Huegle (University of Freiburg) · Moritz Werling (BMWGroup, Unterschleissheim) · Joschka Boedecker (University of Freiburg)</p>
</blockquote>
<p>Popular Maximum Entropy Inverse Reinforcement Learning approaches require the computation of expected state visitation frequencies for the optimal policy under an estimate of the reward function. This usually requires intermediate value estimation in the inner loop of the algorithm, slowing down convergence considerably. In this work, we introduce a novel class of algorithms that only needs to solve the MDP underlying the demonstrated behavior once to recover the expert policy. This is possible through a formulation that exploits a probabilistic behavior assumption for the demonstrations within the structure of Q-learning. We propose Inverse Action-value Iteration which is able to fully recover an underlying reward of an external agent in closed-form analytically. We further provide an accompanying class of sampling-based variants which do not depend on a model of the environment. We show how to extend this class of algorithms to continuous state-spaces via function approximation and how to estimate a corresponding action-value function, leading to a policy as close as possible to the policy of the external agent, while optionally satisfying a list of predefined hard constraints. We evaluate the resulting algorithms called Inverse Action-value Iteration, Inverse Q-learning and Deep Inverse Q-learning on the Objectworld benchmark, showing a speedup of up to several orders of magnitude compared to (Deep) Max-Entropy algorithms. We further apply Deep Constrained Inverse Q-learning on the task of learning autonomous lane-changes in the open-source simulator SUMO achieving competent driving after training on data corresponding to 30 minutes of demonstrations.</p>
<p><strong>Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17822<br>Nino Vieillard (Google Brain) · Tadashi Kozuno (Okinawa Institute of Science and Technology) · Bruno Scherrer (INRIA) · Olivier Pietquin (Google Research    Brain Team) · Remi Munos (DeepMind) · Matthieu Geist (Google Brain)</p>
</blockquote>
<p>Recent Reinforcement Learning (RL) algorithms making use of  Kullback-Leibler (KL) regularization as a core component have shown outstanding performance. Yet, only little is understood theoretically about why KL regularization helps, so far. We study KL regularization within an approximate value iteration scheme and show that it implicitly averages q-values. Leveraging this insight, we provide a very strong performance bound, the very first to combine two desirable aspects: a linear dependency to the horizon (instead of quadratic) and an error propagation term involving an averaging effect of the estimation errors (instead of an accumulation effect). We also study the more general case of an additional entropy regularizer. The resulting abstract scheme encompasses many existing RL algorithms. Some of our assumptions do not hold with neural networks, so we complement this theoretical analysis with an extensive empirical study.</p>
<p><strong>Task-agnostic Exploration in Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17826<br>Xuezhou Zhang (UW-Madison) · Yuzhe Ma (University of Wisconsin-Madison) · Adish Singla (MPI-SWS)</p>
</blockquote>
<p>Efficient exploration is one of the main challenges in reinforcement learning (RL). Most existing sample-efficient algorithms assume the existence of a single reward function during exploration. In many practical scenarios, however, there is not a single underlying reward function to guide the exploration, for instance, when an agent needs to learn many skills simultaneously, or multiple conflicting objectives need to be balanced. To address these challenges,  we propose the \textit{task-agnostic RL} framework: In the exploration phase, the agent first collects trajectories by exploring the MDP without the guidance of a reward function. After exploration, it aims at finding near-optimal policies for $N$ tasks, given the collected trajectories augmented with \textit{sampled rewards} for each task. We present an efficient task-agnostic RL algorithm, \textsc{UCBZero}, that finds $\epsilon$-optimal policies for $N$ arbitrary tasks after at most $\tilde O(\log(N)H^5SA/\epsilon^2)$ exploration episodes. We also provide an $\Omega(\log (N)H^2SA/\epsilon^2)$ lower bound, showing that the $\log$ dependency on $N$ is unavoidable. Furthermore, we provide an $N$-independent sample complexity bound of \textsc{UCBZero} in the statistically easier setting when the ground truth reward functions are known.</p>
<p><strong>Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17836<br>Tianren Zhang (Tsinghua University) · Shangqi Guo (Tsinghua University) · Tian Tan (Stanford University) · Xiaolin Hu (Tsinghua University) · Feng Chen (Tsinghua University)</p>
</blockquote>
<p>Goal-conditioned hierarchical reinforcement learning (HRL) is a promising approach for scaling up reinforcement learning (RL) techniques. However, it often suffers from training inefficiency as the action space of the high-level, i.e., the goal space, is often large. Searching in a large goal space poses difficulties for both high-level subgoal generation and low-level policy learning. In this paper, we show that this problem can be effectively alleviated by restricting the high-level action space from the whole goal space to a k-step adjacent region of the current state using an adjacency constraint. We theoretically prove that the proposed adjacency constraint preserves the optimal hierarchical policy in deterministic MDPs, and show that this constraint can be practically implemented by training an adjacency network that can discriminate between adjacent and non-adjacent subgoals. Experimental results on discrete and continuous control tasks show that incorporating the adjacency constraint improves the performance of state-of-the-art HRL approaches in both deterministic and stochastic environments.</p>
<p><strong>Reinforcement Learning with Feedback Graphs</strong></p>
<blockquote>
<p>ID 17844<br>Christoph Dann (Carnegie Mellon University) · Yishay Mansour (Google) · Mehryar Mohri (Courant Inst. of Math. Sciences &amp; Google Research) · Ayush Sekhari (Cornell University) · Karthik Sridharan (Cornell University)</p>
</blockquote>
<p>We study RL in the tabular MDP setting where the agent receives additional observations per step in the form of transitions samples. Such additional observations can be provided in many tasks by auxiliary sensors or by leveraging prior knowledge about the environment (e.g., when certain actions yield similar outcome).  We formalize this setting using a feedback graph over state-action pairs and show that model-based algorithms can incorporate additional observations for more sample-efficient learning. We give a regret bound that predominantly depends on the size of the maximum acyclic subgraph of the feedback graph, in contrast with a polynomial dependency on the number of states and actions in the absence of side observations. Finally, we highlight fundamental challenges for leveraging a small dominating set of the feedback graph, as compared to the well-studied bandit setting, and propose a new algorithm that can use such a dominating set to learn a near-optimal policy faster.</p>
<p><strong>Storage Efficient and Dynamic Flexible Runtime Channel Pruning via Deep Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17850<br>Jianda Chen (Nanyang Technological University) · Shangyu Chen (Nanyang Technological University, Singapore) · Sinno Jialin Pan (Nanyang Technological University, Singapore)</p>
</blockquote>
<p>In this paper, we propose a deep reinforcement learning (DRL) based framework to efficiently perform runtime channel pruning on convolutional neural networks (CNNs). Our DRL-based framework aims to learn a pruning strategy to determine how many and which channels to be pruned in each convolutional layer, depending on each individual input instance at runtime. Unlike existing runtime pruning methods which require to store all channels parameters for inference, our framework can reduce parameters storage consumption by introducing a static pruning component. Comparison experimental results with existing runtime and static pruning methods on state-of-the-art CNNs demonstrate that our proposed framework is able to provide a tradeoff between dynamic flexibility and storage efficiency in runtime channel pruning.</p>
<p><strong>Towards Safe Policy Improvement for Non-Stationary MDPs</strong></p>
<blockquote>
<p>ID 17861<br>Yash Chandak (University of Massachusetts Amherst) · Scott Jordan (University of Massachusetts Amherst) · Georgios Theocharous (Adobe Research) · Martha White (University of Alberta) · Philip Thomas (University of Massachusetts Amherst)</p>
</blockquote>
<p>Many real-world sequential decision-making problems involve critical systems with financial risks and human-life risks. While several works in the past have proposed methods that are safe for deployment, they assume that the underlying problem is stationary. However, many real-world problems of interest exhibit non-stationarity, and when stakes are high, the cost associated with a false stationarity assumption may be unacceptable. We take the first steps towards ensuring safety, with high confidence, for smoothly-varying non-stationary decision problems. Our proposed method extends a type of safe algorithm, called a Seldonian algorithm, through a synthesis of model-free reinforcement learning with time-series analysis. Safety is ensured using sequential hypothesis testing of a policy’s forecasted performance, and confidence intervals are obtained using wild bootstrap.</p>
<p><strong>Multi-Task Reinforcement Learning with Soft Modularization</strong></p>
<blockquote>
<p>ID 17867<br>Ruihan Yang (UC San Diego) · Huazhe Xu (UC Berkeley) · YI WU (UC Berkeley) · Xiaolong Wang (UCSD/UC Berkeley)</p>
</blockquote>
<p>Multi-task learning is a very challenging problem in reinforcement learning. While training multiple tasks jointly allow the policies to share parameters across different tasks, the optimization problem becomes non-trivial:  It remains unclear what parameters in the network should be reused across tasks, and how the gradients from different tasks may interfere with each other. Thus, instead of naively sharing parameters across tasks, we introduce an explicit modularization technique on policy representation to alleviate this optimization issue. Given a base policy network, we design a routing network which estimates different routing strategies to reconfigure the base network for each task. Instead of directly selecting routes for each task, our task-specific policy uses a method called soft modularization to softly combine all the possible routes, which makes it suitable for sequential tasks. We experiment with various robotics manipulation tasks in simulation and show our method improves both sample efficiency and performance over strong baselines by a large margin.</p>
<p><strong>MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17893<br>Elise van der Pol (University of Amsterdam) · Daniel Worrall (University of Amsterdam) · Herke van Hoof (University of Amsterdam) · Frans Oliehoek (TU Delft) · Max Welling (University of Amsterdam / Qualcomm AI Research)</p>
</blockquote>
<p>This paper introduces MDP homomorphic networks for deep reinforcement learning. MDP homomorphic networks are neural networks that are equivariant under symmetries in the joint state-action space of an MDP. Current approaches to deep reinforcement learning do not usually exploit knowledge about such structure. By building this prior knowledge into policy and value networks using an equivariance constraint, we can reduce the size of the solution space. We specifically focus on group-structured symmetries (invertible transformations). Additionally, we introduce an easy method for constructing equivariant network layers numerically, so the system designer need not solve the constraints by hand, as is typically done. We construct MDP homomorphic MLPs and CNNs that are equivariant under either a group of reflections or rotations. We show that such networks converge faster than unstructured baselines on CartPole, a grid world and Pong.</p>
<p><strong>CoinDICE: Off-Policy Confidence Interval Estimation</strong></p>
<blockquote>
<p>ID 17909<br>Bo Dai (Google Brain) · Ofir Nachum (Google Brain) · Yinlam Chow (Google Research) · Lihong Li (Google Research) · Csaba Szepesvari (DeepMind / University of Alberta) · Dale Schuurmans (Google Brain &amp; University of Alberta)</p>
</blockquote>
<p>We study high-confidence behavior-agnostic off-policy evaluation in reinforcement learning, where the goal is to estimate a confidence interval on a target policy’s value, given only access to a static experience dataset collected by unknown behavior policies. Starting from a function space embedding of the linear program formulation of the Q-function, we obtain an optimization problem with generalized estimating equation constraints. By applying the generalized empirical likelihood method to the resulting Lagrangian, we propose CoinDICE, a novel and efficient algorithm for computing confidence intervals. Theoretically, we prove the obtained confidence intervals are valid, in both asymptotic and finite-sample regimes. Empirically, we show in a variety of benchmarks that the confidence interval estimates are tighter and more accurate than existing methods.</p>
<p><strong>On Efficiency in Hierarchical Reinforcement Learning</strong></p>
<blockquote>
<p>ID 17932<br>Zheng Wen (DeepMind) · Doina Precup (DeepMind) · Morteza Ibrahimi (DeepMind) · Andre Barreto (DeepMind) · Benjamin Van Roy (Stanford University) · Satinder Singh (DeepMind)</p>
</blockquote>
<p>Hierarchical Reinforcement Learning (HRL) approaches promise to provide more efficient solutions to sequential decision making problems, both in terms of statistical as well as computational efficiency. While this has been demonstrated empirically over time in a variety of tasks, theoretical results quantifying the benefits of such methods are still few and far between. In this paper, we discuss the kind of structure in a Markov decision process which gives rise to efficient HRL methods. Specifically, we formalize the intuition that HRL can exploit well repeating “subMDPs”, with similar reward and transition structure. We show that, under reasonable assumptions, a model-based Thompson sampling-style HRL algorithm that exploits this structure is statistically efficient, as established through a finite-time regret bound. We also establish conditions under which planning with structure-induced options is near-optimal and computationally efficient.</p>
<p><strong>Variational Policy Gradient Method for Reinforcement Learning with General Utilities</strong></p>
<blockquote>
<p>ID 17942<br>Junyu Zhang (Princeton University) · Alec Koppel (U.S. Army Research Laboratory) · Amrit Singh Bedi (US Army Research Laboratory) · Csaba Szepesvari (DeepMind / University of Alberta) · Mengdi Wang (Princeton University)</p>
</blockquote>
<p>In recent years, reinforcement learning systems with general goals beyond a cumulative sum of rewards have gained traction, such as in constrained problems, exploration, and acting upon prior experiences. In this paper, we consider policy optimization in Markov Decision Problems, where the objective is a general utility function of the state-action occupancy measure, which subsumes several of the aforementioned examples as special cases. Such generality invalidates the Bellman equation. As this means that dynamic programming no longer works, we focus on direct policy search. Analogously to the Policy Gradient Theorem \cite{sutton2000policy} available for RL with cumulative rewards, we derive a new Variational Policy Gradient Theorem for RL with general utilities, which establishes that the gradient may be obtained as the solution of a stochastic saddle point problem involving the Fenchel dual of the utility function. We develop a variational Monte Carlo gradient estimation algorithm to compute the policy gradient based on sample paths. Further, we prove that the variational policy gradient scheme converges globally to the optimal policy for the general objective, and we also establish its rate of convergence that matches or improves the convergence rate available in the case of RL with cumulative rewards.</p>
<p><strong>A Finite-Time Analysis of Two Time-Scale Actor-Critic Methods</strong></p>
<blockquote>
<p>ID 17944<br>Yue Wu (University of California, Los Angeles) · Weitong ZHANG (University of California, Los Angeles) · Pan Xu (University of California, Los Angeles) · Quanquan Gu (UCLA)</p>
</blockquote>
<p>Actor-critic (AC) methods have exhibited great empirical success compared with other reinforcement learning algorithms, where the actor uses the policy gradient to improve the learning policy and the critic uses temporal difference learning to estimate the policy gradient. Under the two time-scale learning rate schedule, the asymptotic convergence of AC has been well studied in the literature.However, the non-asymptotic convergence and finite sample complexity of actor-critic methods are largely open.In this work, we provide a non-asymptotic analysis for two time-scale actor-critic methods under non-i.i.d. setting. We prove that the actor-critic method is guaranteed to find a first-order stationary point (i.e., $|\nabla J(\bm{\theta})|_2^2 \le \epsilon$) of the non-concave performance function $J(\bm{\theta})$, with $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ sample complexity. To the best of our knowledge, this is the first work providing finite-time analysis and sample complexity bound for two time-scale actor-critic methods.</p>
<p><strong>POLY-HOOT: Monte-Carlo Planning in Continuous Space MDPs with Non-Asymptotic Analysis</strong></p>
<blockquote>
<p>ID 17945<br>Weichao Mao (University of Illinois Urbana-Champaign) · Kaiqing Zhang (University of Illinois at Urbana-Champaign (UIUC)) · Qiaomin Xie (Cornell University) · Tamer Basar (University of Illinois at Urbana-Champaign)</p>
</blockquote>
<p>Monte-Carlo planning, as exemplified by Monte-Carlo Tree Search (MCTS), has demonstrated remarkable performance in applications with finite spaces. In this paper, we consider Monte-Carlo planning in an environment with continuous state-action spaces, a much less understood problem with important applications in control and robotics. We introduce POLY-HOOT, an algorithm that augments MCTS with a continuous armed bandit strategy named Hierarchical Optimistic Optimization (HOO) (Bubeck et al., 2011). Specifically, we enhance HOO by using an appropriate polynomial, rather than logarithmic, bonus term in the upper confidence bounds. Such a polynomial bonus is motivated by its empirical successes in AlphaGo Zero (Silver et al., 2017b), as well as its significant role in achieving theoretical guarantees of finite space MCTS (Shah et al., 2019). We investigate, for the first time, the regret of the enhanced HOO algorithm in non-stationary bandit problems. Using this result as a building block, we establish non-asymptotic convergence guarantees for POLY-HOOT: the value estimate converges to an arbitrarily small neighborhood of the optimal value function at a polynomial rate. We further provide experimental results that corroborate our theoretical findings.</p>
<p><strong>Model-based Reinforcement Learning for Semi-Markov Decision Processes with Neural ODEs</strong></p>
<blockquote>
<p>ID 17959<br>Jianzhun Du (Harvard University) · Joseph Futoma (Harvard University) · Finale Doshi-Velez (Harvard)</p>
</blockquote>
<p>We present two elegant solutions for modeling continuous-time dynamics, in a novel model-based reinforcement learning (RL) framework for semi-Markov decision processes (SMDPs), using neural ordinary differential equations (ODEs). Our models accurately characterize continuous-time dynamics and enable us to develop high-performing policies using a small amount of data. We also develop a model-based approach for optimizing time schedules to reduce interaction rates with the environment while maintaining the near-optimal performance, which is not possible for model-free methods. We experimentally demonstrate the efficacy of our methods across various continuous-time domains.</p>
<p><strong>Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction</strong></p>
<blockquote>
<p>ID 17971<br>Gen Li (Tsinghua University) · Yuting Wei (Carnegie Mellon University) · Yuejie Chi (CMU) · Yuantao Gu (Tsinghua University) · Yuxin Chen (Princeton University)</p>
</blockquote>
<p>Asynchronous Q-learning aims to learn the optimal action-value function (or Q-function) of a Markov decision process (MDP), based on a single trajectory of Markovian samples induced by a behavior policy.  Focusing on a $\gamma$-discounted MDP with state space S and action space A, we demonstrate that the $ \ell_{\infty} $-based sample complexity of classical asynchronous Q-learning —- namely, the number of samples needed to yield an entrywise $\epsilon$-accurate estimate of the Q-function —- is at most on the order of $ \frac{1}{ \mu_{\min}(1-\gamma)^5 \epsilon^2 }+ \frac{ t_{\mathsf{mix}} }{ \mu_{\min}(1-\gamma) } $ up to some logarithmic factor, provided that a proper constant learning rate is adopted. Here, $ t_{\mathsf{mix}} $ and $ \mu_{\min} $ denote respectively the mixing time and the minimum state-action occupancy probability of the sample trajectory. The first term of this bound matches the complexity in the case with independent samples drawn from the stationary distribution of  the trajectory. The second term reflects the expense taken for the empirical distribution of the Markovian trajectory to reach a steady state, which is incurred at the very beginning and becomes amortized as the algorithm runs. Encouragingly, the above bound improves upon the state-of-the-art result by a factor of at least |S||A|.  Further, the scaling on the discount complexity can be improved by means of variance reduction.</p>
<p><strong>Reinforcement Learning with Augmented Data</strong></p>
<blockquote>
<p>ID 18015<br>Misha Laskin (UC Berkeley) · Kimin Lee (UC Berkeley) · Adam Stooke (UC Berkeley) · Lerrel Pinto (New York University) · Pieter Abbeel (UC Berkeley &amp; covariant.ai) · Aravind Srinivas (UC Berkeley)</p>
</blockquote>
<p>Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks.</p>
<p><strong>Improved Sample Complexity for Incremental Autonomous Exploration in MDPs</strong></p>
<blockquote>
<p>ID 18017<br>Jean Tarbouriech (Facebook AI Research Paris &amp; Inria Lille) · Matteo Pirotta (Facebook AI Research) · Michal Valko (DeepMind Paris and Inria Lille - Nord Europe) · Alessandro Lazaric (Facebook Artificial Intelligence Research)</p>
</blockquote>
<p>We study the problem of exploring an unknown environment when no reward function is provided to the agent. Building on the incremental exploration setting introduced by Lim and Auer (2012), we define the objective of learning the set of $\epsilon$-optimal goal-conditioned policies attaining all states that are incrementally reachable within $L$ steps (in expectation) from a reference state $s_0$. In this paper, we introduce a novel model-based approach that interleaves discovering new states from $s_0$ and improving the accuracy of a model estimate that is used to compute goal-conditioned policies. The resulting algorithm, DisCo, achieves a sample complexity scaling as $\widetilde{O}_{\epsilon}(L^5 S_{L+\epsilon} \Gamma_{L+\epsilon} A \epsilon^{-2})$, where $A$ is the number of actions, $S_{L+\epsilon}$ is the number of states that are incrementally reachable from $s_0$ in $L+\epsilon$ steps, and $\Gamma_{L+\epsilon}$ is the branching factor of the dynamics over such states. This improves over the algorithm proposed in (Lim and Auer, 2012) in both $\epsilon$ and $L$ at the cost of an extra $\Gamma_{L+\epsilon}$ factor, which is small in most environments of interest. Furthermore, DisCo is the first algorithm that can return an $\epsilon/c_{\min}$-optimal policy for any cost-sensitive shortest-path problem defined on the $L$-reachable states with minimum cost $c_{\min}$. Finally, we report preliminary empirical results confirming our theoretical findings.</p>
<p><strong>EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning</strong></p>
<blockquote>
<p>ID 18023<br>Jiachen Li (University of California, Berkeley) · Fan Yang (University of California, Berkeley) · Masayoshi Tomizuka (University of California, Berkeley) · Chiho Choi (Honda Research Institute US)</p>
</blockquote>
<p>Multi-agent interacting systems are prevalent in the world, from purely physical systems to complicated social dynamic systems. In many applications, effective understanding of the situation and accurate trajectory prediction of interactive agents play a significant role in downstream tasks, such as decision making and planning. In this paper, we propose a generic trajectory forecasting framework (named EvolveGraph) with explicit relational structure recognition and prediction via latent interaction graphs among multiple heterogeneous, interactive agents. Considering the uncertainty of future behaviors, the model is designed to provide multi-modal prediction hypotheses. Since the underlying interactions may evolve even with abrupt changes, and different modalities of evolution may lead to different outcomes, we address the necessity of dynamic relational reasoning and adaptively evolving the interaction graphs. We also introduce a double-stage training pipeline which not only improves training efficiency and accelerates convergence, but also enhances model performance. The proposed framework is evaluated on both synthetic physics simulations and multiple real-world benchmark datasets in various areas. The experimental results illustrate that our approach achieves state-of-the-art performance in terms of prediction accuracy.</p>
<p><strong>Autofocused oracles for model-based design</strong></p>
<blockquote>
<p>ID 18029<br>Clara Fannjiang (UC Berkeley) · Jennifer Listgarten (UC Berkeley)</p>
</blockquote>
<p>Data-driven design is making headway into a number of application areas, including protein, small-molecule, and materials engineering. The design goal is to construct an object with desired properties, such as a protein that binds to a therapeutic target, or a superconducting material with a higher critical temperature than previously observed. To that end, costly experimental measurements are being replaced with calls to high-capacity regression models trained on labeled data, which can be leveraged in an in silico search for design candidates. However, the design goal necessitates moving into regions of the design space beyond where such models were trained. Therefore, one can ask: should the regression model be altered as the design algorithm explores the design space, in the absence of new data? Herein, we answer this question in the affirmative. In particular, we (i) formalize the data-driven design problem as a non-zero-sum game, (ii) develop a principled strategy for retraining the regression model as the design algorithm proceeds—-what we refer to as autofocusing, and (iii) demonstrate the promise of autofocusing empirically.</p>
<p><strong>Off-Policy Evaluation via the Regularized Lagrangian</strong></p>
<blockquote>
<p>ID 18037<br>Mengjiao Yang (Google) · Ofir Nachum (Google Brain) · Bo Dai (Google Brain) · Lihong Li (Google Research) · Dale Schuurmans (Google Brain &amp; University of Alberta)</p>
</blockquote>
<p>The recently proposed distribution correction estimation (DICE) family of estimators has advanced the state of the art in off-policy evaluation from behavior-agnostic data. While these estimators all perform some form of stationary distribution correction, they arise from different derivations and objective functions. In this paper, we unify these estimators as regularized Lagrangians of the same linear program. The unification allows us to expand the space of DICE estimators to new alternatives that demonstrate improved performance. More importantly, by analyzing the expanded space of estimators both mathematically and empirically we find that dual solutions offer greater flexibility in navigating the tradeoff between optimization stability and estimation bias, and generally provide superior estimates in practice.</p>
<p><strong>Reinforcement Learning with Combinatorial Actions: An Application to Vehicle Routing</strong></p>
<blockquote>
<p>ID 18073<br>Arthur Delarue (MIT) · Ross Anderson (Google Research) · Christian Tjandraatmadja (Google)</p>
</blockquote>
<p>Value-function-based methods have long played an important role in reinforcement learning. However, finding the best next action given a value function of arbitrary complexity is nontrivial when the action space is too large for enumeration. We develop a framework for value-function-based deep reinforcement learning with a combinatorial action space, in which the action selection problem is explicitly formulated as a mixed-integer optimization problem. As a motivating example, we present an application of this framework to the capacitated vehicle routing problem (CVRP), a combinatorial optimization problem in which a set of locations must be covered by a single vehicle with limited capacity. On each instance, we model an action as the construction of a single route, and consider a deterministic policy which is improved through a simple policy iteration algorithm.  Our approach is competitive with other reinforcement learning methods and achieves an average gap of 1.7% with state-of-the-art OR methods on standard library instances of medium size.</p>
<p><strong>MOPO: Model-based Offline Policy Optimization</strong></p>
<blockquote>
<p>ID 18076<br>Tianhe Yu (Stanford University) · Garrett W. Thomas (Stanford University) · Lantao Yu (Stanford University) · Stefano Ermon (Stanford) · James Zou (Stanford University) · Sergey Levine (UC Berkeley) · Chelsea Finn (Stanford) · Tengyu Ma (Stanford University)</p>
</blockquote>
<p>Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a batch of previously collected data. This problem setting is compelling, because it offers the promise of utilizing large, diverse, previously collected datasets to acquire policies without any costly or dangerous active exploration, but it is also exceptionally difficult, due to the distributional shift between the offline training data and the learned policy. While there has been significant progress in model-free offline RL, the most successful prior methods constrain the policy to the support of the data, precluding generalization to new states. In this paper, we observe that an existing model-based RL algorithm on its own already produces significant gains in the offline setting, as compared to model-free approaches, despite not being designed for this setting. However, although many standard model-based RL methods already estimate the uncertainty of their model, they do not by themselves provide a mechanism to avoid the issues associated with distributional shift in the offline setting. We therefore propose to modify existing model-based RL methods to address these issues by casting offline model-based RL into a penalized MDP framework. We theoretically show that, by using this penalized MDP, we are maximizing a lower bound of the return in the true MDP. Based on our theoretical results, we propose a new model-based offline RL algorithm that applies the variance of a Lipschitz-regularized model as a penalty to the reward function. We find that this algorithm outperforms both standard model-based RL methods and existing state-of-the-art model-free offline RL approaches on existing offline RL benchmarks, as well as two challenging continuous control tasks that require generalizing from data collected for a different task.</p>
<p><strong>Variance-Reduced Off-Policy TDC Learning: Non-Asymptotic Convergence Analysis</strong></p>
<blockquote>
<p>ID 18081<br>Shaocong Ma (University of Utah) · Yi Zhou (University of Utah) · Shaofeng Zou (University at Buffalo, the State University of New York)</p>
</blockquote>
<p>Variance reduction techniques have been successfully applied to temporal-difference (TD) learning and help to improve the sample complexity in policy evaluation. However, the existing work applied variance reduction to either the less popular one time-scale TD algorithm or the two time-scale GTD algorithm but with a finite number of i.i.d.\ samples, and both algorithms apply to only the on-policy setting. In this work, we develop a variance reduction scheme for the two time-scale TDC algorithm in the off-policy setting and analyze its non-asymptotic convergence rate over both i.i.d.\ and Markovian samples. In the i.i.d setting, our algorithm achieves an improved sample complexity $\calO(\epsilon^{-\frac{3}{5}} \log{\epsilon}^{-1})$ over the state-of-the-art result $\calO(\epsilon^{-1} \log {\epsilon}^{-1})$. In the Markovian setting, our algorithm achieves the state-of-the-art sample complexity $\calO(\epsilon^{-1} \log {\epsilon}^{-1})$ that is near-optimal. Experiments demonstrate that the proposed variance-reduced TDC achieves a smaller asymptotic convergence error than both the conventional TDC and the variance-reduced TD.</p>
<p><strong>DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction</strong></p>
<blockquote>
<p>ID 18124<br>Aviral Kumar (UC Berkeley) · Abhishek Gupta (University of California, Berkeley) · Sergey Levine (UC Berkeley)</p>
</blockquote>
<p>Deep reinforcement learning can learn effective policies for a wide range of tasks, but is notoriously difficult to use due to instability and sensitivity to hyperparameters. The reasons for this remain unclear. In this paper, we study how RL methods based on bootstrapping-based Q-learning can suffer from a pathological interaction between function approximation and the data distribution used to train the Q-function: with standard supervised learning, online data collection should induce corrective feedback, where new data corrects mistakes in old predictions. With dynamic programming methods like Q-learning, such feedback may be absent. This can lead to potential instability, sub-optimal convergence, and poor results when learning from noisy, sparse or delayed rewards. Based on these observations, we propose a new algorithm, DisCor, which explicitly optimizes for data distributions that can correct for accumulated errors in the value function. DisCor computes a tractable approximation to the distribution that optimally induces corrective feedback, which we show results in reweighting samples based on the estimated accuracy of their target values. Using this distribution for training, DisCor results in substantial improvements in a range of challenging RL settings, such as multi-task learning and learning from noisy reward signals.</p>
<p><strong>FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs</strong></p>
<blockquote>
<p>ID 18128<br>Alekh Agarwal (Microsoft Research) · Sham Kakade (University of Washington) · Akshay Krishnamurthy (Microsoft) · Wen Sun (Microsoft Research NYC)</p>
</blockquote>
<p>In order to deal with the curse of dimensionality in reinforcement learning (RL), it is common practice to make parametric assumptions where values or policies are functions of some low dimensional feature space. This work focuses on the representation learning question: how can we learn such features? Under the assumption that the underlying (unknown) dynamics correspond to a low rank transition matrix, we show how the representation learning question is related to a particular non-linear matrix decomposition problem. Structurally, we make precise connections between these low rank MDPs and latent variable models, showing how they significantly generalize prior formulations, such as block MDPs, for representation learning in RL. Algorithmically, we develop FLAMBE, which engages in exploration and representation learning for provably efficient RL in low rank transition models. On a technical level, our analysis eliminates reachability assumptions that appear in prior results on the simpler block MDP model and may be of independent interest.</p>
<p><strong>Neurosymbolic Reinforcement Learning with Formally Verified Exploration</strong></p>
<blockquote>
<p>ID 18130<br>Greg Anderson (University of Texas at Austin) · Abhinav Verma (Rice University) · Isil Dillig (UT Austin) · Swarat Chaudhuri (The University of Texas at Austin)</p>
</blockquote>
<p>We present REVEL, a partially neural reinforcement learning (RL) framework for provably  safe exploration in continuous state and action spaces. A key challenge for provably safe deep RL is that repeatedly verifying neural networks within a learning loop is computationally infeasible. We address this challenge using two policy classes: a general, neurosymbolic class with approximate gradients and a more restricted class of symbolic policies that allows efficient verification. Our learning algorithm is a mirror descent over policies: in each iteration, it safely lifts a symbolic policy into the neurosymbolic space, performs safe gradient updates to the resulting policy, and projects the updated policy into the safe symbolic subset, all without requiring explicit verification of neural networks. Our empirical results show that REVEL enforces safe exploration in many scenarios in which Constrained Policy Optimization does not, and that it can discover policies that outperform those learned through prior approaches to verified exploration.</p>
<p><strong>Generalized Hindsight for Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18136<br>Alexander Li (UC Berkeley) · Lerrel Pinto (New York University) · Pieter Abbeel (UC Berkeley &amp; covariant.ai)</p>
</blockquote>
<p>One of the key reasons for the high sample complexity in reinforcement learning (RL) is the inability to transfer knowledge from one task to another. In standard multi-task RL settings, low-reward data collected while trying to solve one task provides little to no signal for solving that particular task and is hence effectively wasted. However, we argue that this data, which is uninformative for one task, is likely a rich source of information for other tasks. To leverage this insight and efficiently reuse data, we present Generalized Hindsight: an approximate inverse reinforcement learning technique for relabeling behaviors with the right tasks. Intuitively, given a behavior generated under one task, Generalized Hindsight returns a different task that the behavior is better suited for. Then, the behavior is relabeled with this new task before being used by an off-policy RL optimizer. Compared to standard relabeling techniques, Generalized Hindsight provides a substantially more efficient re-use of samples, which we empirically demonstrate on a suite of multi-task navigation and manipulation tasks.</p>
<p><strong>Finite-Time Analysis for Double Q-learning</strong></p>
<blockquote>
<p>ID 18160<br>Huaqing Xiong (Ohio State University) · Lin Zhao (National University of Singapore) · Yingbin Liang (The Ohio State University) · Wei  Zhang (Southern University of Science and Technology)</p>
</blockquote>
<p>Although Q-learning is one of the most successful algorithms for finding the best action-value function (and thus the optimal policy) in reinforcement learning, its implementation often suffers from large overestimation of Q-function values incurred by random sampling. The double Q-learning algorithm proposed in~\citet{hasselt2010double} overcomes such an overestimation issue by randomly switching the update between two Q-estimators, and has thus gained significant popularity in practice. However, the theoretical understanding of double Q-learning is rather limited. So far only the asymptotic convergence has been established, which does not characterize how fast the algorithm converges. In this paper, we provide the first non-asymptotic (i.e., finite-time) analysis for double Q-learning. We show that both synchronous and asynchronous double Q-learning are guaranteed to converge to an $\epsilon$-accurate neighborhood of the global optimum by taking$\tilde{\Omega}\left(\left( \frac{1}{(1-\gamma)^6\epsilon^2}\right)^{\frac{1}{\omega}} +\left(\frac{1}{1-\gamma}\right)^{\frac{1}{1-\omega}}\right)$ iterations, where $\omega\in(0,1)$ is the decay parameter of the learning rate, and $\gamma$ is the discount factor. Our analysis develops novel techniques to derive finite-time bounds on the difference between two inter-connected stochastic processes, which is new to the literature of stochastic approximation.</p>
<p><strong>Subgroup-based Rank-1 Lattice Quasi-Monte Carlo</strong></p>
<blockquote>
<p>ID 18168<br>Yueming LYU (University of Technology Sydney) · Yuan Yuan (MIT) · Ivor Tsang (University of Technology, Sydney)</p>
</blockquote>
<p>Quasi-Monte Carlo (QMC) is an essential tool for integral approximation,  Bayesian inference, and sampling for simulation in science, etc. In the QMC area, the rank-1 lattice is important due to its simple operation, and nice property for point set construction.  However, the construction of the  generating vector of the rank-1 lattice is usually time-consuming through an exhaustive computer search.  To address this issue,  we propose a simple closed-form rank-1 lattice construction method based on group theory.  Our method reduces the number of distinct pairwise distance values to generate a more regular lattice. We theoretically prove a lower and an upper bound of the minimum pairwise distance of any non-degenerate rank-1 lattice. Empirically, our methods can generate near-optimal rank-1 lattice compared with Korobov exhaustive search regarding the $l_1$-norm and $l_2$-norm minimum distance. Moreover, experimental results show that our method achieves superior approximation performance on the benchmark integration test problems and the kernel approximation problems.</p>
<p><strong>Meta-Gradient Reinforcement Learning with an Objective Discovered Online</strong></p>
<blockquote>
<p>ID 18175<br>Zhongwen Xu (DeepMind) · Hado van Hasselt (DeepMind) · Matteo Hessel (Google DeepMind) · Junhyuk Oh (DeepMind) · Satinder Singh (DeepMind) · David Silver (DeepMind)</p>
</blockquote>
<p>Deep reinforcement learning includes a broad family of algorithms that parameterise an internal representation, such as a value function or policy, by a deep neural network. Each algorithm optimises its parameters with respect to an objective, such as Q-learning or policy gradient, that defines its semantics. In this work, we propose an algorithm based on meta-gradient descent that discovers its own objective, flexibly parameterised by a deep neural network, solely from interactive experience with its environment. Over time, this allows the agent to learn how to learn increasingly effectively. Furthermore, because the objective is discovered online, it can adapt to changes over time. We demonstrate that the algorithm discovers how to address several important issues in RL, such as bootstrapping, non-stationarity, and off-policy learning. On the Atari Learning Environment, the meta-gradient algorithm adapts over time to learn with greater efficiency, eventually outperforming the median score of a strong actor-critic baseline.</p>
<p><strong>TorsionNet: A Reinforcement Learning Approach to Sequential Conformer Search</strong></p>
<blockquote>
<p>ID 18176<br>Tarun Gogineni (University of Michigan) · Ziping Xu (University of Michigan) · Exequiel  Punzalan (University of Michigan) · Runxuan Jiang (University of Michigan) · Joshua Kammeraad (University of Michigan) · Ambuj Tewari (University of Michigan) · Paul Zimmerman (University of Michigan)</p>
</blockquote>
<p>Molecular geometry prediction of flexible molecules, or conformer search, is a long-standing challenge in computational chemistry. This task is of great importance for predicting structure-activity relationships for a wide variety of substances ranging from biomolecules to ubiquitous materials. Substantial computational resources are invested in Monte Carlo and Molecular Dynamics methods to generate diverse and representative conformer sets for medium to large molecules, which are yet intractable to chemoinformatic conformer search methods. We present TorsionNet, an efficient sequential conformer search technique based on reinforcement learning under the rigid rotor approximation. The model is trained via curriculum learning, whose theoretical benefit is explored in detail, to maximize a novel metric grounded in thermodynamics called the Gibbs Score. Our experimental results show that TorsionNet outperforms the highest-scoring chemoinformatics method by 4x on large branched alkanes, and by several orders of magnitude on the previously unexplored biopolymer lignin, with applications in renewable energy. TorsionNet also outperforms the far more exhaustive but computationally intensive Self-Guided Molecular Dynamics sampling method.</p>
<p><strong>Succinct and Robust Multi-Agent Communication With Temporal Message Control</strong></p>
<blockquote>
<p>ID 18225<br>Sai Qian Zhang (Harvard University) · Qi  Zhang (Amazon) · Jieyu Lin (University of Toronto)</p>
</blockquote>
<p>Recent studies have shown that introducing communication between agents can significantly improve overall performance in cooperative Multi-agent reinforcement learning (MARL). However, existing communication schemes often require agents to exchange an excessive number of messages at run-time under a reliable communication channel, which hinders its practicality in many real-world situations. In this paper, we present \textit{Temporal Message Control} (TMC), a simple yet effective approach for achieving succinct and robust communication in MARL. TMC applies a temporal smoothing technique to drastically reduce the amount of information exchanged between agents. Experiments show that TMC can significantly reduce inter-agent communication overhead without impacting accuracy. Furthermore, TMC demonstrates much better robustness against transmission loss than existing approaches in lossy networking environments.</p>
<p><strong>Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18230<br>Cong Zhang (Nanyang Technological University) · Wen Song (Institute of Marine Scinece and Technology, Shandong University) · Zhiguang Cao (National University of Singapore) · Jie Zhang (Nanyang Technological University) · Puay Siew Tan (SIMTECH) · Xu Chi (Singapore Institute of Manufacturing Technology, A-Star)</p>
</blockquote>
<p>Priority dispatching rule (PDR) is widely used for solving real-world Job-shop scheduling problem (JSSP). However, the design of effective PDRs is a tedious task, requiring a myriad of specialized knowledge and often delivering limited performance. In this paper, we propose to automatically learn PDRs via an end-to-end deep reinforcement learning agent. We exploit the disjunctive graph representation of JSSP, and propose a Graph Neural Network based scheme to embed the states encountered during solving. The resulting policy network is size-agnostic, effectively enabling generalization on large-scale instances. Experiments show that the agent can learn high-quality PDRs from scratch with elementary raw features, and demonstrates strong performance against the best existing PDRs. The learned policies also perform well on much larger instances that are unseen in training.</p>
<p><strong>Is Plug-in Solver Sample-Efficient for Feature-based Reinforcement Learning?</strong></p>
<blockquote>
<p>ID 18245<br>Qiwen Cui (Peking University) · Lin Yang (UCLA)</p>
</blockquote>
<p>It is believed that a model-based approach for reinforcement learning (RL) is the key to reduce sample complexity. However, the understanding of the sample optimality of model-based RL is still largely missing, even for the linear case. This work considers sample complexity of finding an $\epsilon$-optimal policy in a Markov decision process (MDP) that admits a linear additive feature representation, given only access to a generative model. We solve this problem via a plug-in solver approach, which builds an empirical model and plans in this empirical model via an arbitrary plug-in solver. We prove that under the anchor-state assumption, which implies implicit non-negativity in the feature space, the minimax sample complexity of finding an $\epsilon$-optimal policy in a $\gamma$-discounted MDP is $O(K/(1-\gamma)^3\epsilon^2)$, which only depends on the dimensionality $K$ of the feature space and has no dependence on the state or action space. We further extend our results to a relaxed setting where anchor-states may not exist and show that a plug-in approach can be sample efficient as well, providing a flexible approach to design model-based algorithms for RL.</p>
<p><strong>Instance-based Generalization in Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18255<br>Martin Bertran (Duke University) · Natalia L Martinez (Duke University) · Mariano Phielipp (Intel AI Labs) · Guillermo Sapiro (Duke University)</p>
</blockquote>
<p>Agents trained via deep reinforcement learning (RL) routinely fail to generalize to unseen environments, even when these share the same underlying dynamics as the training levels. Understanding the generalization properties of RL is one of the challenges of modern machine learning. Towards this goal, we analyze policy learning in the context of Partially Observable Markov Decision Processes (POMDPs) and formalize the dynamics of training levels as instances. We prove that, independently of the exploration strategy, reusing instances introduces signiﬁcant changes on the effective Markov dynamics the agent observes during training. Maximizing expected rewards impacts the learned belief state of the agent by inducing undesired instance-speciﬁc speed-running policies instead of generalizable ones, which are sub-optimal on the training set. We provide generalization bounds to the value gap in train and test environments based on the number of training instances, and use insights based on these to improve performance on unseen levels. We propose training a shared belief representation over an ensemble of specialized policies, from which we compute a consensus policy that is used for data collection, disallowing instance-speciﬁc exploitation. We experimentally validate our theory, observations, and the proposed computational solution over the CoinRun benchmark.</p>
<p><strong>Preference-based Reinforcement Learning with Finite-Time Guarantees</strong></p>
<blockquote>
<p>ID 18267<br>Yichong Xu (Carnegie Mellon University) · Ruosong Wang (Carnegie Mellon University) · Lin Yang (UCLA) · Aarti Singh (CMU) · Artur Dubrawski (Carnegie Mellon University)</p>
</blockquote>
<p>Preference-based Reinforcement Learning (PbRL) replaces reward values in traditional reinforcement learning by preferences to better elicit human opinion on the target objective, especially when numerical reward values are hard to design or interpret. Despite promising results in applications, the theoretical understanding of PbRL is still in its infancy. In this paper, we present the first finite-time analysis for general PbRL problems.We first show that a unique optimal policy may not exist if preferences over trajectories are deterministic for PbRL.  If preferences are stochastic, and the preference probability relates to the hidden reward values, we present algorithms for PbRL, both with and without a simulator, that are able to identify the best policy up to accuracy $\varepsilon$ with high probability. Our method explores the state space by navigating to  under-explored states, and solves PbRL using a combination of dueling bandits and policy search. Experiments show the efficacy of our method when it is applied to real-world problems.</p>
<p><strong>Learning to Decode: Reinforcement Learning for Decoding of Sparse Graph-Based Channel Codes</strong></p>
<blockquote>
<p>ID 18275<br>Salman Habib (New Jersey Institute of Tech) · Allison Beemer (New Jersey Institute of Technology) · Joerg Kliewer (New Jersey Institute of Technology)</p>
</blockquote>
<p>We show in this work that reinforcement learning can be successfully applied to decoding short to moderate length sparse graph-based channel codes. Specifically, we focus on low-density parity check (LDPC) codes, which for example have been standardized in the context of 5G cellular communication systems due to their excellent error correcting performance. These codes are typically decoded via belief propagation iterative decoding on the corresponding bipartite (Tanner) graph of the code via flooding, i.e., all check and variable nodes in the Tanner graph are updated at once. In contrast, in this paper we utilize a sequential update policy which selects the optimum check node (CN) scheduling in order to improve decoding performance. In particular, we model the CN update process as a multi-armed bandit process with dependent arms and employ a Q-learning scheme for optimizing the CN scheduling policy. In order to reduce the learning complexity, we propose a novel graph-induced CN clustering approach to partition the state space in such a way that dependencies between clusters are minimized. Our results show that compared to other decoding approaches from the literature, the proposed reinforcement learning scheme not only significantly improves the decoding performance, but also reduces the decoding complexity dramatically oncethe scheduling policy is learned.</p>
<p><strong>BAIL: Best-Action Imitation Learning for Batch Deep Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18290<br>Xinyue Chen (NYU Shanghai) · Zijian Zhou (NYU Shanghai) · Zheng Wang (NYU Shanghai) · Che Wang (New York University) · Yanqiu Wu (New York University) · Keith Ross (NYU Shanghai)</p>
</blockquote>
<p>There has recently been a surge in research in batch Deep Reinforcement Learning (DRL), which aims for learning a high-performing policy from a given dataset without additional interactions with the environment. We propose a new algorithm, Best-Action Imitation Learning (BAIL), which strives for both simplicity and performance. BAIL learns a V function, uses the V function to select actions it believes to be high-performing, and then uses those actions to train a policy network using imitation learning. For the MuJoCo benchmark, we provide a comprehensive experimental study of BAIL, comparing its performance to four other batch Q-learning and imitation-learning schemes for a large variety of batch datasets. Our experiments show that BAIL’s performance is much higher than the other schemes, and is also computationally much faster than the batch Q-learning schemes.</p>
<p><strong>Task-Agnostic Online Reinforcement Learning with an Infinite Mixture of Gaussian Processes</strong></p>
<blockquote>
<p>ID 18293<br>Mengdi Xu (Carnegie Mellon University) · Wenhao Ding (Carnegie Mellon University) · Jiacheng Zhu (Carnegie Mellon University) · ZUXIN LIU (Carnegie Mellon University) · Baiming Chen (Tsinghua University) · Ding Zhao (Carnegie Mellon University)</p>
</blockquote>
<p>Continuously learning to solve unseen tasks with limited experience has been extensively pursued in meta-learning and continual learning, but with restricted assumptions such as accessible task distributions, independently and identically distributed tasks, and clear task delineations. However, real-world physical tasks frequently violate these assumptions, resulting in performance degradation. This paper proposes a continual online model-based reinforcement learning approach that does not require pre-training to solve task-agnostic problems with unknown task boundaries. We maintain a mixture of experts to handle nonstationarity, and represent each different type of dynamics with a Gaussian Process to efficiently leverage collected data and expressively model uncertainty. We propose a transition prior to account for the temporal dependencies in streaming data and update the mixture online via sequential variational inference. Our approach reliably handles the task distribution shift by generating new models for never-before-seen dynamics and reusing old models for previously seen dynamics. In experiments, our approach outperforms alternative methods in non-stationary tasks, including classic control with changing dynamics and decision making in different driving scenarios.</p>
<p><strong>On Reward-Free Reinforcement Learning with Linear Function Approximation</strong></p>
<blockquote>
<p>ID 18304<br>Ruosong Wang (Carnegie Mellon University) · Simon Du (Institute for Advanced Study) · Lin Yang (UCLA) · Russ Salakhutdinov (Carnegie Mellon University)</p>
</blockquote>
<p>Reward-free reinforcement learning (RL) is a framework which is suitable for both the batch RL setting and the setting where there are many reward functions of interest. During the exploration phase, an agent collects samples without using a pre-specified reward function. After the exploration phase, a reward function is given, and the agent uses samples collected during the exploration phase to compute a near-optimal policy. Jin et al. [2020] showed that in the tabular setting, the agent only needs to collect polynomial number of samples (in terms of the number states, the number of actions, and the planning horizon) for reward-free RL. However, in practice, the number of states and actions can be large, and thus function approximation schemes are required for generalization. In this work, we give both positive and negative results for reward-free RL with linear function approximation. We give an algorithm for reward-free RL in the linear Markov decision process setting where both the transition and the reward admit linear representations. The sample complexity of our algorithm is polynomial in the feature dimension and the planning horizon, and is completely independent of the number of states and actions. We further give an exponential lower bound for reward-free RL in the setting where only the optimal $Q$-function admits a linear representation. Our results imply several interesting exponential separations on the sample complexity of reward-free RL.</p>
<p><strong>Near-Optimal Reinforcement Learning with Self-Play</strong></p>
<blockquote>
<p>ID 18322<br>Yu Bai (Salesforce Research) · Chi Jin (Princeton University) · Tiancheng Yu (MIT )</p>
</blockquote>
<p>This paper considers the problem of designing optimal algorithms for reinforcement learning in two-player zero-sum games. We focus on self-play algorithms which learn the optimal policy by playing against itself without any direct supervision. In a tabular episodic Markov game with S states, A max-player actions and B min-player actions, the best existing algorithm for finding an approximate Nash equilibrium requires \tlO(S^2AB) steps of game playing, when only highlighting the dependency on (S,A,B). In contrast, the best existing lower bound scales as \Omega(S(A+B)) and has a significant gap from the upper bound. This paper closes this gap for the first time: we propose an optimistic variant of the Nash Q-learning algorithm with sample complexity \tlO(SAB), and a new Nash V-learning algorithm with sample complexity \tlO(S(A+B)). The latter result matches the information-theoretic lower bound in all problem-dependent parameters except for a polynomial factor of the length of each episode. In addition, we present a computational hardness result for learning the best responses against a fixed opponent in Markov games—-a learning objective different from finding the Nash equilibrium.</p>
<p><strong>Robust Multi-Agent Reinforcement Learning with Model Uncertainty</strong></p>
<blockquote>
<p>ID 18342<br>Kaiqing Zhang (University of Illinois at Urbana-Champaign (UIUC)) · TAO SUN (Amazon.com) · Yunzhe Tao (Amazon Artificial Intelligence) · Sahika Genc (Amazon Artificial Intelligence) · Sunil Mallya (Amazon AWS) · Tamer Basar (University of Illinois at Urbana-Champaign)</p>
</blockquote>
<p>In this work, we study the problem of multi-agent reinforcement learning (MARL)with model uncertainty, which is referred to as robust MARL. This is naturally motivated by some multi-agent applications where each agent may not have perfectly accurate knowledge of the model, e.g., all the reward functions of other agents. Little a priori work on MARL has accounted for such uncertainties, neither in problem formulation nor in algorithm design. In contrast, we model the problem as a robust Markov game, where the goal of all agents is to find policies such that no agent has the incentive to deviate, i.e., reach some equilibrium point, which is also robust to the possible uncertainty of the MARL model. We first introduce the solution concept of robust Nash equilibrium in our setting, and develop a Q-learning algorithm to find such equilibrium policies, with convergence guarantees under certain conditions. In order to handle possibly enormous state-action spaces in practice, we then derive the policy gradients for robust MARL, and develop an actor-critic algorithm with function approximation. Our experiments demonstrate that the proposed algorithm outperforms several baseline MARL methods that do not account for the model uncertainty, in several standard but uncertain cooperative and competitive MARL environments.</p>
<p><strong>Towards Minimax Optimal Reinforcement Learning in Factored Markov Decision Processes</strong></p>
<blockquote>
<p>ID 18351<br>Yi Tian (MIT) · Jian Qian (MIT) · Suvrit Sra (MIT)</p>
</blockquote>
<p>We study minimax optimal reinforcement learning in episodic factored Markov decision processes (FMDPs), which are MDPs with conditionally independent transition components. Assuming the factorization is known, we propose two model-based algorithms. The first one achieves minimax optimal regret guarantees for a rich class of factored structures, while the second one enjoys better computational complexity with a slightly worse regret. A key new ingredient of our algorithms is the design of a bonus term to guide exploration. We complement our algorithms by presenting several structure dependent lower bounds on regret for FMDPs that reveal the difficulty hiding in the intricacy of the structures.</p>
<p><strong>Scalable Multi-Agent Reinforcement Learning for Networked Systems with Average Reward</strong></p>
<blockquote>
<p>ID 18367<br>Guannan Qu (California Institute of Technology) · Yiheng Lin (California Institute of Technology) · Adam Wierman (California Institute of Technology) · Na Li (Harvard University)</p>
</blockquote>
<p>It has long been recognized that multi-agent reinforcement learning (MARL) faces significant scalability issues due to the fact that the size of the state and action spaces are exponentially large in the number of agents. In this paper, we identify a rich class of networked MARL problems where the model exhibits a local dependence structure that allows it to be solved in a scalable manner. Specifically, we propose a Scalable Actor-Critic (SAC) method that can learn a near optimal localized policy for optimizing the average reward with complexity scaling with the state-action space size of local neighborhoods, as opposed to the entire network. Our result centers around identifying and exploiting an exponential decay property that ensures the effect of agents on each other decays exponentially fast in their graph distance.</p>
<p><strong>Constrained episodic reinforcement learning in concave-convex and knapsack settings</strong></p>
<blockquote>
<p>ID 18379<br>KiantÃ© Brantley (The University of Maryland College Park) · Miro Dudik (Microsoft Research) · Thodoris Lykouris (Microsoft Research NYC) · Sobhan Miryoosefi (Princeton University) · Max Simchowitz (Berkeley) · Aleksandrs Slivkins (Microsoft Research) · Wen Sun (Microsoft Research NYC)</p>
</blockquote>
<p>We propose an algorithm for tabular episodic reinforcement learning with constraints. We provide a modular analysis with strong theoretical guarantees for settings with concave rewards and convex constraints, and for settings with hard constraints (knapsacks). Most of the previous work in constrained reinforcement learning is limited to linear constraints, and the remaining work focuses on either the feasibility question or settings with a single episode. Our experiments demonstrate that the proposed algorithm significantly outperforms these approaches in existing constrained episodic environments.</p>
<p><strong>Sample Efficient Reinforcement Learning via Low-Rank Matrix Estimation</strong></p>
<blockquote>
<p>ID 18391<br>Devavrat Shah (Massachusetts Institute of Technology) · Dogyoon Song (Massachusetts Institute of Technology) · Zhi Xu (MIT) · Yuzhe Yang (MIT)</p>
</blockquote>
<p>We consider the question of learning $Q$-function in a sample efficient manner for reinforcement learning with continuous state and action spaces under a generative model. If $Q$-function is Lipschitz continuous, then the minimal sample complexity for estimating $\epsilon$-optimal $Q$-function is known to scale as $\Omega(\frac{1}{\epsilon^{d_1+d_2+2}})$ per classical non-parametric learning theory, where $d_1$ and $d_2$ denote the dimensions of the state and action spaces respectively. The $Q$-function, when viewed as a kernel, induces a Hilbert-Schmidt operator and hence possesses square-summable spectrum. This motivates us to consider a parametric class of $Q$-functions parameterized by its “rank” $r$, which contains all Lipschitz $Q$-functions  as $r\to\infty$. As our key contribution, we develop a simple, iterative learning algorithm that finds $\epsilon$-optimal $Q$-function with sample complexity of $\widetilde{O}(\frac{1}{\epsilon^{\max(d_1, d_2)+2}})$ when the optimal $Q$-function has low rank $r$ and the discounting factor $\gamma$ is below a certain threshold. Thus, this provides an exponential improvement in sample complexity. To enable our result, we develop a novel Matrix Estimation algorithm that faithfully estimates an unknown low-rank matrix in the  $\ell_\infty$ sense even in the presence of arbitrary bounded noise, which might be of interest in its own right. Empirical results on several stochastic control tasks confirm the efficacy of our “low-rank” algorithms.</p>
<p><strong>Trajectory-wise Multiple Choice Learning for Dynamics Generalization in Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18402<br>Younggyo Seo (KAIST) · Kimin Lee (UC Berkeley) · Ignasi Clavera Gilaberte (UC Berkeley) · Thanard Kurutach (University of California Berkeley) · Jinwoo Shin (KAIST) · Pieter Abbeel (UC Berkeley &amp; covariant.ai)</p>
</blockquote>
<p>Model-based reinforcement learning (RL) has shown great potential in various control tasks in terms of both sample-efficiency and final performance. However, learning a generalizable dynamics model robust to changes in dynamics remains a challenge since the target transition dynamics follow a multi-modal distribution. In this paper, we present a new model-based RL algorithm, coined trajectory-wise multiple choice learning, that learns a multi-headed dynamics model for dynamics generalization. The main idea is updating the most accurate prediction head to specialize each head in certain environments with similar dynamics, i.e., clustering environments. Moreover, we incorporate context learning, which encodes dynamics-specific information from past experiences into the context latent vector, enabling the model to perform online adaptation to unseen environments. Finally, to utilize the specialized prediction heads more effectively, we propose an adaptive planning method, which selects the most accurate prediction head over a recent experience. Our method exhibits superior zero-shot generalization performance across a variety of control tasks, compared to state-of-the-art RL methods. Source code and videos are available at <a href="https://sites.google.com/view/trajectory-mcl">https://sites.google.com/view/trajectory-mcl</a>.</p>
<p><strong>Cooperative Heterogeneous Deep Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18406<br>Han Zheng (UTS) · Pengfei Wei (National University of Singapore) · Jing Jiang (University of Technology Sydney) · Guodong Long (University of Technology Sydney (UTS)) · Qinghua Lu (Data61, CSIRO) · Chengqi Zhang (University of Technology Sydney)</p>
</blockquote>
<p>Numerous deep reinforcement learning agents have been proposed, and eachof them has its strengths and flaws. In this work, we present a CooperativeHeterogeneous Deep Reinforcement Learning (CHDRL) framework that can learna policy by integrating the advantages of heterogeneous agents. Specifically, wepropose a cooperative learning framework that classifies heterogeneous agents into two classes: global agents and local agents. Global agents are off-policy agents that can utilize experiences from the other agents. Local agents are either on-policy agents or population-based evolutionary algorithms (EAs) agents that can explore the local area effectively. We employ global agents, which are sample-efficient, to guide the learning of local agents so that local agents can benefit from the sample-efficient agents and simultaneously maintain their advantages, e.g., stability. Global agents also benefit from effective local searches. Experimental studies on a range of continuous control tasks from the Mujoco benchmark show that CHDRL achieves better performance compared with state-of-the-art baselines.</p>
<p><strong>Implicit Distributional Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18434<br>Yuguang Yue (University of Texas at Austin) · Zhendong Wang (University of Texas, Austin) · Mingyuan Zhou (University of Texas at Austin)</p>
</blockquote>
<p>To improve the sample efficiency of policy-gradient based reinforcement learning algorithms, we propose implicit distributional actor-critic (IDAC) that consists of a distributional critic, built on two deep generator networks (DGNs), and a semi-implicit actor (SIA), powered by a flexible policy distribution. We adopt a distributional perspective on the discounted cumulative return and model it with a state-action-dependent implicit distribution, which is approximated by the DGNs that take state-action pairs and random noises as their input. Moreover, we use the SIA to provide a semi-implicit policy distribution, which mixes the policy parameters with a reparameterizable distribution that is not constrained by an analytic density function. In this way, the policy’s marginal distribution is implicit, providing the potential to model complex properties such as covariance structure and skewness, but its parameter and entropy can still be estimated. We incorporate these features with an off-policy algorithm framework to solve problems with continuous action space and compare IDAC with state-of-the-art algorithms on representative OpenAI Gym environments. We observe that IDAC outperforms these baselines in most tasks. Python code is provided.</p>
<p><strong>Efficient Exploration of Reward Functions in Inverse Reinforcement Learning via Bayesian Optimization</strong></p>
<blockquote>
<p>ID 18436<br>Sreejith Balakrishnan (National University of Singapore) · Quoc Phong Nguyen (National University of Singapore) · Bryan Kian Hsiang Low (National University of Singapore) · Harold Soh (National University Singapore)</p>
</blockquote>
<p>The problem of inverse reinforcement learning (IRL) is relevant to a variety of tasks including value alignment and robot learning from demonstration. Despite significant algorithmic contributions in recent years, IRL remains an ill-posed problem at its core; multiple reward functions coincide with the observed behavior and the actual reward function is not identifiable without prior knowledge or supplementary information. This paper presents an IRL framework called Bayesian optimization-IRL (BO-IRL) which identifies multiple solutions that are consistent with the expert demonstrations by efficiently exploring the reward function space. BO-IRL achieves this by utilizing Bayesian Optimization along with our newly proposed kernel that (a) projects the parameters of policy invariant reward functions to a single point in a latent space and (b) ensures nearby points in the latent space correspond to reward functions yielding similar likelihoods. This projection allows the use of standard stationary kernels in the latent space to capture the correlations present across the reward function space. Empirical results on synthetic and real-world environments (model-free and model-based) show that BO-IRL discovers multiple reward functions while minimizing the number of expensive exact policy optimizations.</p>
<p><strong>Provably Efficient Reinforcement Learning with Kernel and Neural Function Approximations</strong></p>
<blockquote>
<p>ID 18454<br>Zhuoran Yang (Princeton) · Chi Jin (Princeton University) · Zhaoran Wang (Northwestern University) · Mengdi Wang (Princeton University) · Michael Jordan (UC Berkeley)</p>
</blockquote>
<p>Reinforcement learning (RL) algorithms combined with modern function approximators such as kernel functions and deep neural networks have achieved significant empirical successes in large-scale application problems with  a massive number of states. From a theoretical perspective, however,  RL with functional approximation  poses a fundamental challenge  to developing algorithms  with provable computational and statistical efficiency,  due to the need to take into consideration  both the exploration-exploitation tradeoff that is inherent in RL and the bias-variance tradeoff that is innate in statistical estimation.  To address such a challenge,  focusing on the episodic setting where the action-value functions are represented by a kernel function or over-parametrized neural network,we propose the first provable RL algorithm with both polynomial runtime and sample complexity, without additional assumptions on the data-generating model. In particular, for both the kernel and neural settings,  we prove that an optimistic modification of the least-squares value iteration algorithm incurs an $\tilde{\mathcal{O}}(\delta_{\cF}  H^2 \sqrt{T})$ regret, where $\delta_{\cF}$ characterizes the intrinsic complexity of the function class $\cF$, $H$ is the length of each episode, and  $T$ is the total number of episodes. Our regret bounds are independent of the number of states and therefore even allows it to diverge,  which exhibits the benefit of function approximation.</p>
<p><strong>Upper Confidence Primal-Dual Reinforcement Learning for CMDP with Adversarial Loss</strong></p>
<blockquote>
<p>ID 18460<br>Shuang Qiu (University of Michigan) · Xiaohan Wei (University of Southern California) · Zhuoran Yang (Princeton) · Jieping Ye (University of Michigan) · Zhaoran Wang (Northwestern University)</p>
</blockquote>
<p>We consider online learning for episodic stochastically constrained Markov decision processes (CMDP), which plays a central role in ensuring the safety of reinforcement learning. Here the loss function can vary arbitrarily across the episodes, whereas both the loss received and the budget consumption are revealed at the end of each episode. Previous works solve this problem under the restrictive assumption that the transition model of the MDP is known a priori and establish regret bounds that depend polynomially on the cardinalities of the state space $\mathcal{S}$ and the action space $\mathcal{A}$. In this work, we propose a new \emph{upper confidence primal-dual} algorithm, which only requires the trajectories sampled from the transition model. In particular, we prove that the proposed algorithm achieves $\widetilde{\mathcal{O}}(L|\mathcal{S}|\sqrt{|\mathcal{A}|T})$ upper bounds of both the regret and the constraint violation, where $L$ is the length of each episode. Our analysis incorporates a new high-probability drift analysis of Lagrange multiplier processes into the celebrated regret analysis of upper confidence reinforcement learning, which demonstrates the power of   ``optimism in the face of uncertainty’’ in constrained online learning.</p>
<p><strong>Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity</strong></p>
<blockquote>
<p>ID 18478<br>Kaiqing Zhang (University of Illinois at Urbana-Champaign (UIUC)) · Sham Kakade (University of Washington) · Tamer Basar (University of Illinois at Urbana-Champaign) · Lin Yang (UCLA)</p>
</blockquote>
<p>Model-based reinforcement learning (RL), which finds an optimal policy using an empirical model, has long been recognized as one of the cornerstones of RL. It is especially suitable for multi-agent RL (MARL), as it naturally decouples the learning and the planning phases, and avoids the non-stationarity problem when all agents are improving their policies simultaneously using samples. Though intuitive and widely-used, the sample complexity of model-based MARL algorithms has been investigated relatively much less often. In this paper, we aim to address the fundamental open question about the sample complexity of model-based MARL. We study arguably the most basic MARL setting: two-player discounted zero-sum Markov games, given only access to a generative model of state transition. We show that model-based MARL achieves a sample complexity of  $\tilde \cO(|\cS||\cA||\cB|(1-\gamma)^{-3}\epsilon^{-2})$ for finding the Nash equilibrium (NE) \emph{value} up to some $\epsilon$ error, and the $\epsilon$-NE \emph{policies}, where $\gamma$ is the discount factor, and $\cS,\cA,\cB$ denote the state space, and the action spaces for the two agents. We also show that this method is near-minimax optimal with a tight dependence on $1-\gamma$ and $|\cS|$ by providing a lower bound of $\Omega(|\cS|(|\cA|+|\cB|)(1-\gamma)^{-3}\epsilon^{-2})$. Our results justify the efficiency of this simple model-based approach in the multi-agent RL setting.</p>
<p><strong>PlanGAN: Model-based Planning With Sparse Rewards and Multiple Goals</strong></p>
<blockquote>
<p>ID 18497<br>Henry Charlesworth (University of Warwick) · Giovanni Montana (University of Warwick)</p>
</blockquote>
<p>Learning with sparse rewards remains a significant challenge in reinforcement learning (RL), especially when the aim is to train a policy capable of achieving multiple different goals. To date, the most successful approaches for dealing with multi-goal, sparse reward environments have been model-free RL algorithms. In this work we propose PlanGAN, a model-based algorithm specifically designed for solving multi-goal tasks in environments with sparse rewards. Our method builds on the fact that any trajectory of experience collected by an agent contains useful information about how to achieve the goals observed during that trajectory. We use this to train an ensemble of conditional generative models (GANs) to generate plausible trajectories that lead the agent from its current state towards a specified goal. We then combine these imagined trajectories into a novel planning algorithm in order to achieve the desired goal as efficiently as possible. The performance of PlanGAN has been tested on a number of robotic navigation/manipulation tasks in comparison with a range of model-free reinforcement learning baselines, including Hindsight Experience Replay. Our studies indicate that  PlanGAN can achieve comparable performance whilst being around 4-8 times more sample efficient.</p>
<p><strong>Improving Generalization in Reinforcement Learning with Mixture Regularization</strong></p>
<blockquote>
<p>ID 18498<br>KAIXIN WANG (National University of Singapore) · Bingyi Kang (National University of Singapore) · Jie Shao (Fudan University) · Jiashi Feng (National University of Singapore)</p>
</blockquote>
<p>Deep reinforcement learning (RL) agents trained in a limited set of environments tend to suffer overfitting and fail to generalize to unseen testing environments. To improve their generalizability, data augmentation approaches (e.g. cutout and random convolution) are previously explored to increase the data diversity. However, we find these approaches only locally perturb the observations regardless of the training environments, showing limited effectiveness on enhancing the data diversity and the generalization performance. In this work, we introduce a simple approach, named mixreg, which trains agents on a mixture of observations from different training environments and imposes linearity constraints on the observation interpolations and the supervision (e.g. associated reward) interpolations. Mixreg increases the data diversity more effectively and helps learn smoother policies. We verify its effectiveness on improving generalization by conducting extensive experiments on the large-scale Procgen benchmark. Results show mixreg outperforms the well-established baselines on unseen testing environments by a large margin. Mixreg is simple, effective and general. It can be applied to both policy-based and value-based RL algorithms. Code is available at <a href="https://github.com/kaixin96/mixreg">https://github.com/kaixin96/mixreg</a>.</p>
<p><strong>A game-theoretic analysis of networked system control for common-pool resource management using multi-agent reinforcement learning</strong></p>
<blockquote>
<p>ID 18523<br>Arnu Pretorius (InstaDeep) · Scott Cameron (Instadeep) · Elan van Biljon (Stellenbosch University) · Thomas Makkink (InstaDeep) · Shahil Mawjee (InstaDeep) · Jeremy du Plessis (University of Cape Town) · Jonathan Shock (University of Cape Town) · Alexandre Laterre (InstaDeep) · Karim Beguir (InstaDeep)</p>
</blockquote>
<p>Multi-agent reinforcement learning has recently shown great promise as an approach to networked system control. Arguably, one of the most difficult and important tasks for which large scale networked system control is applicable is common-pool resource management. Crucial common-pool resources include arable land, fresh water, wetlands, wildlife, fish stock, forests and the atmosphere, of which proper management is related to some of society’s greatest challenges such as food security, inequality and climate change. Here we take inspiration from a recent research program investigating the game-theoretic incentives of humans in social dilemma situations such as the well-known \textit{tragedy of the commons}. However, instead of focusing on biologically evolved human-like agents, our concern is rather to better understand the learning and operating behaviour of engineered networked systems comprising general-purpose reinforcement learning agents, subject only to nonbiological constraints such as memory, computation and communication bandwidth. Harnessing tools from empirical game-theoretic analysis, we analyse the differences in resulting solution concepts that stem from employing different information structures in the design of networked multi-agent systems. These information structures pertain to the type of information shared between agents as well as the employed communication protocol and network topology. Our analysis contributes new insights into the consequences associated with certain design choices and provides an additional dimension of comparison between systems beyond efficiency, robustness, scalability and mean control performance.</p>
<p><strong>Fast Adaptive Non-Monotone Submodular Maximization Subject to a Knapsack Constraint</strong></p>
<blockquote>
<p>ID 18526<br>Georgios Amanatidis (University of Essex) · Federico Fusco (Sapienza University of Rome) · Philip Lazos (Sapienza University of Rome) · Stefano Leonardi (Sapienza University of Rome) · Rebecca ReiffenhÃ¤user (Sapienza University of Rome)</p>
</blockquote>
<p>Constrained submodular maximization problems encompass a wide variety of applications, including personalized recommendation, team formation, and revenue maximization via viral marketing. The massive instances occurring in modern-day applications can render existing algorithms prohibitively slow. Moreover, frequently those instances are also inherently stochastic. Focusing on these challenges, we revisit the classic problem of maximizing a (possibly non-monotone) submodular function subject to a knapsack constraint. We present a simple randomized greedy algorithm that achieves a $5.83$ approximation and runs in $O(n \log n)$ time, i.e., at least a factor $n$ faster than other state-of-the-art algorithms. The robustness of our approach allows us to further transfer it  to a stochastic version of the problem. There, we obtain a 9-approximation to the best adaptive policy, which is the first constant approximation for non-monotone objectives. Experimental evaluation of our algorithms showcases their improved performance on real and synthetic data.</p>
<p><strong>Planning in Markov Decision Processes with Gap-Dependent Sample Complexity</strong></p>
<blockquote>
<p>ID 18528<br>Anders Jonsson (Universitat Pompeu Fabra) · Emilie Kaufmann (CNRS) · Pierre Menard (Inria) · Omar Darwiche Domingues (Inria) · Edouard Leurent (INRIA) · Michal Valko (DeepMind)</p>
</blockquote>
<p>We propose MDP-GapE, a new trajectory-based Monte-Carlo Tree Search algorithm for planning in a Markov Decision Process in which transitions have a finite support. We prove an upper bound on the number of sampled trajectories needed for MDP-GapE to identify a near-optimal action with high probability. This problem-dependent result is expressed in terms of the sub-optimality gaps of the state-action pairs that are visited during exploration. Our experiments reveal that MDP-GapE is also effective in practice, in contrast with other algorithms with sample complexity guarantees in the fixed-confidence setting, that are mostly theoretical.</p>
<p><strong>Deep Reinforcement Learning with Stacked Hierarchical Attention for Text-based Games</strong></p>
<blockquote>
<p>ID 18534<br>Yunqiu Xu (University of Technology Sydney) · Meng Fang (Tencent) · Ling Chen (“ University of Technology, Sydney, Australia”) · Yali Du (University College London) · Joey Tianyi Zhou (IHPC, A*STAR) · Chengqi Zhang (University of Technology Sydney)</p>
</blockquote>
<p>We study reinforcement learning (RL) for text-based games, which are interactive simulations in the context of natural language. While different methods have been developed to represent the environment information and language actions, existing RL agents are not empowered with any reasoning capabilities to deal with textual games. In this work, we aim to conduct explicit reasoning with knowledge graphs for decision making, so that the actions of an agent are generated and supported by an interpretable inference procedure. We propose a stacked hierarchical attention mechanism to construct an explicit representation of the reasoning process by exploiting the structure of the knowledge graph. We extensively evaluate our method on a number of man-made benchmark games, and the experimental results demonstrate that our method performs better than existing text-based agents.</p>
<p><strong>Robust Reinforcement Learning via Adversarial training with Langevin Dynamics</strong></p>
<blockquote>
<p>ID 18548<br>Parameswaran Kamalaruban (EPFL) · Yu-Ting Huang (EPFL) · Ya-Ping Hsieh (EPFL) · Paul Rolland (EPFL) · Cheng Shi (Unversity of Basel) · Volkan Cevher (EPFL)</p>
</blockquote>
<p>We introduce a \emph{sampling} perspective to tackle the challenging task of training robust Reinforcement Learning (RL) agents. Leveraging the powerful Stochastic Gradient Langevin Dynamics, we present a novel, scalable two-player RL algorithm, which is a sampling variant of the two-player policy gradient method. Our algorithm consistently outperforms existing baselines, in terms of generalization across different training and testing conditions, on several MuJoCo environments. Our experiments also show that, even for objective functions that entirely ignore potential environmental shifts, our sampling approach remains highly robust in comparison to standard RL algorithms.</p>
<p><strong>Interferobot: aligning an optical interferometer by a reinforcement learning agent</strong></p>
<blockquote>
<p>ID 18550<br>Dmitry Sorokin (Russian Quantum Center) · Alexander Ulanov (Russian Quantum Center) · Ekaterina Sazhina (Russian Quantum Center) · Alexander Lvovsky (Oxford University)</p>
</blockquote>
<p>Limitations in acquiring training data restrict potential applications of deep reinforcement learning (RL) methods to the training of real-world robots. Here we train an RL agent to align a Mach-Zehnder interferometer, which is an essential part of many optical experiments, based on images of interference fringes acquired by a monocular camera. The agent is trained in a simulated environment, without any hand-coded features or a priori information about the physics, and subsequently transferred to a physical interferometer. Thanks to a set of domain randomizations simulating uncertainties in physical measurements, the agent successfully aligns this interferometer without any fine-tuning, achieving a performance level of a human expert.</p>
<p><strong>Reinforcement Learning for Control with Multiple Frequencies</strong></p>
<blockquote>
<p>ID 18560<br>Jongmin Lee (KAIST) · ByungJun Lee (KAIST) · Kee-Eung Kim (KAIST)</p>
</blockquote>
<p>Many real-world sequential decision problems involve multiple action variables whose control frequencies are different, such that actions take their effects at different periods. While these problems can be formulated with the notion of multiple action persistences in factored-action MDP (FA-MDP), it is non-trivial to solve them efficiently since an action-persistent policy constructed from a stationary policy can be arbitrarily suboptimal, rendering solution methods for the standard FA-MDPs hardly applicable. In this paper, we formalize the problem of multiple control frequencies in RL and provide its efficient solution method. Our proposed method, Action-Persistent Policy Iteration (AP-PI), provides a theoretical guarantee on the convergence to an optimal solution while incurring only a factor of $|A|$ increase in time complexity during policy improvement step, compared to the standard policy iteration for FA-MDPs. Extending this result, we present Action-Persistent Actor-Critic (AP-AC), a scalable RL algorithm for high-dimensional control tasks. In the experiments, we demonstrate that AP-AC significantly outperforms the baselines on several continuous control tasks and a traffic control simulation, which highlights the effectiveness of our method that directly optimizes the periodic non-stationary policy for tasks with multiple control frequencies.</p>
<p><strong>Learning to Play Sequential Games versus Unknown Opponents</strong></p>
<blockquote>
<p>ID 18582<br>Pier Giuseppe Sessa (ETH ZÃ¼rich) · Ilija Bogunovic (ETH Zurich) · Maryam Kamgarpour (ETH ZÃ¼rich) · Andreas Krause (ETH Zurich)</p>
</blockquote>
<p>We consider a repeated sequential game between a learner, who plays first, and an opponent who responds to the chosen action. We seek to design strategies for the learner to successfully interact with the opponent. While most previous approaches consider known opponent models, we focus on the setting in which the opponent’s model is unknown. To this end, we use kernel-based regularity assumptions to capture and exploit the structure in the opponent’s response. We propose a novel algorithm for the learner when playing against an adversarial sequence of opponents. The algorithm combines ideas from bilevel optimization and online learning to effectively balance between exploration (learning about the opponent’s model) and exploitation (selecting highly rewarding actions for the learner). Our results include algorithm’s regret guarantees that depend on the regularity of the opponent’s response and scale sublinearly with the number of game rounds. Moreover, we specialize our approach to repeated Stackelberg games, and empirically demonstrate its effectiveness in a traffic routing and wildlife conservation task.</p>
<p><strong>Contextual Games: Multi-Agent Learning with Side Information</strong></p>
<blockquote>
<p>ID 18586<br>Pier Giuseppe Sessa (ETH ZÃ¼rich) · Ilija Bogunovic (ETH Zurich) · Andreas Krause (ETH Zurich) · Maryam Kamgarpour (ETH ZÃ¼rich)</p>
</blockquote>
<p>We formulate the novel class of contextual games, a type of repeated games driven by contextual information at each round. By means of kernel-based regularity assumptions, we model the correlation between different contexts and game outcomes and propose a novel online (meta) algorithm that exploits such correlations to minimize the contextual regret of individual players. We define game-theoretic notions of contextual Coarse Correlated Equilibria (c-CCE) and optimal contextual welfare for this new class of games and show that c-CCEs and optimal welfare can be approached whenever players’ contextual regrets vanish. Finally, we empirically validate our results in a traffic routing experiment, where our algorithm leads to better performance and higher welfare compared to baselines that do not exploit the available contextual information or the correlations present in the game.</p>
<p><strong>Risk-Sensitive Reinforcement Learning: Near-Optimal Risk-Sample Tradeoff in Regret</strong></p>
<blockquote>
<p>ID 18597<br>Yingjie Fei (Cornell University) · Zhuoran Yang (Princeton) · Yudong Chen (Cornell University) · Zhaoran Wang (Northwestern University) · Qiaomin Xie (Cornell University)</p>
</blockquote>
<p>We study risk-sensitive reinforcement learning in episodic Markov decision processes with unknown transition kernels, where the goal is to optimize the total reward under the risk measure of exponential utility. We propose two provably efficient model-free algorithms, Risk-Sensitive Value Iteration (RSVI) and Risk-Sensitive Q-learning (RSQ). These algorithms implement a form of risk-sensitive optimism in the face of uncertainty, which adapts to both risk-seeking and risk-averse modes of exploration. We prove that RSVI attains an $\tilde{O}\big(\lambda( | \beta | H^2) \cdot \sqrt{H^{3} S^{2}AT} \big)$ regret, while RSQ attains an $\tilde{O}\big(\lambda(|\beta| H^2) \cdot \sqrt{H^{4} SAT} \big)$ regret, where $\lambda(u) = (e^{3u}-1)/u$ for $u&gt;0$. In the above, $\beta$ is the risk parameter of the exponential utility function, $S$ the number of states, $A$ the number of actions,  $T$ the total number of timesteps, and $H$ the episode length. On the flip side, we establish a regret lower bound showing that the exponential dependence on $|\beta|$ and $H$ is unavoidable for any algorithm with an $\tilde{O}(\sqrt{T})$ regret (even when the risk objective is on the same scale as the original reward), thus certifying the near-optimality of the proposed algorithms. Our results demonstrate that incorporating risk awareness into reinforcement learning necessitates an exponential cost in $|\beta|$ and $H$,   which quantifies the fundamental tradeoff between risk sensitivity (related to aleatoric uncertainty) and sample efficiency (related to epistemic uncertainty). To the best of our knowledge, this is the first regret analysis of risk-sensitive reinforcement learning with the exponential utility.</p>
<p><strong>Expert-Supervised Reinforcement Learning for Offline Policy Learning and Evaluation</strong></p>
<blockquote>
<p>ID 18626<br>Aaron Sonabend (Harvard University) · Junwei Lu () · Leo Anthony Celi (Massachusetts Institute of Technology) · Tianxi Cai (Harvard School of Public Health) · Peter Szolovits (MIT)</p>
</blockquote>
<p>Offline Reinforcement Learning (RL) is a promising approach for learning optimalpolicies in environments where direct exploration is expensive or unfeasible. However, the adoption of such policies in practice is often challenging, as they are hardto interpret within the application context, and lack measures of uncertainty for thelearned policy value and its decisions. To overcome these issues, we propose anExpert-Supervised RL (ESRL) framework which uses uncertainty quantificationfor offline policy learning. In particular, we have three contributions: 1) the methodcan learn safe and optimal policies through hypothesis testing, 2) ESRL allows fordifferent levels of risk averse implementations tailored to the application context,and finally, 3) we propose a way to interpret ESRL’s policy at every state throughposterior distributions, and use this framework to compute off-policy value functionposteriors. We provide theoretical guarantees for our estimators and regret boundsconsistent with Posterior Sampling for RL (PSRL). Sample efficiency of ESRLis independent of the chosen risk aversion threshold and quality of the behaviorpolicy.</p>
<p><strong>Dynamic allocation of limited memory resources in reinforcement learning</strong></p>
<blockquote>
<p>ID 18633<br>Nisheet Patel (University of Geneva) · Luigi Acerbi (University of Helsinki) · Alexandre Pouget (University of Geneva)</p>
</blockquote>
<p>Biological brains are inherently limited in their capacity to process and store information, but are nevertheless capable of solving complex tasks with apparent ease. Intelligent behavior is related to these limitations, since resource constraints drive the need to generalize and assign importance differentially to features in the environment or memories of past experiences. Recently, there have been parallel efforts in reinforcement learning and neuroscience to understand strategies adopted by artificial and biological agents to circumvent limitations in information storage. However, the two threads have been largely separate. In this article, we propose a dynamical framework to maximize expected reward under constraints of limited resources, which we implement with a cost function that penalizes precise representations of action-values in memory, each of which may vary in its precision. We derive from first principles an algorithm, Dynamic Resource Allocator (DRA), which we apply to two standard tasks in reinforcement learning and a model-based planning task, and find that it allocates more resources to items in memory that have a higher impact on cumulative rewards. Moreover, DRA learns faster when starting with a higher resource budget than what it eventually allocates for performing well on tasks, which may explain why frontal cortical areas in biological brains appear more engaged in early stages of learning before settling to lower asymptotic levels of activity. Our work provides a normative solution to the problem of learning how to allocate costly resources to a collection of uncertain memories in a manner that is capable of adapting to changes in the environment.</p>
<p><strong>AttendLight: Universal Attention-Based Reinforcement Learning Model for Traffic Signal Control</strong></p>
<blockquote>
<p>ID 18634<br>Afshin Oroojlooy (SAS Institute, Inc) · Mohammadreza Nazari (SAS Institute Inc.) · Davood Hajinezhad (SAS Institute Inc.) · Jorge Silva (SAS)</p>
</blockquote>
<p>We propose AttendLight, an end-to-end Reinforcement Learning (RL) algorithm for the problem of traffic signal control. Previous approaches for this problem have the shortcoming that they require training for each new intersection with a different structure or traffic flow distribution. AttendLight solves this issue by training a single, universal model for intersections with any number of roads, lanes, phases (possible signals), and traffic flow. To this end, we propose a deep RL model which incorporates two attention models. The first attention model is introduced to  handle different numbers of roads-lanes; and the second attention model is intended for enabling decision-making with any number of phases in an intersection. As a result, our proposed model works for any intersection configuration, as long as a similar configuration is represented in the training set. Experiments were conducted with both synthetic and real-world standard benchmark datasets. Our numerical experiment covers intersections with three or four approaching roads; one-directional/bi-directional roads with one, two, and three lanes; different number of phases; and different traffic flows. We consider two regimes: (i) single-environment training, single-deployment, and (ii) multi-environment training, multi-deployment. AttendLight outperforms both classical and other RL-based approaches on all cases in both regimes.</p>
<p><strong>Sample-Efficient Reinforcement Learning of Undercomplete POMDPs</strong></p>
<blockquote>
<p>ID 18663<br>Chi Jin (Princeton University) · Sham Kakade (University of Washington) · Akshay Krishnamurthy (Microsoft) · Qinghua Liu (Princeton University)</p>
</blockquote>
<p>Partial observability is a common challenge in many reinforcement learning applications, which requires an agent to maintain memory, infer latent states, and integrate this past information into exploration. This challenge leads to a number of computational and statistical hardness results for learning general Partially Observable Markov Decision Processes (POMDPs). This work shows that these hardness barriers do not preclude efficient reinforcement learning for rich and interesting  subclasses of POMDPs. In particular, we present a sample-efficient algorithm, OOM-UCB, for episodic finite undercomplete POMDPs, where the number of observations is larger than the number of latent states and where exploration is essential for learning, thus distinguishing our results from prior works. OOM-UCB achieves an optimal sample complexity of $\tilde{\mathcal{O}}(1/\varepsilon^2)$ for finding an $\varepsilon$-optimal policy, along with being polynomial in all other relevant quantities. As an interesting special case, we also provide a computationally and statistically efficient algorithm for POMDPs with deterministic state transitions.</p>
<p><strong>Learning discrete distributions with infinite support</strong></p>
<blockquote>
<p>ID 18680<br>Doron Cohen (Ben-Gurion University of the Negev) · Aryeh Kontorovich (Ben Gurion University) · Geoï¬€rey Wolfer (Ben-Gurion University of the Negev)</p>
</blockquote>
<p>We present a novel approach to estimating discrete distributions with (potentially) infinite support in the total variation metric. In a departure from the established paradigm, we make no structural assumptions whatsoever on the sampling distribution. In such a setting, distribution-free risk bounds are impossible, and the best one could hope for is a fully empirical data-dependent bound. We derive precisely such bounds, and demonstrate that these are, in a well-defined sense, the best possible. Our main discovery is that the half-norm of the empirical distribution provides tight upper and lower estimates on the empirical risk. Furthermore, this quantity decays at a nearly optimal rate as a function of the true distribution. The optimality follows from a minimax result, of possible independent interest. Additional structural results are provided, including an exact Rademacher complexity calculation and apparently a first connection between the total variation risk and the missing mass.</p>
<p><strong>R-learning in actor-critic model offers a biologically relevant mechanism for sequential decision-making</strong></p>
<blockquote>
<p>ID 18736<br>Sergey Shuvaev (Cold Spring Harbor Laboratory) · Sarah Starosta (Washington University in St. Louis) · Duda Kvitsiani (Aarhus University) · Adam Kepecs (Washington University in St. Louis) · Alexei Koulakov (Cold Spring Harbor Laboratory)</p>
</blockquote>
<p>In real-world settings, we repeatedly decide whether to pursue better conditions or to keep things unchanged. Examples include time investment, employment, entertainment preferences etc. How do we make such decisions? To address this question, the field of behavioral ecology has developed foraging paradigms – the model settings in which human and non-human subjects decided when to leave depleting food resources. Foraging theory, represented by the marginal value theorem (MVT), provided accurate average-case stay-or-leave rules consistent with behaviors of subjects towards depleting resources. Yet, the algorithms underlying individual choices and ways to learn such algorithms remained unclear. In this work, we build interpretable deep actor-critic models to show that R-learning – a reinforcement learning (RL) approach balancing short-term and long-term rewards – is consistent with the way real-life agents may learn making stay-or-leave decisions. Specifically we show that deep R-learning predicts choice patterns consistent with behavior of mice in foraging tasks; its TD error, the training signal in our model, correlates with dopamine activity of ventral tegmental area (VTA) neurons in the brain. Our theoretical and experimental results show that deep R-learning agents leave depleting reward resources when reward intake rates fall below their exponential averages over past trials. This individual-case decision rule, learned within RL and matching the MVT on average, bridges the gap between these major approaches to sequential decision-making. We further argue that our proposed decision rule, resulting from R-learning and consistent with animals’ behavior, is Bayes optimal in dynamic real-world environments. Overall, our work links available sequential decision-making theories including the MVT, RL, and Bayesian approaches to propose the learning mechanism and an optimal decision rule for sequential stay-or-leave choices in natural environments.</p>
<p><strong>Multi-agent active perception with prediction rewards</strong></p>
<blockquote>
<p>ID 18762<br>Mikko Lauri (University of Hamburg) · Frans Oliehoek (TU Delft)</p>
</blockquote>
<p>Multi-agent active perception is a task where a team of agents cooperatively gathers observations to compute a joint estimate of a hidden variable. The task is decentralized and the joint estimate can only be computed after the task ends by fusing observations of all agents. The objective is to maximize the accuracy of the estimate. The accuracy is quantified by a centralized prediction reward determined by a centralized decision-maker who perceives the observations gathered by all agents after the task ends. In this paper, we model multi-agent active perception as a decentralized partially observable Markov decision process (Dec-POMDP) with a convex centralized prediction reward. We prove that by introducing individual prediction actions for each agent, the problem is converted into a standard Dec-POMDP with a decentralized prediction reward. The loss due to decentralization is bounded, and we give a sufficient condition for when it is zero. Our results allow application of any Dec-POMDP solution algorithm to multi-agent active perception problems, and enable planning to reduce uncertainty without explicit computation of joint estimates. We demonstrate the empirical usefulness of our results by applying a standard Dec-POMDP algorithm to multi-agent active perception problems, showing increased scalability in the planning horizon.</p>
<p><strong>RL Unplugged: A Collection of Benchmarks for Offline Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18764<br>Ziyu Wang (Deepmind) · Caglar Gulcehre (Deepmind) · Alexander Novikov (DeepMind) · Thomas Paine (DeepMind) · Sergio GÃ³mez (DeepMind) · Konrad Zolna (DeepMind) · Rishabh Agarwal (Google Research, Brain Team) · Josh Merel (DeepMind) · Daniel Mankowitz (DeepMind) · Cosmin Paduraru (DeepMind) · Gabriel Dulac-Arnold (Google Research) · Jerry Li (Google) · Mohammad Norouzi (Google Brain) · Matthew Hoffman (DeepMind) · Nicolas Heess (Google DeepMind) · Nando de Freitas (DeepMind)</p>
</blockquote>
<p>Offline methods for reinforcement learning have a potential to help bridge the gap between reinforcement learning research and real-world applications. They make it possible to learn policies from offline datasets, thus overcoming concerns associated with online data collection in the real-world, including cost, safety, or ethical concerns.  In this paper, we propose a benchmark called RL Unplugged to evaluate and compare offline RL methods. RL Unplugged includes data from a diverse range of domains including games e.g., Atari benchmark) and simulated motor control problems (e.g., DM Control Suite). The datasets include domains that are partially or fully observable, use continuous or discrete actions, and  have stochastic vs. deterministic dynamics. We propose detailed evaluation protocols for each domain in RL Unplugged and provide an extensive analysis of supervised learning and offline RL methods using these protocols. We will release data for all our tasks and open-source all algorithms presented in this paper. We hope that our suite of benchmarks will increase the reproducibility of experiments and make it possible to study challenging tasks with a limited computational budget, thus making RL research both more systematic and more accessible across the community. Moving forward, we view RL Unplugged as a living benchmark suite that will evolve and grow with datasets contributed by the research community and ourselves. Our project page is available on github.</p>
<p><strong>Learning to Play No-Press Diplomacy with Best Response Policy Iteration</strong></p>
<blockquote>
<p>ID 18768<br>Thomas Anthony (DeepMind) · Tom Eccles (DeepMind) · Andrea Tacchetti (DeepMind) · JÃ¡nos KramÃ¡r (DeepMind) · Ian Gemp (DeepMind) · Thomas Hudson (DeepMind) · Nicolas Porcel (DeepMind) · Marc Lanctot (DeepMind) · Julien Perolat (DeepMind) · Richard Everett (DeepMind) · Satinder Singh (DeepMind) · Thore Graepel (DeepMind) · Yoram Bachrach ()</p>
</blockquote>
<p>Recent advances in deep reinforcement learning (RL) have led to considerable progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The purely adversarial nature of such games allows for conceptually simple and principled application of RL methods. However real-world settings are many-agent, and agent interactions are complex mixtures of common-interest and competitive aspects. We consider Diplomacy, a 7-player board game designed to accentuate dilemmas resulting from many-agent interactions. It also features a large combinatorial action space and simultaneous moves, which are challenging for RL algorithms. We propose a simple yet effective approximate best response operator, designed to handle large combinatorial action spaces and simultaneous moves. We also introduce a family of policy iteration methods that approximate fictitious play. With these methods, we successfully apply RL to Diplomacy: we show that our agents convincingly outperform the previous state-of-the-art, and game theoretic equilibrium analysis shows that the new process yields consistent improvements.</p>
<p><strong>The Value Equivalence Principle for Model-Based Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18779<br>Christopher Grimm (University of Michigan) · Andre Barreto (DeepMind) · Satinder Singh (DeepMind) · David Silver (DeepMind)</p>
</blockquote>
<p>Learning models of the environment from data is often viewed as an essential component to building intelligent reinforcement learning (RL) agents. The common practice is to separate the learning of the model from its use, by constructing a model of the environment’s dynamics that correctly predicts the observed state transitions.  In this paper we argue that the limited representational resources of model-based RL agents are better used to build models that are directly useful for value-based planning.  As our main contribution, we introduce the principle of value equivalence: two models are value equivalent with respect to a set of functions and policies if they yield the same Bellman updates. We propose a formulation of the model learning problem based on the value equivalence principle and analyze how the set of feasible solutions is impacted by the choice of policies and functions. Specifically, we show that, as we augment the set of policies and functions considered, the class of value equivalent models shrinks, until eventually collapsing to a single point corresponding to a model that perfectly describes the environment. In many problems, directly modelling state-to-state transitions may be both difficult and unnecessary. By leveraging the value-equivalence principle one may find simpler models without compromising performance, saving computation and memory.  We illustrate the benefits of value-equivalent model learning with experiments comparing it against more traditional counterparts like maximum likelihood estimation.  More generally, we argue that the principle of value equivalence underlies a number of recent empirical successes in RL, such as Value Iteration Networks, the Predictron, Value Prediction Networks, TreeQN, and MuZero, and provides a first theoretical underpinning of those results.</p>
<p><strong>Multi-agent Trajectory Prediction with Fuzzy Query Attention</strong></p>
<blockquote>
<p>ID 18799<br>Nitin Kamra (University of Southern California) · Hao Zhu (Peking University) · Dweep Kumarbhai Trivedi (University of Southern California) · Ming Zhang (Peking University) · Yan Liu (University of Southern California)</p>
</blockquote>
<p>Trajectory prediction for scenes with multiple agents and entities is a challenging problem in numerous domains such as traffic prediction, pedestrian tracking and path planning. We present a general architecture to address this challenge which models the crucial inductive biases of motion, namely, inertia, relative motion, intents and interactions. Specifically, we propose a relational model to flexibly model interactions between agents in diverse environments. Since it is well-known that human decision making is fuzzy by nature, at the core of our model lies a novel attention mechanism which models interactions by making continuous-valued (fuzzy) decisions and learning the corresponding responses. Our architecture demonstrates significant performance gains over existing state-of-the-art predictive models in diverse domains such as human crowd trajectories, US freeway traffic, NBA sports data and physics datasets. We also present ablations and augmentations to understand the decision-making process and the source of gains in our model.</p>
<p><strong>Trust the Model When It Is Confident: Masked Model-based Actor-Critic</strong></p>
<blockquote>
<p>ID 18801<br>Feiyang Pan (Institute of Computing Technology, Chinese Academy of Sciences) · Jia He (Huawei) · Dandan Tu (Huawei) · Qing He (Institute of Computing Technology, Chinese Academy of Sciences)</p>
</blockquote>
<p>It is a popular belief that model-based Reinforcement Learning (RL) is more sample efficient than model-free RL, but in practice, it is not always true due to overweighed model errors. In complex and noisy settings, model-based RL tends to have trouble using the model if it does not know when to trust the model. In this work, we find that better model usage can make a huge difference. We show theoretically that if the use of model-generated data is restricted to state-action pairs where the model error is small, the performance gap between model and real rollouts can be reduced. It motivates us to use model rollouts only when the model is confident about its predictions. We propose Masked Model-based Actor-Critic (M2AC), a novel policy optimization algorithm that maximizes a model-based lower-bound of the true value function. M2AC implements a masking mechanism based on the model’s uncertainty estimation to decide whether the model should be used or not. Consequently, the new algorithm tends to give robust policy improvements. Experiments on continuous control benchmarks demonstrate that M2AC has strong performance even when using long model rollouts in very noisy environments, and significantly outperforms previous state-of-the-art methods.</p>
<p><strong>POMDPs in Continuous Time and Discrete Spaces</strong></p>
<blockquote>
<p>ID 18802<br>Bastian Alt (Technische UniversitÃ¤t Darmstadt) · Matthias Schultheis (Technische UniversitÃ¤t Darmstadt) · Heinz Koeppl (Technische UniversitÃ¤t Darmstadt)</p>
</blockquote>
<p>Many processes, such as discrete event systems in engineering or population dynamics in biology, evolve in discrete space and continuous time. We consider the problem of optimal decision making in such discrete state and action space systems under partial observability. This places our work at the intersection of optimal filtering and optimal control. At the current state of research, a mathematical description for simultaneous decision making and filtering in continuous time with finite state and action spaces is still missing. In this paper, we give a mathematical description of a continuous-time partial observable Markov decision process (POMDP). By leveraging optimal filtering theory we derive a Hamilton-Jacobi-Bellman (HJB) type equation that characterizes the optimal solution. Using techniques from deep learning we approximately solve the resulting partial integro-differential equation. We present (i) an approach solving the decision problem offline by learning an approximation of the value function and (ii) an online algorithm which provides a solution in belief space using deep reinforcement learning. We show the applicability on a set of toy examples which pave the way for future methods providing solutions for high dimensional problems.</p>
<p><strong>Steady State Analysis of Episodic Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18805<br>Huang Bojun (Rakuten Institute of Technology)</p>
</blockquote>
<p>Reinforcement Learning (RL) tasks generally divide into two kinds: continual learning and episodic learning. The concept of steady state has played a foundational role in the continual setting, where unique steady-state distribution is typically presumed to exist in the task being studied, which enables principled conceptual framework as well as efficient data collection method for continual RL algorithms. On the other hand, the concept of steady state has been widely considered irrelevant for episodic RL tasks, in which the decision process terminates in finite time. Alternative concepts, such as episode-wise visitation frequency, are used in episodic RL algorithms, which are not only inconsistent with their counterparts in continual RL, and also make it harder to design and analyze RL algorithms in the episodic setting.In this paper we proved that unique steady-state distributions pervasively exist in the learning environment of episodic learning tasks, and that the marginal distributions of the system state indeed approach to the steady state in essentially all episodic tasks. This observation supports an interestingly reversed mindset against conventional wisdom: While steady states are traditionally presumed to exist in continual learning and considered less relevant in episodic learning, it turns out they are guaranteed to exist for the latter under any behavior policy. We further developed interesting connections for important concepts that have been separately treated in episodic and continual RL. At the practical side, the existence of unique and approachable steady state implies a general, reliable, and efficient way to collect data in episodic RL algorithms. We applied this method to policy gradient algorithms, based on a new steady-state policy gradient theorem. We also proposed and experimentally evaluated a perturbation method to enforce faster convergence to steady state in real-world episodic RL tasks.</p>
<p><strong>Learning Multi-Agent Communication through Structured Attentive Reasoning</strong></p>
<blockquote>
<p>ID 18807<br>Murtaza Rangwala (Virginia Tech) · Ryan K Williams (Virginia Tech)</p>
</blockquote>
<p>Learning communication via deep reinforcement learning has recently been shown to be an effective way to solve cooperative multi-agent tasks. However, learning which communicated information is beneficial for each agent’s decision-making process remains a challenging task. In order to address this problem, we explore relational reinforcement learning which leverages attention-based networks to learn efficient and interpretable relations between entities. On the foundation of relations, we introduce a novel communication architecture that exploits a memory-based attention network that selectively reasons about the value of information received from other agents while considering its past experiences.  Specifically, the model communicates by first computing the relevance of messages received from other agents and then extracts task-relevant information from memories given the newly received information. We empirically demonstrate the strength of our model in cooperative and competitive multi-agent tasks, where inter-agent communication and reasoning over prior information substantially improves performance compared to baselines. We further show in the accompanying videos and experimental results that the agents learn a sophisticated and diverse set of cooperative behaviors to solve challenging tasks, both for discrete and continuous action spaces using on-policy and off-policy gradient methods. By developing an explicit architecture that is targeted towards communication, our work aims to open new directions to overcome important challenges in multi-agent cooperation through learned communication.</p>
<p><strong>Information-theoretic Task Selection for Meta-Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18810<br>Ricardo Luna Gutierrez (University of Leeds) · Matteo Leonetti (University of Leeds)</p>
</blockquote>
<p>In Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of tasks to prepare for and learn faster in new, unseen, but related tasks. The training tasks are usually hand-crafted to be representative of the expected distribution of target tasks and hence all used in training. We show that given a set of training tasks, learning can be both faster and more effective (leading to better performance in the target tasks), if the training tasks are appropriately selected. We propose a task selection algorithm based on information theory, which optimizes the set of tasks used for training in meta-RL, irrespectively of how they are generated. The algorithm establishes which training tasks are both sufficiently relevant for the target tasks, and different enough from one another. We reproduce different meta-RL experiments from the literature and show that our task selection algorithm improves the final performance in all of them.</p>
<p><strong>The Mean-Squared Error of Double Q-Learning</strong></p>
<blockquote>
<p>ID 18840<br>Wentao Weng (Tsinghua University) · Harsh Gupta (University of Illinois at Urbana-Champaign) · Niao He (UIUC) · Lei Ying (University of Michigan) · R. Srikant (University of Illinois at Urbana-Champaign)</p>
</blockquote>
<p>In this paper, we establish a theoretical comparison between the asymptotic mean square errors of double Q-learning and Q-learning. Our result builds upon an analysis for linear stochastic approximation based on Lyapunov equations and applies to both tabular setting or with linear function approximation, provided that the optimal policy is unique and the algorithms converge. We show that the asymptotic mean-square error of Double Q-learning is exactly equal to that of Q-learning if Double Q-learning uses twice the learning rate of Q-learning and the output of Double Q-learning is the average of its two estimators. We also present some practical implications of this theoretical observation using simulations.</p>
<p><strong>A Unifying View of Optimism in Episodic Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18871<br>Gergely Neu (Universitat Pompeu Fabra) · Ciara Pike-Burke (Imperial College London)</p>
</blockquote>
<p>The principle of ``optimism in the face of uncertainty’’ underpins many theoretically successful reinforcement learning algorithms. In this paper we provide a general framework for designing, analyzing and implementing such algorithms in the episodic reinforcement learning problem. This framework is built upon Lagrangian duality, and demonstrates that every model-optimistic algorithm that constructs an optimistic MDP has an equivalent representation as a value-optimistic dynamic programming algorithm. Typically, it was thought that these two classes of algorithms were distinct, with model-optimistic algorithms benefiting from a cleaner probabilistic analysis while value-optimistic algorithms are easier to implement and thus more practical. With the framework developed in this paper, we show that it is possible to get the best of both worlds by providing a class of algorithms which have a computationally efficient dynamic-programming implementation and also a simple probabilistic analysis. Besides being able to capture many existing algorithms in the tabular setting, our framework can also address large-scale problems under realizable function approximation, where it enables a simple model-based analysis of some recently proposed methods.</p>
<p><strong>Accelerating Reinforcement Learning through GPU Atari Emulation</strong></p>
<blockquote>
<p>ID 18879<br>Steven Dalton (Nvidia) · iuri frosio (nvidia)</p>
</blockquote>
<p>We introduce CuLE (CUDA Learning Environment), a CUDA port of the Atari Learning Environment (ALE) which is used for the development of deep reinforcement algorithms. CuLE overcomes many limitations of existing CPU-based emulators and scales naturally to multiple GPUs. It leverages GPU parallelization to run thousands of games simultaneously and it renders frames directly on the GPU, to avoid the bottleneck arising from the limited CPU-GPU communication bandwidth. CuLE generates up to 155M frames per hour on a single GPU, a finding previously achieved only through a cluster of CPUs. Beyond highlighting the differences between CPU and GPU emulators in the context of reinforcement learning, we show how to leverage the high throughput of CuLE by effective batching of the training data, and show accelerated convergence for A2C+V-trace. CuLE is available at <a href="https://github.com/NVlabs/cule">https://github.com/NVlabs/cule</a>.</p>
<p><strong>Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations</strong></p>
<blockquote>
<p>ID 18912<br>Huan Zhang (UCLA) · Hongge Chen (MIT) · Chaowei Xiao (University of Michigan, Ann Arbor) · Bo Li (UIUC) · mingyan liu (university of Michigan, Ann Arbor) · Duane Boning (Massachusetts Institute of Technology) · Cho-Jui Hsieh (UCLA)</p>
</blockquote>
<p>A deep reinforcement learning (DRL) agent observes its states through observations, which may contain natural measurement errors or adversarial noises. Since the observations deviate from the true states, they can mislead the agent into making suboptimal actions. Several works have shown this vulnerability via adversarial attacks, but how to improve the robustness of DRL under this setting has not been well studied. We show that naively applying existing techniques on improving robustness for classification tasks, like adversarial training, are ineffective for many RL tasks. We propose the state-adversarial Markov decision process (SA-MDP) to study the fundamental properties of this problem, and develop a theoretically principled policy regularization which can be applied to a large family of DRL algorithms, including deep deterministic policy gradient (DDPG), proximal policy optimization (PPO) and deep Q networks (DQN), for both discrete and continuous action control problems. We significantly improve the robustness of DDPG, PPO and DQN agents under a suite of strong white box adversarial attacks, including two new attacks of our own. Additionally, we find that a robust policy noticeably improves DRL performance in a number of environments.</p>
<p><strong>Bridging Imagination and Reality for Model-Based Deep Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18918<br>Guangxiang Zhu (Tsinghua university) · Minghao Zhang (Tsinghua University) · Honglak Lee (Google / U. Michigan) · Chongjie Zhang (Tsinghua University)</p>
</blockquote>
<p>Sample efficiency has been one of the major challenges for deep reinforcement learning. Recently, model-based reinforcement learning has been proposed to address this challenge by performing planning on imaginary trajectories with a learned world model. However, world model learning may suffer from overfitting to training trajectories, and thus model-based value estimation and policy search will be prone to be sucked in an inferior local policy. In this paper, we propose a novel model-based reinforcement learning algorithm, called BrIdging Reality and Dream (BIRD). It maximizes the mutual information between imaginary and real trajectories so that the policy improvement learned from imaginary trajectories can be easily generalized to real trajectories. We demonstrate that our approach improves sample efficiency of model-based planning, and achieves state-of-the-art performance on challenging visual control benchmarks.</p>
<p><strong>Direct Policy Gradients: Direct Optimization of Policies in Discrete Action Spaces</strong></p>
<blockquote>
<p>ID 18932<br>Guy Lorberbom (Technion) · Chris J. Maddison (University of Toronto) · Nicolas Heess (Google DeepMind) · Tamir Hazan (Technion) · Daniel Tarlow (Google Brain)</p>
</blockquote>
<p>Direct optimization (McAllester et al., 2010; Song et al., 2016) is an appealing framework that replaces integration with optimization of a random objective for approximating gradients in models with discrete random variables (Lorberbom et al., 2018).  A* sampling (Maddison et al., 2014) is a framework for optimizing such random objectives over large spaces.  We show how to combine these techniques to yield a reinforcement learning algorithm that approximates a policy gradient by finding trajectories that optimize a random objective.  We call the resulting algorithms \emph{direct policy gradient} (DirPG) algorithms. A main benefit of DirPG algorithms is that they allow the insertion of domain knowledge in the form of upper bounds on return-to-go at training time, like is used in heuristic search, while still directly computing a policy gradient. We further analyze their properties, showing there are cases where DirPG has an exponentially larger probability of sampling informative gradients compared to REINFORCE. We also show that there is a built-in variance reduction technique and that a parameter that was previously viewed as a numerical approximation can be interpreted as controlling risk sensitivity. Empirically, we evaluate the effect of key degrees of freedom and show that the algorithm performs well in illustrative domains compared to baselines.</p>
<p><strong>A Unified Switching System Perspective and Convergence Analysis of Q-Learning Algorithms</strong></p>
<blockquote>
<p>ID 18942<br>Niao He (UIUC) · Donghwan Lee (KAIST)</p>
</blockquote>
<p>This paper develops a novel and unified framework to analyze the convergence of a large family of Q-learning algorithms from the switching system perspective. We show that the  nonlinear  ODE  models  associated  with Q-learning and many of its variants can be naturally formulated as affine switching systems.  Building on their asymptotic stability, we obtain a number of interesting results: (i) we provide a simple ODE analysis for the convergence of asynchronous  Q-learning under relatively weak assumptions; (ii) we establish the first convergence analysis of the averaging Q-learning algorithm; and (iii) we  derive a new sufficient condition for the convergence of Q-learning with linear function approximation.</p>
<p><strong>Adaptive Discretization for Model-Based Reinforcement Learning</strong></p>
<blockquote>
<p>ID 18943<br>Sean Sinclair (Cornell University) · Tianyu Wang (Duke University) · Gauri Jain (Cornell University) · Siddhartha Banerjee (Cornell University) · Christina Yu (Cornell University)</p>
</blockquote>
<p>We introduce the technique of adaptive discretization to design an efficient model-based episodic reinforcement learning algorithm in large (potentially continuous) state-action spaces. Our algorithm is based on optimistic one-step value iteration extended to maintain an adaptive discretization of the space. From a theoretical perspective we provide worst-case regret bounds for our algorithm which are competitive compared to the state-of-the-art model-based algorithms. Moreover, our bounds are obtained via a modular proof technique which can potentially extend to incorporate additional structure on the problem. From an implementation standpoint, our algorithm has much lower storage and computational requirements due to maintaining a more efficient partition of the state and action spaces. We illustrate this via experiments on several canonical control problems, which shows that our algorithm empirically performs significantly better than fixed discretization in terms of both faster convergence and lower memory usage. Interestingly, we observe empirically that while fixed discretization model-based algorithms vastly outperform their model-free counterparts, the two achieve comparable performance with adaptive discretization.</p>
<p><strong>Stateful Posted Pricing with Vanishing Regret via Dynamic Deterministic Markov Decision Processes</strong></p>
<blockquote>
<p>ID 18946<br>Yuval Emek (Technion - Israel Institute of Technology) · Ron Lavi (Technion) · Rad Niazadeh (Chicago Booth School of Business) · Yangguang Shi (Technion - Israel Institute of Technology)</p>
</blockquote>
<p>In this paper, a rather general online problem called \emph{dynamic resource allocation with capacity constraints (DRACC)} is introduced and studied in the realm of posted price mechanisms. This problem subsumes several applications of stateful pricing, including but not limited to posted prices for online job scheduling and matching over a dynamic bipartite graph. As the existing online learning techniques do not yield vanishing-regret mechanisms for this problem, we develop a novel online learning framework defined over deterministic Markov decision processes with \emph{dynamic} state transition and reward functions. We then prove that if the Markov decision process is guaranteed to admit an oracle that can simulate any given policy from any initial state with bounded loss —- a condition that is satisfied in the DRACC problem —- then the online learning problem can be solved with vanishing regret. Our proof technique is based on a reduction to online learning with \emph{switching cost}, in which an online decision maker incurs an extra cost every time she switches from one arm to another. We formally demonstrate this connection and further show how DRACC can be used in our proposed applications of stateful pricing.</p>
<p><strong>Provably Good Batch Off-Policy Reinforcement Learning Without Great Exploration</strong></p>
<blockquote>
<p>ID 18950<br>Yao Liu (Stanford University) · Adith Swaminathan (Microsoft Research) · Alekh Agarwal (Microsoft Research) · Emma Brunskill (Stanford University)</p>
</blockquote>
<p>Batch reinforcement learning (RL) is important to apply RL algorithms to many high stakes tasks. Doing batch RL in a way that yields a reliable new policy in large domains is challenging: a new decision policy may visit states and actions outside the support of the batch data, and function approximation and optimization with limited samples can further increase the potential of learning policies with overly optimistic estimates of their future performance. Some recent approaches to address these concerns have shown promise, but can still be overly optimistic in their expected outcomes. Theoretical work that provides strong guarantees on the performance of the output policy relies on a strong concentrability assumption, which makes it unsuitable for cases where the ratio between state-action distributions of behavior policy and some candidate policies is large. This is because, in the traditional analysis, the error bound scales up with this ratio. We show that using \emph{pessimistic value estimates} in the low-data regions in Bellman optimality and evaluation back-up can yield more adaptive and stronger guarantees when the concentrability assumption does not hold. In certain settings, they can find the approximately best policy within the state-action space explored by the batch data, without requiring a priori assumptions of concentrability. We highlight the necessity of our pessimistic update and the limitations of previous algorithms and analyses by illustrative MDP examples and demonstrate an empirical comparison of our algorithm and other state-of-the-art batch RL baselines in standard benchmarks.</p>
<p><strong>Off-Policy Interval Estimation with Lipschitz Value Iteration</strong></p>
<blockquote>
<p>ID 18990<br>Ziyang Tang (UT Austin) · Yihao Feng (UT Austin) · Na Zhang (Tsinghua University) · Jian Peng (University of Illinois at Urbana-Champaign) · Qiang Liu (UT Austin)</p>
</blockquote>
<p>Off-policy evaluation provides an essential tool for evaluating the effects of different policies or treatments using only observed data. When applied to high-stakes scenarios such as medical diagnosis or financial decision-making, it is essential to provide provably correct upper and lower bounds of the expected reward, not just a classical single point estimate, to the end-users, as executing a poor policy can be very costly. In this work, we propose a provably correct method for obtaining interval bounds for off-policy evaluation in a general continuous setting. The idea is to search for the maximum and minimum values of the expected reward among all the Lipschitz Q-functions that are consistent with the observations, which amounts to solving a constrained optimization problem on a Lipschitz function space. We go on to introduce a Lipschitz value iteration method to monotonically tighten the interval, which is simple yet efficient and provably convergent. We demonstrate the practical efficiency of our method on a range of benchmarks.</p>
<p><strong>Provably adaptive reinforcement learning in metric spaces</strong></p>
<blockquote>
<p>ID 18993<br>Tongyi Cao (University of Massachusetts Amherst) · Akshay Krishnamurthy (Microsoft)</p>
</blockquote>
<p>We study reinforcement learning in continuous state and action spaces endowed with a metric. We provide a refined analysis of the algorithm of Sinclair, Banerjee, and Yu (2019) and show that its regret scales with the zooming dimension of the instance. This parameter, which originates in the bandit literature, captures the size of the subsets of near optimal actions and is always smaller than the covering dimension used in previous analyses. As such, our results are the first provably adaptive guarantees for reinforcement learning in metric spaces.</p>
<p><strong>Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model</strong></p>
<blockquote>
<p>ID 18997<br>Alex Lee (UC Berkeley) · Anusha Nagabandi (UC Berkeley) · Pieter Abbeel (UC Berkeley &amp; covariant.ai) · Sergey Levine (UC Berkeley)</p>
</blockquote>
<p>Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these high-dimensional observation spaces present a number of  challenges in practice, since the policy must now solve two problems: representation learning and task learning. In this work, we tackle these two problems separately, by explicitly learning latent representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method, by learning a compact latent representation and then performing RL in the model’s learned latent space. Our experimental evaluation demonstrates that our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. Our code and videos of our results are available at our website.</p>
<p><strong>Inverse Reinforcement Learning from a Gradient-based Learner</strong></p>
<blockquote>
<p>ID 19018<br>Giorgia Ramponi (Politecnico di Milano) · Gianluca Drappo (Politecnico di Milano) · Marcello Restelli (Politecnico di Milano)</p>
</blockquote>
<p>Inverse Reinforcement Learning addresses the problem of inferring an expert’s reward function from demonstrations. However, in many applications, we not only have access to the expert’s near-optimal behaviour, but we also observe part of her learning process.In this paper, we propose a new algorithm for this setting, in which the goal is to recover the reward function being optimized by an agent, given a sequence of policies produced during learning. Our approach is based on the assumption that the observed agent is updating her policy parameters along the gradient direction. Then we extend our method to deal with the more realistic scenario where we only have access to a dataset of learning trajectories. For both settings, we provide theoretical insights into our algorithms’ performance. Finally, we evaluate the approach in a simulated GridWorld environment and on the MuJoCo environments, comparing it with the state-of-the-art baseline.</p>
<p><strong>Efficient Planning in Large MDPs with Weak Linear Function Approximation</strong></p>
<blockquote>
<p>ID 19044<br>Roshan Shariff (University of Alberta) · Csaba Szepesvari (DeepMind / University of Alberta)</p>
</blockquote>
<p>Large-scale Markov decision processes (MDPs) require planning algorithms withruntime independent of the number of states of the MDP. We consider the planningproblem in MDPs using linear value function approximation with only weakrequirements: low approximation error for the optimal value function, and a smallset of “core” states whose features span those of other states. In particular, wemake no assumptions about the representability of policies or value functions ofnon-optimal policies. Our algorithm produces almost-optimal actions for any stateusing a generative oracle (simulator) for the MDP, while its computation time scalespolynomially with the number of features, core states, and actions and the effectivehorizon.</p>
]]></content>
      <categories>
        <category>Paper List</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>NIPS 2020</tag>
        <tag>Paper List</tag>
      </tags>
  </entry>
</search>
