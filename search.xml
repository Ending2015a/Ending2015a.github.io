<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>[Note] Position Based Fluids</title>
    <url>/2020/09/02/position-based-fluids-note/</url>
    <content><![CDATA[<p><img src="https://i.imgur.com/k5HI4sV.png" alt=""></p>
<blockquote>
<p>原論文：M.Macklin and M. Müller. Position Based Fluids. ACM Transactions on Graphics, 2013.</p>
</blockquote>
<a id="more"></a>
<iframe width="100%" height="600" src="https://www.docdroid.net/K8Qac1F/pbf-sig-preprint-pdf" frameborder="0" allowtransparency allowfullscreen></iframe>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>對於互動的環境來說，流體模擬的穩定性非常重要。Smoothed Particle Hydrodynamics (SPH) 是一個在互動環境中被廣泛使用的 Particle-Based 方法，然而他卻有個致命的缺點：當粒子周圍的相鄰粒子過少時會很難維持住這個算法的穩定性，尤其以在流體表面、或是在邊緣的流體粒子更常發生。將每個 Time step 調到足夠小，或是增加足夠的粒子數量即使可避免掉這項問題，卻會大幅度增加計算成本。</p>
<p>Position Based Dynamics (PBD) 在遊戲開發或是影片製作中都是很受歡迎的一套物理模擬方式，作者選擇使用 PBD 正是因為其具有 Unconditionally Stable 以及穩定性等性質。這篇 Paper 將介紹如何使用 PBD Framework 來模擬 Incompressible flow，並克服上述在 Free Surfaces 發生 Particle Deficiency 的問題。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><ul>
<li>Muller [2003] 等人在 <a href="http://matthias-mueller-fischer.ch/publications/sca03.pdf">Particle-Based Fluid Simulation for Intereactive Applicatoins</a> 中提出使用 Smoothed Particle Hydrodynamics (SPH) 模擬具有 Viscosity 跟 Surface Tension 的流體</li>
<li>為了維持住 Incompressibility，<a href="https://cg.informatik.uni-freiburg.de/publications/2007_SCA_SPH.pdf">Weakly Compressible SPH (WCSPH)</a> [2007] 與標準 SPH 所使用的 Stiff Equation 會大大限制住 Time step 的長度。</li>
<li><a href="https://graphics.ethz.ch/~sobarbar/papers/Sol09/Sol09.pdf">Predictive-Corrective Incompressible SPH</a> [2009] 利用 Iterative Jacobi-style 方法，藉由不斷迭代、累積流體壓力並逐步施力的方式，來確保流體能夠在較長的Time step 上也能夠穩定存在，而不需要額外設置 Stiffness value 且可以分散掉不斷矯正相鄰粒子密度的計算成本。</li>
<li>Bodin [2012] 將 Incompressibility 組成一個 Velocity Constraints 的線性方程，並使用 Gauss-Seidel Iteration 解出線性方程來確保流體密度的一致。相反的，Position-Based 跟 PCISPH 則是使用類似 Jacobi Iteraion 的方法，解出非線性方程，並不斷重新估計誤差與梯度。</li>
<li>Hybrid Method，像是 Fluid Implicit-Particle (FLIP) 則是使用 Grid 來解出流體壓力並將流體速度擴散到粒子上模擬。<a href="https://www.cs.ubc.ca/~rbridson/docs/zhu-siggraph05-sandfluid.pdf">Zhu 與 Bridson</a> [2005] 結合 PIC/FLIP 方法。Raveendran [2011] 使用 Coarse grid 方法混合 SPH 趨近 divergence free velocity field。</li>
<li>Clavet [2005] 使用 Position Based 方法模擬 Viscoelastic Fluids。但是他們的方法只是 Conditionally Stable。</li>
<li>Position Based Dynamics [2007] 基於 Verlet integration 提供一個在遊戲中模擬物理的方法。</li>
</ul>
<h2 id="3-Enforcing-Incompressibility"><a href="#3-Enforcing-Incompressibility" class="headerlink" title="3. Enforcing Incompressibility"></a>3. Enforcing Incompressibility</h2><p>對每一個粒子使用 Density Constraint 來確保流體密度不變。Constraint 是每個粒子位置與鄰近粒子位置的函數，位置寫作 $\mathbf{p}_1,\cdots,\mathbf{p}_n$，以下是對第$i$個粒子的 Constraint Function：</p>
<script type="math/tex; mode=display">
C_i\big(\mathbf{p}_1,\cdots,\mathbf{p}_n\big)=\frac{\rho_i}{\rho_0}-1\tag{1}</script><p>其中 $\rho_0$是靜止密度 (Rest Density)，$\rho_i$ 是由 SPH density estimator 計算出來的粒子密度:</p>
<script type="math/tex; mode=display">
\rho_i=\sum_jm_jW\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\tag{2}</script><p>使用 <strong>Poly6</strong> 作為 Density Estimator 的 Kernel。並使用 <strong>Spiky Kernel</strong> 計算 gradient</p>
<div class="note warning">
            <p><strong>註</strong>: 由於原論文中考慮粒子質量 $m_j$ 為定值且相同，因此在後續推倒公式中皆省略不寫，但是在本篇 Note 中會將他歸至原位。</p>
          </div>
<div class="note info">
            <p><strong>註:</strong> Poly 6 為 $6^{th}$ degree polynomial kernel 的簡稱，以下為其函數:</p><script type="math/tex; mode=display">W_{\text{poly6}}\big(\mathbf{r},h\big)=\frac{315}{64\pi h^9}\begin{cases}    \big(h^2-\lVert\mathbf{r}\rVert^2\big)^3, &0\le\lVert\mathbf{r}\rVert\le h\\    0, &\lVert\mathbf{r}\rVert\gt h\end{cases}</script><p>Poly 6 的 Gradient:</p><script type="math/tex; mode=display">\nabla W_{\text{poly6}}\big(\mathbf{r},h\big)=-\frac{945}{32\pi h^9}\mathbf{r} \big(h^2-\lVert\mathbf{r}\rVert^2\big)^2</script><p>Poly 6 的 Laplacian:</p><script type="math/tex; mode=display">\nabla^2 W_{\text{poly6}}\big(\mathbf{r},h\big)=-\frac{945}{32\pi h^9}\big(h^2-\lVert\mathbf{r}\rVert^2\big)\big(3h^2-7\lVert\mathbf{r}\rVert^2\big)</script>
          </div>
<div class="note info">
            <p><strong>註:</strong> Spiky Kernel 函數:</p><script type="math/tex; mode=display">W_{\text{spiky}}\big(\mathbf{r},h\big)=\frac{15}{\pi h^6}\begin{cases}    \big(h-\lVert\mathbf{r}\rVert\big)^3, &0\le\lVert\mathbf{r}\rVert\le h\\    0, &\lVert\mathbf{r}\rVert\gt h\end{cases}</script><p>Spiky Kernel 的 Gradient:</p><script type="math/tex; mode=display">\begin{gathered}\nabla W_{\text{spiky}}\big(\mathbf{r},h\big)=-\frac{45}{\pi h^6}\frac{\mathbf{r}}{\lVert\mathbf{r}\rVert}\big(h-\lVert\mathbf{r}\rVert\big)^2,\\\lim_{r\rightarrow0^-}\nabla W_{\text{spiky}}\big(\mathbf{r},h\big)=\frac{45}{\pi h^6},\\\lim_{r\rightarrow0^+}\nabla W_{\text{spiky}}\big(\mathbf{r},h\big)=-\frac{45}{\pi h^6},\\\end{gathered}</script><p>Spiky Kernel 的 Laplacian:</p><script type="math/tex; mode=display">\begin{gathered}\nabla^2 W_{\text{spiky}}\big(\mathbf{r},h\big)=-\frac{90}{\pi h^6}\frac{1}{\lVert\mathbf{r}\rVert}\big(h-\lVert\mathbf{r}\rVert\big)\big(h-2\lVert\mathbf{r}\rVert\big),\\\lim_{r\rightarrow0}\nabla^2 W_{\text{spiky}}\big(\mathbf{r},h\big)=-\infty\end{gathered}</script>
          </div>
<p>PBD 的目標在於找出粒子位置的修正項 $\Delta\mathbf{p}$來滿足Constraint:</p>
<script type="math/tex; mode=display">
C\big(\mathbf{p}+\Delta\mathbf{p}\big)=0\tag{3}</script><p>使用牛頓法來求解:</p>
<script type="math/tex; mode=display">
\Delta\mathbf{p}\approx\nabla C(\mathbf{p})\lambda \tag{4}</script><script type="math/tex; mode=display">
\begin{align}
C\big(\mathbf{p}+\Delta\mathbf{p}\big) &\approx C\big(\mathbf{p}\big)+\nabla C^T\Delta\mathbf{p}=0\tag{5}\\
&\approx C\big(\mathbf{p}\big)+\nabla C^T\nabla C\lambda=0\tag{6}
\end{align}</script><div class="note info">
            <p><strong>註</strong>: $\lambda$ 是一個變量。這個思路其實很簡單，我們要追求每一個 Time step 都要滿足約束函數 $C$，但是粒子的位置 $\mathbf{p}$ 不一定會滿足，因此我們必須求出修正項 $\Delta \mathbf{p}$ 來修正粒子位置直到滿足約束條件。而修正項可以定義為往 $\nabla C(\mathbf{p})$ 方向乘上一個微小變量 $\lambda$。根據 Taylor 一階展開式，$C\big(\mathbf{p}+\Delta\mathbf{p}\big)\approx C\big(\mathbf{p}\big)+\nabla C^T\Delta\mathbf{p}$ 最後再將 $\Delta \mathbf{p}$ 帶入，得到式(6)。</p>
          </div>
<div class="note info">
            <p><strong>註</strong>: $\nabla C^T$ 中的 $T$ 是 Transpose，由於 $\nabla C$ 是向量，$\nabla C^T\Delta\mathbf{p}$ 表示的是向量$\nabla C$與向量$\Delta\mathbf{p}$ 的內積，因此 $\nabla C$ 會需要 Transpose。</p>
          </div>
<p>根據 SPH 方法，我們可以將粒子 $i$ 對粒子 $k$ 的約束函數 $C$ 梯度定義為：</p>
<script type="math/tex; mode=display">
\nabla_{\mathbf{p}_k}C_i=\frac{1}{\rho_0}\sum_j m_j\nabla_{\mathbf{p}_k}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\tag{7}</script><p>根據 $i$ 與 $k$ 的關係分為:</p>
<script type="math/tex; mode=display">
\nabla_{\mathbf{p}_k}C_i=\frac{1}{\rho_0}
\begin{cases}
    \displaystyle \sum_j m_j\nabla_{\mathbf{p}_k}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big) &\text{if $k=i$}\\
    \displaystyle -m_j\nabla_{\mathbf{p}_k}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big) &\text{if $k=j$}
\end{cases} \tag{8}</script><div class="note info">
            <p><strong>註</strong>: $i$ 是自身粒子，$j$ 是鄰近粒子。由於 Kernel $W$ 是 $\mathbf{p}_i-\mathbf{p}_j$ 的函數，因此對於其他 $k\ne i$ 與 $k\ne j$ 的粒子來說其梯度皆為 $0$，只有 $k=i$ 與 $k=j$ 會滿足條件。至於對 $\mathbf{p}_i$ 或 $\mathbf{p}_j$ 的梯度差別只在於方向: 由於對象都是座標 $\mathbf{p}$，因此梯度皆為針對座標每個維度作微分，只是由於 $W$ 是 $\mathbf{p}_i-\mathbf{p}_j$ 的函數，如果是對 $\mathbf{p}_i$ 取 $W$ 梯度則根據 Chain Rule 得到 $\nabla_{\mathbf{p}_i}W=W’\nabla_{\mathbf{p}_i}(\mathbf{p}_i-\mathbf{p}_j)=W’$，而對 $\mathbf{p}_j$ 取 $W$ 梯度則得到 $\nabla_{\mathbf{p}_j}W=W’\nabla_{\mathbf{p}_j}(\mathbf{p}_i-\mathbf{p}_j)=-W’$，因此式(8)在 $k=j$的情況下加了一個負號，將方向導正。</p>
          </div>
<p>將式(8)代入式(6)求得變量 $\lambda$:</p>
<script type="math/tex; mode=display">
\lambda_i=-\frac{C_i\big(\mathbf{p}_i,\cdots,\mathbf{p}_j\big)}{\sum_k\big|\nabla_{\mathbf{p}_k}C_i\big|^2}\tag{9}</script><div class="note info">
            <p><strong>註:</strong> $|\nabla_{\mathbf{p}_k}C_i|^2$ 是因為式(6)中的 $\nabla C^T\nabla C$</p>
          </div>
<p>由於約束函數(1)是非線性，當粒子逐漸分離時，Kernel的邊緣值會趨近0，導致式(9)的梯度分母逐漸趨近0，進而造成整體模擬的不穩定性。使用 Constraint Force Mixing (CFM) 則可以避免這個情況，將式(6)改寫為:</p>
<script type="math/tex; mode=display">
C\big(\mathbf{p}+\Delta\mathbf{p}\big) \approx  C\big(\mathbf{p}\big)+\nabla C^T\nabla C\lambda+\varepsilon\lambda=0\tag{10}</script><p>其中 $\varepsilon$ 是使用者自訂的 Relaxation Parameter，$\lambda_i$變成:</p>
<script type="math/tex; mode=display">
\lambda_i=-\frac{C_i\big(\mathbf{p}_i,\cdots\mathbf{p}_j\big)}{\sum_k\big|\nabla_{\mathbf{p}_k}C_i\big|^2+\varepsilon}\tag{11}</script><div class="note info">
            <p><strong>註:</strong> 簡而言之就是在分母的部份加上一個不為 $0$ 的微小常數來確保分母不會為 $0$</p>
          </div>
<p>最後修正項 $\Delta \mathbf{p}_i$ 的函數變成:</p>
<script type="math/tex; mode=display">
\Delta \mathbf{p}_i=\frac{1}{\rho_0}\sum_j\big(\lambda_i+\lambda_j\big)m_j\nabla W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\tag{12}</script><div class="note info">
            <p><strong>註:</strong> </p><script type="math/tex; mode=display">\begin{align}\Delta \mathbf{p}_i     &= \lambda_i\nabla_{\mathbf{p}_i}C_i+\sum_j\lambda_j\nabla_{\mathbf{p}_j}C_i\\    &= \frac{1}{\rho_0}\sum_j \lambda_i m_j\nabla_{\mathbf{p}_i}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big) + \Big(-\frac{1}{\rho_0}\sum_j\lambda_j m_j\nabla_{\mathbf{p}_j}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\Big)\\    &= \frac{1}{\rho_0}\sum_j \lambda_i m_j\nabla_{\mathbf{p}_i}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big) + \frac{1}{\rho_0}\sum_j\lambda_j m_j\nabla_{\mathbf{p}_i}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big) \\    &= \frac{1}{\rho_0}\sum_j\big(\lambda_i+\lambda_j\big)m_j\nabla_{\mathbf{p}_i}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\end{align}</script><p>其中第2行到第3行是因為:</p><script type="math/tex; mode=display">\because\nabla_{\mathbf{p}_j}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)=-\nabla_{\mathbf{p}_i}W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)</script>
          </div>
<h2 id="4-Tensile-Instability"><a href="#4-Tensile-Instability" class="headerlink" title="4. Tensile Instability"></a>4. Tensile Instability</h2><p>SPH 中常見的問題就是因鄰近粒子數量不足無法達到 Rest Density，而產生負壓力所導致的粒子的聚集效應 (粒子間的排斥力變成吸力)。</p>
<p><img src="https://i.imgur.com/xWMEnJj.jpg" alt=""><br><em>上圖為聚集效應產生的不自然現象 / 下圖為施加人工壓力項的結果</em></p>
<p>一種解決方案是對壓力做 Clamping 讓壓力不為負值，但是這會讓粒子的內聚力衰弱。<br><div class="note info">
            <p><strong>註:</strong><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">clamp(x) &#123;</span><br><span class="line">    return x&gt;0 ? x:0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
          </div></p>
<p>其他解決方案:</p>
<ul>
<li>Clavet [2005] 使用 second near pressure term</li>
<li>Alduan and Otaduy [2011] 使用 discrete element (DEM) forces</li>
<li>Schechter and Bridson [2012] 使用 ghost particles</li>
</ul>
<p>此篇論文使用 Monaghan [2000] 的方法，加上一個人工壓力項:</p>
<script type="math/tex; mode=display">
s_{corr}=-k\bigg(\frac{W(\mathbf{p}_i-\mathbf{p}_j, h)}{W(\Delta \mathbf{q}, h)}\bigg)^n\tag{13}</script><p>其中 $\Delta \mathbf{q}$ 是固定的距離，$k$ 是微小的正常數。通常取 $|\Delta \mathbf{q}|=0.1h, \cdots,0.3h$，$k=0.1$，$n=4$。將此項納入修正項:</p>
<script type="math/tex; mode=display">
\Delta \mathbf{p}_i=\frac{1}{\rho_0}\sum_j\big(\lambda_i+\lambda_j+s_{corr}\big)m_j\nabla W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\tag{14}</script><p>這個醜陋的項會保持粒子略低於 Rest Density，最終產生出類似 Surface Tension 的效果。</p>
<h2 id="5-Vorticity-Confinement-and-Viscosity"><a href="#5-Vorticity-Confinement-and-Viscosity" class="headerlink" title="5. Vorticity Confinement and Viscosity"></a>5. Vorticity Confinement and Viscosity</h2><p>PBD 方法通常會產生額外的 Damping 造成渦流快速消散。Fedkiw [2001] 引進 Vorticity Confinement 方法來克服模擬煙霧時的消散問題(Numerical Dissipation)，後來由 Lentine [2011] 當作 Energy Conserving 引入流體模擬。Hong [2008] 展示如何將 Vorticity Confinement 導入 Hybrid 的模擬方法。</p>
<p><img src="https://i.imgur.com/AeZSiRb.png" alt=""><br><em>左邊沒有加 Vorticity Confinement / 右邊有加 Vorticity Confinement</em></p>
<p>此篇文章使用 Vorticity Confinement 來補充流失的能量。此方法需要先計算粒子位置的 Vorticity，Monaghan [1992]:</p>
<script type="math/tex; mode=display">
\omega_i=\nabla\times\mathbf{v}=\sum_j m_j\mathbf{v}_{ij}\times\nabla\mathbf{p}_j W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\tag{15}</script><script type="math/tex; mode=display">
\mathbf{f}^{\text{vorticity}}_i=\varepsilon\big(\mathbf{N}\times\omega_i\big)\tag{16}</script><p>其中 $\mathbf{N}=\frac{\eta}{|\eta|}$，$\eta=\nabla|\omega|_i$， $\mathbf{v}_{ij}=\mathbf{v}_{j}-\mathbf{v}_{i}$ 。<br><div class="note info">
            <p><strong>註:</strong> 這邊有點小 Tricky，原論文裏面也沒有寫清楚，首先這邊目標是加速粒子的渦流速度，因此他先算出旋度向量 $\omega_i$，也就是遵守右手定則的旋轉軸心方向 (大姆指指的方向)。接著用 $\omega_i$ 的 Gradient L2-norm 算出往旋轉軸心方向的向量 $\eta$，算出單位向量 $\mathbf{N}$，最後再用外積算出旋轉方向並乘上 $\varepsilon$ 算出 Vorticity forces</p>
          </div></p>
<div class="note info">
            <p><strong>註:</strong></p><script type="math/tex; mode=display">\begin{align}\eta &= \nabla|\omega|=\nabla\bigg(\sum_{k=1}^n\omega_k^2\bigg)^{\frac{1}{2}}\\    &= \sum_{j=1}^n\frac{\partial}{\partial \omega_j}\bigg(\sum_{k=1}^n\omega_k^2\bigg)^{\frac{1}{2}}\hat{\omega_j}\\    &= \frac{1}{2}\sum_{j=1}^n\frac{2\omega_j}{\Big(\displaystyle\sum_{k=1}^n\omega_k^2\Big)^{\frac{1}{2}}}\hat{\omega_j}\\    &=\sum_{j=1}^n\frac{\omega_j}{|\omega|}\hat{\omega_j}\end{align}</script>
          </div>
<p>此外，此篇論文也使用 XSPH viscosity 來模擬流體的黏滯力:</p>
<script type="math/tex; mode=display">
\mathbf{v}_i^{new}=\mathbf{v}_i+c\sum_jm_j\mathbf{v}_{ij}\cdot W\big(\mathbf{p}_i-\mathbf{p}_j, h\big)\tag{17}</script><p>其中，參數 $c$ 通常設為 $0.1$。</p>
<h2 id="6-Algorithm"><a href="#6-Algorithm" class="headerlink" title="6. Algorithm"></a>6. Algorithm</h2><p><img src="https://i.imgur.com/PZz8Zvo.png =500x" alt=""></p>
<h2 id="7-Rendering"><a href="#7-Rendering" class="headerlink" title="7. Rendering"></a>7. Rendering</h2><p>使用 GPU based ellipsoid splatting technique 詳情見 <a href="https://zhuanlan.zhihu.com/p/38280537">液体渲染：一种屏幕空间方法</a></p>
]]></content>
      <categories>
        <category>Computer Graphics</category>
        <category>Physics Simulation</category>
        <category>Fluid Simulation</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Paper Note</tag>
        <tag>Computer Graphics</tag>
        <tag>Physics Simulation</tag>
        <tag>Fluid Simulation</tag>
        <tag>Position Based Fluids</tag>
      </tags>
  </entry>
  <entry>
    <title>[Note] AuTO: Scaling Deep Reinforcement Learnign for Datacenter-Scale Automatic Traffic Optimization</title>
    <url>/2020/09/02/auto-note/</url>
    <content><![CDATA[<p><img src="https://i.imgur.com/xLpxJlN.png" alt=""></p>
<blockquote>
<p>原論文：Li Chen, J. Lingys, Kai Chen and Feng Liu. AuTO: Scaling Deep Reinforcement Learnign for Datacenter-Scale Automatic Traffic Optimization. SIGCOMM 2018.</p>
</blockquote>
<a id="more"></a>
<div class="pdfobject-container" data-target="https://conferences.sigcomm.org/events/apnet2018/papers/auto.pdf" data-height="500px"></div>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Traffic optimization (TO) 是很重要的 datacenter management 問題，包括：</p>
<ul>
<li>flow/coflow scheduling</li>
<li>congestion control</li>
<li>load balancing &amp; routing</li>
</ul>
<p>目前 TO 普遍仰賴專家 heuristics 來 hand-craft，但實際上 TO 很注重 parameter 的 setting。如果 parameter mismatch 的話，TO 的效能反而會變得更差。例如：</p>
<ul>
<li>PIAS 的 thresholds 靠計算過去的 long term flow size distribution 來決定，因此容易發生與現在的 size distribution mismatch 的問題。效能最嚴重可能降低 38.64%</li>
<li>pFabric 也會發生類似的問題，在某些情況下即使很仔細的優化 Threshold，平均 FCT 甚至會下降 30%</li>
<li>Aalo 則因為無法動態 adaptation，因此必須仰賴操作員挑選數值。</li>
</ul>
<div class="note info">
            <p>FCT: Flow Completion Time</p>
          </div>
<p>通常，設定一個好的 TO 非常耗時，甚至會花上 1 週來選定 parameters。因為操作人員必須擁有很好的 insight、application knowledge、traffic statistics (長時間的收集)。通常步驟入下：</p>
<ol>
<li>先建立 monitoring system來蒐集終端系統的資料與數據</li>
<li>蒐集、分析、設定 parameters、在 simulation tools 上測試效果</li>
<li>將最後 heuristics 找出來最佳的 parameter 套用到 System 上</li>
</ol>
<p>因此，能夠自動化 TO 是非常吸引人的事情。此篇 Paper 設計了一套 自動化 TO 的演算法，可以套用到 volumnious、uncertain、volatile data center traffic 的環境，同時也能達到操作員期望的效果。為了測試 DRL 方法的效率，作者建立一個 flow-level centralized TO system，並使用 basic RL algorithm: Policy gradient。經測試後發現，即使使用很好的設備 (GPU) 運行 DRL，也無法在真正的 datacenter 的 scale ($\gt10^5 servers$) 下運行。關鍵在於 computation time ($~100ms$): short flow 在 DRL 下 Decision 前就已經不見了。</p>
<p>因此這篇的目標是 <em>如何使用 DRL-based 方法在 datacenter-scale 下進行自動 TO</em>。首先需要知道的是 flow 的 distribution，大部分的 flow 都是 short flow，淡大部份的 bytes 則來自 long flows。所以 TO 為 short flow 下 decision 必須快，而 long flow 的 decision 更具影響力。</p>
<p>使用 end-to-end DRL 設計 AuTO，並在 datacenter-scale 下使用 commodity hardware (一般商用硬體) 運行。AuTO 是 two-level DRL system，mimicking Peripheral Systems (PS; 周圍神經系統) and Central Systems (CS; 中樞神經系統)。PS 在所有 end-hosts 上運行，負責蒐集 flow information 並下立即的 TO decision 處理 short flow。PS 的 decision 會通過 Central System 來告知其他 host。global traffic information 都會在 CS 上集中處理。CS 也會處理 long flow 的 TO decision。</p>
<p>AuTO 的 scalability 關鍵是將耗時的 DRL 處理與需要立即下判斷的 short flow 分開。因此，使用 Multi-Level Feedback Queue 與給定的 threshold 來 schedule PS 的 flow。利用 MLFQ，PS 能夠根據 local information (bytes-sent and thresholds) 立即下決定，而 thresholds 則是通過 CS 的 DRL 來優化。透過這種方是，global TO decision 是以 MLFQ threshold 的形式從 CS 來通知所有 PS。另外，MLFQ 能夠分離 short flow 與 long flow，long flow 就會交由 CS 的 DRL 來決定他的 routing, rate limiting, 跟 priority。</p>
<div class="note info">
            <ul><li>Multi-Level Feedback Queue:<br>  Multiple FIFO queues are used and the operation is as follows:<ol><li>A new process is inserted at the end (tail) of the top-level FIFO queue.</li><li>At some stage the process reaches the head of the queue and is assigned the CPU.</li><li>If the process is completed within the time quantum of the given queue, it leaves the system.</li><li>If the process voluntarily relinquishes control of the CPU, it leaves the queuing network, and when the process becomes ready again it is inserted at the tail of the same queue which it relinquished earlier.</li><li>If the process uses all the quantum time, it is pre-empted and inserted at the end of the next lower level queue. This next lower level queue will have a time quantum which is more than that of the previous higher level queue.</li><li>This scheme will continue until the process completes or it reaches the base level queue.<ul><li>At the base level queue the processes circulate in round robin fashion until they complete and leave the system. Processes in the base level queue can also be scheduled on a first come first served basis.[4]</li><li>Optionally, if a process blocks for I/O, it is ‘promoted’ one level, and placed at the end of the next-higher queue. This allows I/O bound processes to be favored by the scheduler and allows processes to ‘escape’ the base level queue.</li></ul></li></ol></li></ul>
          </div>
<p>使用 Python 實做 AuTO，因此 AuTO can compatible with popular learning frameworks, such as Keras/TensorFlow。</p>
<p>使用 32 台 server 接上 2 台 switch 來測試 AuTO 的效果。經實驗證實，在 traffic 是 stable load and flow size sidtribution 的情況下，經過 8 小時的 training，AuTO 能夠改善效能 48.14% (與 huristics 比較) 。也顯示 AuTO 能夠穩定的學習且應用到 temporally and spatially heterogeneous traffic: 經過 8 小時的 training，AuTO 能夠減少 8.71%(9.18%) 的 average FCT (與 huristics 比較)。</p>
<h2 id="2-Background-and-Motivation"><a href="#2-Background-and-Motivation" class="headerlink" title="2. Background and Motivation"></a>2. Background and Motivation</h2><h3 id="2-1-Deep-Reinforcement-Learning-DRL"><a href="#2-1-Deep-Reinforcement-Learning-DRL" class="headerlink" title="2.1 Deep Reinforcement Learning (DRL)"></a>2.1 Deep Reinforcement Learning (DRL)</h3><p><img src="https://i.imgur.com/jkGb0Vn.png" alt=""></p>
<p>使用 policy gradient。</p>
<h3 id="2-2-Example-DRL-for-Flow-Scheduling"><a href="#2-2-Example-DRL-for-Flow-Scheduling" class="headerlink" title="2.2 Example: DRL for Flow Scheduling"></a>2.2 Example: DRL for Flow Scheduling</h3><p>解釋如何將 Flow scheduling 轉換為 DRL settings</p>
<p><strong>Flow scheduling problem</strong> 假設很多 server 跟一台 switch，network 是 non-blocking，full-bisection bandwidth 且是 proper load-balancing。在這個情況下 flow scheduling problem 可以簡化為決定 flow 的發送順序。preemptive scheduling，strict priority queuing。在每個 server 都建立 $K$ 個等級的 priority queue 來分類 flow，並遵守 strict pripority queuing。這 $K$ 個 queue 能夠透過 switches 控制，每個 flow 的 priority 能夠動態控制。每個 flow 的 packet 都會 tag 上他的 priority number。<br><div class="note info">
            <ul><li>Preemptive scheduling: Running process 可以被更高 priority 的 process 中斷</li><li>strict priority queueing: 分成 $K$ 個等級的 Queue，高等級的 process 會優先分配到 Resource。低等級的會餓死。</li></ul>
          </div></p>
<p><strong>DRL formulation</strong><br>Action space: Mapping from Active flows to priorities<br>State space: 不考量 routing 跟 load-balancing，state 只包含 flow states。flow states 用 5-tuple 表示，其中 active flow 為 $F^t_a$，deactive float (finished flow) 為 $F^t_d$，tuple 則是 source/desination IP, source/destination port numbers, and transport protocol。active flow 還多一項 priority，而 finished flow 則多兩項：FCT 跟 flow size。</p>
<p>Rewards: Reward 只有在 finished flow 才會給，finished flow $f$ 的 average throughput ${Tput}_f=\frac{\text{Size}_f}{\text{FCT}_f}$。Reward model 成與前一個 timestep $t-1$ 的總 throughput 的 ratio。</p>
<script type="math/tex; mode=display">
r_t = \frac{ \sum_{f^t\in F^t_d} {Tput}^t_f }{ \sum_{f^{t-1}\in F^{t-1}_d} {Tput}^{t-1}_f }</script><p>如果前一個 time step 的 throughput 比較大則表示 the agent has degraded the overall performance。目標是 maximize average throughput。</p>
<p><strong>DRL algorithm</strong> 使用 update rule</p>
<script type="math/tex; mode=display">
\theta \leftarrow \theta + \alpha + \sum_t \nabla_\theta \log\pi_\theta(s_t, a_t)(v_t -baseline)</script><p>其中 $v_t$ 是 empirical reward，$baseline$ 則是 cumulative average of experienced rewards per server。</p>
<h3 id="2-3-Problem-Identified"><a href="#2-3-Problem-Identified" class="headerlink" title="2.3 Problem Identified"></a>2.3 Problem Identified</h3><p>使用 PG algorithm，agent 只有一層 hidden layer。使用 two servers，一台負責 train DRL agent，另一台負責用 RPC interface 傳 traffic information，sending rate 設 1000 flows per second。</p>
<p>統計不同 implementation 的 processing latency: finish sending flow information 直到 receiving the DRL action。得到下圖：</p>
<p><img src="https://i.imgur.com/B2pNTBR.png" alt=""></p>
<p>由上圖可看出，即使只有 1 hidden layer，處理一次 flow 的 delay 還是需要至少 60ms，相當於在 1 Gbps bandwidth 上流過 7.5MB 的資料量。但根據某 well-known traffic trace 網站 與 Microsoft datacenter 的統計，7.5MB 的 flow 分別大於 99.9% 與 95.13% 的 flow。這表示大部分的 DRL action 都將無用。</p>
<h2 id="3-Auto-Design"><a href="#3-Auto-Design" class="headerlink" title="3. Auto Design"></a>3. Auto Design</h2><h3 id="3-1-Overview"><a href="#3-1-Overview" class="headerlink" title="3.1 Overview"></a>3.1 Overview</h3><p>DRL system 最關鍵的問題在於他的蒐集資料與下決策的 long latency。目前主流 datacenter 都使用 $\ge 10 Gbps$ link speed，如此一來 round-trip latency 必須至少是 sub-millisecond 等級。因此問題在於要如何利用 DRL 達成 datacenter scale 的 TO？</p>
<p>近期研究指出，大部分的 flow 都是 short flow，但大部分的 traffic bytes 都是 from long flow。因此，有個方法是，將 short flow 的都委派給 end-host 自己處理，然後 long flow 再用 DRL algorithm 處理。</p>
<p>將 AuTO 設計成 two-level system，模仿 Peripheral and Central Nervous Systems in animals。Peripheral Systems (PS) 在 end-host 上運行，負責蒐集資料、下 short flow 的決策以減少 delay。Central System (CS) 則下 long flow 的 decision。除此之外，PS 下 decision 的條件會經由 CS 蒐集、處理 global traffic information 後設定。</p>
<p><img src="https://i.imgur.com/arRlkCw.png" alt=""></p>
<h3 id="3-2-Peripheral-System"><a href="#3-2-Peripheral-System" class="headerlink" title="3.2 Peripheral System"></a>3.2 Peripheral System</h3><p>AuTO 的 scalability 的關鍵在於，PS 利用 local information 根據 globally informed TO 來下 short flow 的決策。PS 有兩個 module: enforce module and monitoring module。</p>
<p><strong>Enforcement module</strong> 使用 Multi-Level Feedback Queueing (MLFQ) 來 schedule flows。Perform packet tagging in the DSCP field of IP packets at each end-host。總共 $K$ 個 Priorities $P_i$ 其中 $1\le i\le K$，$(K-1)$ 個 threshold $\alpha_j$，$1\le j\le K-1$。設定所有 switchs 根據 DSCP field 執行 strict priority queueing。當一個 flow 在 end-host 產生時，他會先被 tag 為 $P_1$，隨著傳輸的 bytes 增加，Priority 會往後調整 $P_j$ ($2\le j \le K$)。</p>
<p><img src="https://i.imgur.com/isqJQ2o.png" alt=""></p>
<p>使用 MLFQ 可以有以下特性：</p>
<ul>
<li>PS 可以馬上根據 local information 與 threshold 來下 decisions。</li>
<li>可以適用到 global traffic variations。CS 不直接下 flow 的 decision，而是根據 global information 來控制 PS 的 threshold。</li>
<li>short flow 與 long flow 會很自然的分開，short flow 會在前面一點的 priority queue，long flow 則會到後面的 Queue。CS 可以個別處理 long flow，決定他的 routing、rate limit、 priority。</li>
</ul>
<p><img src="https://i.imgur.com/NCEBLSh.png" alt=""></p>
<p><strong>Monitoring Module</strong> 負責蒐集 flow size 與 completion times，讓 CS 能夠分析 flow 的 distribution，並制定適當的 threshold。如果出現 long flow 的話 monitoring module 也會負責回報給 CS，讓 CS 能夠下決策。</p>
<h3 id="3-3-Central-System"><a href="#3-3-Central-System" class="headerlink" title="3.3 Central System"></a>3.3 Central System</h3><p>CS 上有兩個 DRL agents：short flow RLA 負責優化 thresholds for MLFQ，long flow RLA (lRLA) 負責決定 long flow 的 rates, routes, priorities。sRLA 解決 FCT minimization 問題，使用 Deep Deterministic Policy Gradient。lRLA 使用 PG algorithm (Sec. 2.2)。</p>
<h2 id="4-DRL-Formulation-and-Solutions"><a href="#4-DRL-Formulation-and-Solutions" class="headerlink" title="4. DRL Formulation and Solutions"></a>4. DRL Formulation and Solutions</h2><h3 id="4-1-Optimizing-MLFQ-thresholds"><a href="#4-1-Optimizing-MLFQ-thresholds" class="headerlink" title="4.1 Optimizing MLFQ thresholds"></a>4.1 Optimizing MLFQ thresholds</h3><p>flow sheduling 在每個 hosts 與 network switches 上執行，使用 $K$ strict priority queues，並在每個 flow 的 IP header 的 DSCP field 設定 priority。根據 Shortest-Job-First (SJF) 的特性，愈長的 flow 則愈低 priority。</p>
<p>其中一項難點就是要如何優化 MLFQ 的 threshold。前作 [8, 9, 14] 都是使用 mathematical analysis。[9] 則建議使用 collected flow-level traces weekly/monthly re-compute threshold。AuTO 則是直接使用 DRL 方式來預測 threshold $\alpha$。<br>Sec 2.2 的 PG 是最基本的 algorithm。Agent 目標在最佳化 policy $\pi_\theta(a|s)$ parameterized by $\theta$。然而，REINFORCE 與其他 PG algorithm 都是 stochastic policies，$\pi_\theta(a|s)=P[a|s;\theta]$。PG 沒有辦法處理 real values 的 action，因此，使用類似 Deterministic Policy Gradient (DPG) 的方法來處理 real value action $\{a_0, a_1, \dots, a_n\}$，給定 state $s$，$\alpha_i=\mu_\theta(s)$。DPG 是一種 actor-critic algorithm 用來訓練 deterministic policy，actor function $\mu_\theta$ 用來表示目前的 policy，critic neural network $Q(s, a)$ 用 Bellman equation 更新。</p>
<p><img src="https://i.imgur.com/pLzg9rQ.png" alt=""></p>
<p>Agent 的 actor 負責 sample environment 並使用下式更新 parameters $\theta$：</p>
<script type="math/tex; mode=display">
\theta^{k+1} \leftarrow \theta^k+\alpha E_{s\sim\rho^{\mu^k}}\left[\nabla_\theta\mu_\theta(s)\nabla_a Q^{\mu^k}(s, a)\middle|_{a=\mu_\theta(s)}\right]</script><p>其中 $\rho^{\mu^k}$ 是 state distribution at time $k$</p>
<p>Maximize objective function：</p>
<script type="math/tex; mode=display">
\begin{align}
J(\mu_\theta)
    &=\int_\mathcal{S}\rho^\mu(s)r(s,\mu_\theta(s))ds\\
    &=E_{s\sim\rho^\mu}\left[r(s,\mu_\theta(s))\right]
\end{align}</script><script type="math/tex; mode=display">
\begin{align}
\nabla_\theta J(\mu_\theta)
    &=\int_\mathcal{S}\rho^\mu(s)\nabla_\theta\mu_\theta(s)\nabla_a Q^{\mu^k}(s, a)\Big|_{a=\mu_\theta(s)}ds\\
    &=E_{s\sim\rho^u}\left[ \nabla_\theta\mu_\theta(s)\nabla_a Q^{\mu^k}(s, a)\Big|_{a=\mu_\theta(s)} \right]
\end{align}</script><p>使用 DDPG 更新，有 4 個 network：一個 actor $\mu_{\theta^\mu}(s)$，一個 critic $Q_{\theta^Q}(s, a)$，另外兩個是 target network $\mu’_{\theta^{\mu’}}(s)$ 和 $Q’_{\theta^{Q’}}(s, a)$。random mini-batch with size $N$，transition tuple $(s_i, a_i, r_i, s_{i+1})$。當時的 state-of-the-art。</p>
<p><img src="https://i.imgur.com/6U4HTDS.png =500x" alt=""></p>
<p><strong>DRL formulation</strong> 找到一組 optimal set of threshold $\{\alpha_i\}$ 來 minimize average FCT，把這個目標轉換成 DRL problem。設 cumulative density function of flow size distribution as $F(x)$，因此 $F(x)$ 代表 probability that a flow size is no larger than $x$。$L_i$ 表示 the number of packets a given flow brings in queue $Q_i$ for $i=1,\dots, K$。因此，$E[L_i]\le (\alpha_i-\alpha_{i-1})(1-F(\alpha_{i-1}))$。設 flow arrival rate as $\lambda$，則 packet arrival rate to queue $Q_i$ 是 $\lambda_i=\lambda E[L_i]$。設 $P_1$ service rate $\mu_1=\mu$ 其中 $\mu$ 是 link 的 service rate。則 $Q_1$ 的 idle rate 就是 $(1-\rho_1)$ 其中 $\rho_i=\lambda_i/\mu_i$ 是 $Q_i$ 的 utilization rate。$Q_2$ 的 service rate 則是 $\mu_2=(1-\rho_1)\mu$。<br>得到 $\mu_i=\Pi^{i-1}_{j=0}(1-\rho_{j})\mu$，其中 $\rho_0=0$。因此 $T_i=1/(\mu_i-\lambda_i)$ 就會是 average delay。對於一個 size $[\alpha_{i-1}, \alpha_{i})$ 的 flow 來說，在不同的 queue 會經歷不同的 delay。若說 $T_i$ 是 $Q_i$ 所需的 average spent time，設 $i_{max}(x)$ 代表比 flow size $x$ 還大一個層級的 threshold，則 flow size $x$ 的 average FCT $T(x)$ 會有 upper bound: $\sum^{i_{max}(x)}_{i=1}T_i$。</p>
<div class="note info">
            <ul><li>$\alpha_i$ 是 threshold 其實就是 flow 的 size</li><li>$\mu$ 代表 link 的使用 rate，$\rho_i$ 代表某 queue $Q_i$ 佔用 link 的比例，由於先給 priority 高的使用因此 priority 低的就用剩下的部份，低 priority 的 service rate (佔 link 的比例) 就會是 $\mu_i=\Pi^{i-1}_{j=0}(1-\rho_{j})\mu$ (link 的 total capacity 乘上前面所有 queue 用剩的比例)</li></ul>
          </div>
<p>Let $g_i=F(\alpha_i)-F(\alpha_{i-1})$ 表示 percentage of flows with size $[\alpha_{i-1}, \alpha_{i})$。則可以 formulate FCT minimization problem:</p>
<script type="math/tex; mode=display">
\begin{eqnarray}
&\min_{\{g\}} 
    &\mathcal{T}(\{g\})=\sum^K_{l=1}(g_l\sum^l_{m=1}T_m)=\sum^K_{l=1}(T_l\sum^K_{m=l}g_m)\\
&\text{subject to}~~~
    & g_i\ge0, i=1,\dots,K-1
\end{eqnarray}</script><p><em>State space</em>: states are the set of, the set of all finished flows $F_d$ inthe entire network in the current time step。每個 flow 用 5-tuple 表示：source/destination IP, source/destination port number, transport protocal。finished flows 還包含 FCT 與 flow size。共 7 個 feature。</p>
<p><em>Action space</em>: action space 由 centralized agent, sRLA 處理。action 是一組 MLFQ threshold ${\alpha^t_i}$。</p>
<p><em>Rewards</em>: Reward 是 delayed feedback 來告訴 agent 在前一個 timestep 的 action 如何。reward 設為與前一個 objective (expectation of FCT) 的 ratio $r_t=\frac{\mathcal{T}^{t-1}}{\mathcal{T}^{t}}$，如果效能變差了 $r_t\le1$，變好了則 $r_t\ge 1$。</p>
<p><strong>DRL algorithm</strong> 使用 buffer 儲存 tuple $(s_t, a_t, r_t, s_{t+1})$。</p>
<h3 id="4-2-Optimizing-Long-Flows"><a href="#4-2-Optimizing-Long-Flows" class="headerlink" title="4.2 Optimizing Long Flows"></a>4.2 Optimizing Long Flows</h3><p>MLFQ thresholds $\alpha_{K-1}$ 將 long flow 與 short flow 區分出來，$\alpha_{K_1}$ 是會動態更新的，不像前作是 fixed [1, 22] 的。lRLA 使用 PG algorithm，差別只在於 Action space。</p>
<p><em>Action space</em>: 對每個 active flow $f$ 在 timestep $t$ 時的 action 是 $\{Prio_t(f), Rate_t(f), Path_t(f)\}$。其中 $Prio_t(f)$ 是 flow priority，$Rate_t(f)$ 是 rate limit，$Path_t(f)$ 是 path to take for flow $f$。假設 path 是跟前作 XPath [32] 一樣的方式定義。</p>
<p><em>State space</em>: 同 Sec 2.2</p>
<p><em>Rewards</em>: Reward 由 finished flows 定義，可以是：difference or ratios of sending rate, link utilization, throughput in consecutive timesteps。在 $10Gbps$ 的 link 下很難測量 active flows 的時間性的 flow-level information，因此只使用 finished flows，並使用 average throughputs 的 ratio。</p>
<script type="math/tex; mode=display">
r_t = \frac{ \sum_{f^t\in F^t_d} {Tput}^t_f }{ \sum_{f^{t-1}\in F^{t-1}_d} {Tput}^{t-1}_f }</script><h2 id="5-Implementation"><a href="#5-Implementation" class="headerlink" title="5. Implementation"></a>5. Implementation</h2><p>使用 Python 2.7、Keras</p>
<h3 id="5-1-Peripheral-System"><a href="#5-1-Peripheral-System" class="headerlink" title="5.1 Peripheral System"></a>5.1 Peripheral System</h3><p>PS 有 Monitoring Module (MM) 跟 Enforcement Module (EM)。MM 負責蒐集 information，包括 recently finished flows 以及 presently active long flows。MM 會固定時間回傳資訊給 CS。EM 則負責 tagging active flows，以及 routing、rate limiting、priority tagging for long flows。本篇使用 Remote Procedure Call (RPC) 來實作傳輸界面。</p>
<h4 id="5-1-1-Monitoring-Module"><a href="#5-1-1-Monitoring-Module" class="headerlink" title="5.1.1 Monitoring Module"></a>5.1.1 Monitoring Module</h4><p>MM 雖然可以實做成 Linux kernel module，如 PIAS [8]，但因為這次實驗是用 flow generator 來產生 flow，所以 Monitoring Module 直接實做在 generator 裡面。</p>
<p>每 $T$ 秒，MM 會將 $n_l$ 個 active long flows (6 個 attributes) 跟 $m_l$ 個 finished long flows (7 個 attributes) 以及 $m_s$ 個 active short flows (7個 attributes) 整合起來回傳給 CS。</p>
<p>$\{n_l,m_l,m_s\}$ 根據 traffic load 跟 $T$ 來決定，這幾個變數會是 upper-bound，若每 $T$ 秒無法蒐集齊的話，會 zero-padding，會這樣做是因為 DNN 的 Input 大小是固定的。而因實驗使用 flow generator，所以這部份可以控制。本篇使用 $\{n_l=11,m_l=10.m_s=100\}$<br><div class="note success">
            <p>Future work: Dynamic DNN、RNN (Dynamic input size)</p>
          </div></p>
<h4 id="5-1-2-Enforcement-Module"><a href="#5-1-2-Enforcement-Module" class="headerlink" title="5.1.2 Enforcement Module"></a>5.1.2 Enforcement Module</h4><p>EM 會定期接收 CS 的 action。包括 MLDQ thresholds 跟 local long flows 的 TO decisions。EM 是建立在 PIAS [8] kernel module，並加上可以 dynamic configuration threshold。</p>
<p>short flow 使用 ECMP 處理 routing 跟 load-balancing，因為 short flow 不需要 centralized per-flow control 跟 DCTCP for congestion control。</p>
<p>long flows 則使用 TO actions，包括 priority、rate limiting、routing。使用相同的 kernel module 還做 priority tagging。Rate limiting 使用 Hierarchical token bucket (HTB) queueing discipline in Linux traffic control (TC)。使用 parent class in HTB 來控制 total outbound bandwidth，讓 CS 能夠控制每個 Node 的 outbound rate limit。每當一個 flow 被判定為 long flow (掉到 MLFQ 最後一個 queue) EM 就會 create 一個 HTB filter，利用 5-tuple 來 filter 掉 long flow。當 EM 收到 rate allocation decisions from the CS，EM 就會發送 Netlink messages 到 Linux kernel 來更新 the child class of the particular flow：TC class 的 rate 設定成 CS 指定的 rate，而 TC class 的 rate 的 ceiling 則是設定成 CS 指定的兩倍但不超過 original rate。</p>
<h3 id="5-2-Central-System"><a href="#5-2-Central-System" class="headerlink" title="5.2 Central System"></a>5.2 Central System</h3><p>CS run RL agents (sRLA &amp; lRLA) 來 optimize TO decisions。使用 SEDA-link architecture [58] 來處理 incoming updates 並 sending actions to PS。architecture 分成幾個 stages：http request handling，Deep network learning/processing，response sending。每個 stage 都會有各自的 processes，之間使用 queue 來傳輸訊息。確保 CS 的 cores 都能夠被用上、處理所有 PS 的 requests、分攤 load。</p>
<h4 id="5-2-1-sRLA"><a href="#5-2-1-sRLA" class="headerlink" title="5.2.1 sRLA"></a>5.2.1 sRLA</h4><p>使用 Keras 實做 sRLA 並使用 DDPG algorithm。<br><em>Actors</em>: two fully-connected hidden layers with 600 and 600 neurons。output layer with $K-1$ output units (one for each threshold)。input layer 吃 states (700 features per-sever ($m_s=100$)) and outputs MLFQ thresholds for each host for timestep $t$</p>
<p><em>Critics</em>: three hidden layers。輸入 states，最後一層 hidden layer 前會 concat output from actors。</p>
<p>從 replay buffer 裡面 sample mini-batches of experiences: $\{s_t,a_t,r_t,s_{t+1}\}$。</p>
<h4 id="5-2-2-lRLA"><a href="#5-2-2-lRLA" class="headerlink" title="5.2.2 lRLA"></a>5.2.2 lRLA</h4><p>10 hidden layers 全部都是 fully-connected layer with 300 neurons。input states (136 features per-server ($n_l=11,m_l=10$)) output probabilities for the actions for all the active flows。</p>
<p><strong>Summary</strong><br>hyper-parameters 是經過實驗挑選的，發現更深的 network 並沒有表現的多好反而花更長時間 training。綜合所有考量 (準確度、training 時間、時間延遲)，認為這樣的參數設定是最好的。</p>
<h2 id="6-Evaluation"><a href="#6-Evaluation" class="headerlink" title="6. Evaluation"></a>6. Evaluation</h2><p>主要 evaluate</p>
<ol>
<li>在 Stable traffic (固定 flow size distribution and traffic load) 情況下 AuTO 與 standard heuristics 的差</li>
<li>在 varying traffic characteristics 情況下 AuTO 適不適用</li>
<li>AuTO 的反應 traffic dynamics 的速度</li>
<li>performance overheads and overall scalability</li>
</ol>
<p><strong>Summary of results (grouped by scenarios):</strong></p>
<ul>
<li><strong>Homogeneous</strong>: fixed flow size distribution 跟 load 情況下，AuTO 的 threshold 能 converge 且 average FCT 表現的比 standard heuristics 快 48.14% </li>
<li><strong>Spatially Heterogeneous</strong>: 將 server 分成 4 個 clusters，各自產生不同 flow size distribution 跟 load；AuTO 的 threshold 能 converge，且 average FCT 表現的比 standard heuristics 快 37.20%</li>
<li><strong>Spatially &amp; Temporally Heterogeneous</strong>: 同上述情景，但 flow size distribution 跟 load 會回時間改變，AuTO 能夠 learning 且展現很好的 adaptation。跟 fixed heuristics 比較，heuristics 只有在某些特定的 traffic settings combination 下能夠險勝 AuTO。</li>
<li><strong>System Overhead</strong>: AuTO implementation 能夠在 10ms 內給予立即的 state 更新。AuTO 同時也展現在 CPU urilization 和 throughput degradation 方面的 minimal end-host overhead。</li>
</ul>
<p><strong>Setting</strong> AuTO 實驗使用 32 台 server。Switch 支援 ECN 跟 struct priority queueing 最多有 8 個 queues。Server 則是 Dell PowerEdge R320 + 4-core Intel E5-1410 2.8GHz CPU，8G memory，and Broadcom BCM5719 NetXtreme Gigabit Ethernet NIC with 4x1Gbps ports。系統則是 64-bit Debian 8.7 (3.16.39-1 Kernel)。By default, advanced NIC offload mechanisms are enabled to reduce the CPU overhead. The base round-trip time (RTT) of our testbed is 100us。</p>
<p><img src="https://i.imgur.com/pzANneq.png" alt=""></p>
<p>使用前作 [2, 7, 9, 15] 使用的 traffic generator [20]，給定 flow size distribution 跟 traffic load 來自動產生 traffic flows。</p>
<p>使用兩種 realistic workloads：web search workload 跟 data mining workload。其中 15 台負責產生 flow 的叫做 application server，剩下一台就是 CS。每台 application server 會有 3 個 ports 連到 data plane switch，剩下的 port (1個) 連到 control plane switch 跟 CS 溝通。3 個 port 設定在不同的 subnet。兩台 Switch 都是 Pronto-3297 48-port Gigabit Ethernet switch。<br><img src="https://i.imgur.com/v6AZRML.png" alt=""></p>
<p><strong>Comparison Targets</strong> 跟兩種 heuristics 比較：Shortest-Job-First (SJF) 與 Least-Attained-Service-First (LAS)。差異在於，SJF 需要在一開始就給定 flow size。這兩個算法都需要蒐集一段時間才能決定 threshold，通常會蒐集幾周 (表示每更新一次 threshold 都需要隔幾周)。<br>實驗使用 quantized SJF 跟 LAS with 4 priority levels。</p>
<ul>
<li>Quantized SJF (QSJF): 三個 thresholds $\alpha_0&lt;\alpha_1&lt;\alpha_2$。直接從 flow generator 得到 flow size。最小 size 的 flow 有最高 priority。</li>
<li>Quantized LAS (QLAS): 三個 thresholds $\beta_0&lt;\beta_1&lt;\beta_2$。每個 flow 一開始都是最高 priority 直到送超過 $\beta_i$ 時就會降低 priority。</li>
</ul>
<p>Thresholds 則是根據 flow size distribution and traffic load 按照 [14] 方式計算。除非有指定，否則都是使用 DCTCP distribution at 80% load 情景計算出來的 thresholds。</p>
<h3 id="6-1-Experiments"><a href="#6-1-Experiments" class="headerlink" title="6.1 Experiments"></a>6.1 Experiments</h3><h4 id="6-1-1-Homogeneous-traffic"><a href="#6-1-1-Homogeneous-traffic" class="headerlink" title="6.1.1 Homogeneous traffic"></a>6.1.1 Homogeneous traffic</h4><p>flow size distribution 跟 load 都固定。Web Search (WS) 跟 Data Mining (DM) distribution 設定 80% load。這兩個 distribution 分別代表不同 group 的 flows: a mixture of short and long flows (WS), a set of short flows (DM)。average 跟 99th percentile FCT 如下：<br><img src="https://i.imgur.com/H1BypuL.png" alt=""></p>
<p>Train AuTO 8小時，使用 train 完後的 model schedule flow for one hour。</p>
<ul>
<li>mixture of short and long flow (WS)，AuTO 比 standard heuristics 快 48.14% average FCT。因為 AuTO 可以動態調整 long flow 的 priority 避免 starvation problem。</li>
<li>short flows (DM)，AuTO 表現跟 heuristics 差不多，因為 AuTO 在一開始也是給所有 flow 最高的 priority，因此表現結果跟 QLAS 差不多。</li>
<li>RL Training 期間能夠讓 average FCT 減少 18.61% 跟 4.12% for WS&amp;DM。</li>
<li>將 incast traffic 獨立出來看，發現 QLAS 跟 QSJF 表現差不多，因為這在 congestion control 就已經處理好。DCTCP 已經把 incast 處理的很好。<div class="note info">
            <p>incast traffic: 多對一傳輸</p>
          </div>
</li>
</ul>
<h4 id="6-1-2-Spatially-heterogeneous-traffic"><a href="#6-1-2-Spatially-heterogeneous-traffic" class="headerlink" title="6.1.2 Spatially heterogeneous traffic"></a>6.1.2 Spatially heterogeneous traffic</h4><p>把 server 分成 4 個 clusters，create spatially hetrogeneous traffic。分別分成四種不同的 load 跟 distribution：<WS 60%>，<WS 80%>，<DM 60%>，<DM 60%></p>
<p><img src="https://i.imgur.com/fMKDPEf.png" alt=""></p>
<p>heuristics 的 thresholds 4 個 clusters 分別根據 distribution 與 load 計算。比較結果後發現跟 homogeneous traffic 的情況很像。 AuTO 跟 QLAS 比，average FCT 快 37.20%，99th percentile FCT 快 19.78%；跟 QSJF 比，average FCT 快 27.95%，99th percentile FCT 快 11.98%。</p>
<h4 id="6-1-3-Temporally-amp-spatially-heterogeneous-traffic"><a href="#6-1-3-Temporally-amp-spatially-heterogeneous-traffic" class="headerlink" title="6.1.3 Temporally &amp; spatially heterogeneous traffic"></a>6.1.3 Temporally &amp; spatially heterogeneous traffic</h4><p>每小時改變 flow size distribution 與 network load：load value 為 {60%, 70%, 80%}，distribution 則是隨機挑選，並確保每個小時的 load 與 distribution 會不同。實驗 run 8 小時。</p>
<p><img src="https://i.imgur.com/yEUC2Xx.png" alt=""><br><img src="https://i.imgur.com/G5axuHf.png" alt=""></p>
<ul>
<li>heuristics with fixed parameters，當 traffic 特性 match 到 parameters 的時候 average 99th percentile FCT 都表現的比其他方法好。但當 mismatch 時，FCT 瞬間變差，顯示他的適應 dynamic traffic 的能力差。</li>
<li>AuTO 能夠學習適應 dynamiccally changed traffic，最後一個 hour，AuTO achieves 8.71%(9.18%) average (99th percentile) FCT compared to QSJF。這是因為 AuTO 使用 2 個 agent 來學習並動態調整 priorities of flows。不需人為干涉，能夠自動調整。</li>
</ul>
<p>觀察 AuTO 可以發現 a constant decline in FCTs 表示他能夠在 dynamic traffic 下學習，最後收斂到 local optimum。表示 datacenter traffic scheduling 可以轉換為 RL problem and DRL techniques can be applied to solve it。</p>
<h3 id="6-2-Deep-Dive"><a href="#6-2-Deep-Dive" class="headerlink" title="6.2 Deep Dive"></a>6.2 Deep Dive</h3><h4 id="6-2-1-Optimizing-MLFQ-thresholds-using-DRL"><a href="#6-2-1-Optimizing-MLFQ-thresholds-using-DRL" class="headerlink" title="6.2.1 Optimizing MLFQ thresholds using DRL"></a>6.2.1 Optimizing MLFQ thresholds using DRL</h4><p>在 60% load 的環境下 train sRLA for 8 小時之後，跟 PIAS 方法算出來的 threshold 做比較，發現除了最後一個 threshold 有差之外其他都差不多。</p>
<p><img src="https://i.imgur.com/HVzSsc3.png" alt=""></p>
<p>把 average FCT 跟 99t percentile FCT 畫出來看發現，兩個的 performance 差不多。因此下結論，AuTO train 8 個小時後 performance 跟 PIAS 差不多。</p>
<p><img src="https://i.imgur.com/nWnlRe2.png" alt=""><br><img src="https://i.imgur.com/ocnTpii.png" alt=""></p>
<h4 id="6-2-2-Optimizing-Long-Flows-using-DRL"><a href="#6-2-2-Optimizing-Long-Flows-using-DRL" class="headerlink" title="6.2.2 Optimizing Long Flows using DRL"></a>6.2.2 Optimizing Long Flows using DRL</h4><p>在 experiment (Sec 6.1.3) 時持續 5 分鐘紀錄 long flow 的數量。設 $L$ 表示 the set of all links，$N_l(t)$ 表示 the number of long flows on link $l\in L$。$N(t)=\{N_l(t),\forall l\}$，下圖表示 $\max(N(t))-min(N(t))m \forall t$，用來說明 load imbalance。</p>
<p><img src="https://i.imgur.com/aaAxoY6.png" alt=""></p>
<p>發現通常 imbalance 不超過 10 個 flow。即使偶爾發生很不 imbalance 的情況，lRLA 會自動調整將過多的 flow 送到比較少的 link 上。這是因為如 Sec 2.2 所說，使用 Throughput 當作 reward，當多個 flows 集中在同一條 link 上，他的 throughput 會比分散在各個 link 上還低。</p>
<h4 id="6-2-3-System-Overhead"><a href="#6-2-3-System-Overhead" class="headerlink" title="6.2.3 System Overhead"></a>6.2.3 System Overhead</h4><p>探討 AuTO performance 跟 System overhead。首先先探討 CS 的 response latency，以及 scalability。接著探討 end-host PS 的 overhead</p>
<p><strong>CS Response Latency</strong> 測量方式如下：$t_u$ 代表 CS 收到 update 的時間點，$t_s$ 代表 CS 回饋 action 的時間點。response time 是 $t_s-t_u$。發現 CS 平均可以在 10ms 內回應所有 server。這個時間是由 computation overhead of DNN 跟 update queueing delay 造成。AuTO 目前只用 CPU。<br><div class="note success">
            <p>能夠保證降低 latency 的方式是 CPU-GPU hybrid training and serving，CPU 負責 interfact with the environment，GPU 負責在背景 training model。</p>
          </div><br><img src="https://i.imgur.com/UBxN26v.png" alt=""></p>
<p>Response latency 也會隨 DNN computation complexity 增加。AuTO 的 network size 是由 $\{n_l, m_l, m_s\}$ 決定。若把 $\{n_l, m_l\}$ 從 ${11, 10}$ 增加到 ${1000, 1000}$，average response time 會變成 81.82ms。下圖為增加 $m_s$ 數量的走勢圖。<br><img src="https://i.imgur.com/NUGOXTW.png" alt=""></p>
<p>發現 $m_s$ 越大， response latency 增加愈慢，因為 $m_s$ 數量會決定 input layer size。只會影響 input layer 到第一層 hidden layer 的 matrix size。<br><div class="note success">
            <p>未來如果 AuTO 使用更複雜的 DNN，可以利用 parallelization techniques for DRL [6, 25, 27, 39] 來 reduce the response latency。</p>
          </div></p>
<p><strong>CS Scalability</strong> 實驗環境比較小，CS 的 NIC capacity 並沒有用滿，monitoring flow 所使用的 bandwidth 只有 12.40Kbps per server。假設 1Gbps network interface，CS 最多可以 monitor 80.64K servers。或是利用下列方法提昇 CS scalability<br><div class="note success">
            <ol><li>現行的 datacenter 使用 10Gbps 或更高的 network interface</li><li>CS 使用 GPUs 或其他加速計算的裝置</li><li>reduce the bandwidth of monitoring flows by compression</li></ol>
          </div></p>
<p><strong>PS Overhead</strong> 測量 CPU utilization，跟 throughput reduction。嘗試測量沒有使用 MM 跟 EM 發現 overhead 差異極小 (CPU utilization 差異少於 1%)，表示 AuTO 的 throughput 跟 GPU overhead 極小，跟 PIAS 差不多。</p>
<h2 id="7-Related-Works"><a href="#7-Related-Works" class="headerlink" title="7. Related Works"></a>7. Related Works</h2><h2 id="8-Conclution-Remarks"><a href="#8-Conclution-Remarks" class="headerlink" title="8.Conclution Remarks"></a>8.Conclution Remarks</h2><div class="note success">
            <p>RL algorithm for congestion control and task scheduling<br>WAN bandwidth management</p>
          </div>
]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Reinforcement Learning</category>
        <category>Applications</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Paper Note</tag>
        <tag>Deep Learning</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Datacenter Management</tag>
        <tag>Traffic Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>[Note] 神的語言 Metaprogramming: one_of</title>
    <url>/2020/09/01/cpp-metaprogramming-1/</url>
    <content><![CDATA[<h2 id="發想"><a href="#發想" class="headerlink" title="發想"></a>發想</h2><p>最近在寫程式的時候遇到一個情景，讓我非常困擾。<br>下面這個情況我不需要多加解釋，應該很多人也都有遇過類似這種困擾。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">test_status</span><span class="params">(STATUS_t unknown_status)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(unknown_status == STATUS_1 || unknown_status == STATUS_3 || unknown_status == STATUS_5 || unknown_status == STATUS_7)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// do some stuff</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(unknwon_status == STATUS_2 || unknown_status == STATUS_3 || unknown_status == STATUS_4)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// do some stuff</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(unknwon_status == STATUS_5 || unknwon_status == STATUS_6 || unknwon_status == STATUS_9 || unknwon_status == STATUS_11 || unknwon_status == STATUS_27 || unknwon_status == STATUS_38)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//do some stuff</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>當你遇到狀況 1、3、5、7 時要處理一些事情，遇到狀況 2、3、4 時要處理一些事情，最後遇到狀況 5、6、9、11、27、38 的時候又要再處理一些事情….，可能有些人遇到的，後面還要再拉更多的 if 跟重複寫一堆相同且沒有意義的變數名 (unknown_status) 跟operator (||)。不但程式變得很長，不容易閱讀，你也很容易在寫這一長串的時候不小心出錯，例如 operator == 寫成 operator = ，結果還 de 不出 bug，諸如此類的小陷阱。</p>
<p>當然你可以抱怨說，到底是誰這麼沒水準，定義這種沒有規則的 STATUS。但是有時候可能你因為一些被限制的因素而只能使用這種不符合你預期的規則的 lib ，你也無從選擇只好接受。</p>
<p>於是就讓我萌生了一些想法：<strong>我有沒有辦法用一個很簡單的表達式來省略掉這些高度重複的變數名、還有 operator</strong>。當然其實我早就知道這是可行的，而且方法非常多，隨便想都可以想的到 3~5種偷懶的方法，像是開個 vector 把狀況們都捆成一包，再用 for 回圈去檢查、或是用 std::any_of 搭配 lambda function 的方式解決，又甚至自己重新 mapping 一次 STATUS，變成可以使用 Binary OR 的方式檢查。</p>
<p>問題就在於，我要如何在解決問題的同時又能夠解決的<strong>漂亮</strong>，這是一個很大的問題。我當然我也可以選擇不動腦就寫一堆垃圾 Code 來解決這種不起眼的小問題，但是這就不是我的 Style 啦。於是我決定要動手設計了一個新的 operator (我稱他是 operator 啦，雖然他只是一堆 function 跟 struct 的疊加)，這個 operator 的特點就是看起來要極其順眼，非常容易使用，最重要的是<strong>在 compile 之後的效能要能夠跟原本的暴力破解垃圾 Code 不相上下。</strong></p>
<p>於是我第一個想到的 operator 就是 one_of，這是我的目標：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">test_status</span><span class="params">(STATUS unknown_status)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(unknown_status == one_of(STATUS_1, STATUS_3,</span><br><span class="line">                                STATUS_5, STATUS_7))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//do some stuff</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(unknwon_status == one_of(STATUS_2, STATUS_3, STATUS_4))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// do some stuff</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(unknwon_status == one_of(STATUS_5, STATUS_6,</span><br><span class="line">                                STATUS_9, STATUS_11,</span><br><span class="line">                                STATUS_27, STATUS_38))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//do some stuff</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>跟原本的寫法比起來是不是變得更順眼、更易讀、更易寫（不容易出錯）？這個 operator 非常口語化的詮釋了我想做的事情：</p>
<ul>
<li>if: 當</li>
<li>unknown_status: 某變數</li>
<li>==: 等於</li>
<li>one_of: 下列其中一個</li>
<li>STATUS_1, STATUS_3, STATUS_5, STATUS_7</li>
<li>我就 do some stuff</li>
</ul>
<h2 id="目標"><a href="#目標" class="headerlink" title="目標"></a>目標</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(unknown_status == one_of(STATUS_1, STATUS_3,</span><br><span class="line">                            STATUS_5, STATUS_7))</span><br><span class="line">&#123;</span><br><span class="line">        <span class="comment">//do some stuff</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我想要定義一個新的 operator one_of 來解決掉一堆難看的垃圾 Code 問題。而且他擁有以下特點：</p>
<ul>
<li>易寫、易讀</li>
<li>可以接受不定個數的參數 (可以使用 parameter pack 實現)</li>
<li>可以接受任何型態 (可以使用 template 實現)</li>
<li>編譯後效能可以跟原本的垃圾 Code 一樣好 (metaprogramming 實現)</li>
</ul>
<p>你看看，從上列開出的特點來看，就是只能用 metaprogramming 實現了。</p>
<h2 id="實現"><a href="#實現" class="headerlink" title="實現"></a>實現</h2><p>好了，現在有了目標，問題就在於要如何實現這個 one_of？</p>
<p>首先，可以看的出來，我們要把 unknown_status 跟 one_of(…) 做比較，能做到的方法其實就幾個：第一個是 one_of 可能是一個 struct/class，他的 Constructor 能夠接納無限個 parameters，然後我拿某個變數 unknown_status 跟這個 struct/class 做 <code>==</code> 比較 (operator overloading)。第二個是 one_of 可能是一個 function，他能夠接納無限個 parameters，呼叫後會回傳一個包好的 struct/class (我傳入的 parameters 都在裏面)，然後再用 unknown_status 去跟這個 struct/class 做 <code>==</code> 比較 (operator overloading)。</p>
<p>我們使用第一個方法來實作，流程是這樣，one_of 的 constructor 可以接受 parameter pack，之後我們將 parameter pack 存到 <code>std::tuple</code> 裏面放著，等到呼叫 == 時再從 <code>std::tuple</code> unpack 一個一個做判斷：</p>
<p>所以第一步先建立好 one_of 的 constructor (這邊使用 struct 是因為 struct 的 member 預設是 public，我們不需要再多一步用 <code>public:</code> 來指定)</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">one_of</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line">    one_of(Ts&amp;&amp;...args);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>這邊的語法 <code>typename...Ts</code> 就是 parameter pack，意思就是我會傳入不定個數的參數。不多解釋，不懂的自己去 <code>google</code>。接下來我們要把傳進來的參數 <code>args</code> 包成一份 <code>std::tuple</code>，而因為 <code>std::tuple</code> 也需要不定個數的欄位，我們勢必必須把 <code>struct one_of</code> 也宣告成 template struct：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;tuple&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span></span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">one_of</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="built_in">std</span>::tuple&lt;ArgTypes...&gt; args;  <span class="comment">//tuple</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line">    one_of(Ts&amp;&amp;...args) : args(<span class="built_in">std</span>::forward&lt;Ts&gt;(args)...)&#123;&#125;;  <span class="comment">// constructor</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>這邊我們使用 <code>std::tuple</code> 因此我們必須 <code>#include &lt;tuple&gt;</code>，還有我們使用 <code>std::forward</code> 來 unpack parameter pack 變成一列 arguments 傳進 tuple 的 constructor 裏面結束這回合，因此我們還必須 <code>#include &lt;utility&gt;</code>。</p>
<p>但是接下來我意識到一件事情，如果我們使用這種方法來設計我們的 one_of 的話，由於現在 one_of struct 已經變成 template struct 了，到時候呼叫的方法就會變成:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(unknown_status == one_of&lt;STATUS_t, STATUS_t, STATUS_t, STATUS_t&gt;(STATUS_1, STATUS_3, STATUS_5, STATUS_7))</span><br></pre></td></tr></table></figure></p>
<p><em>喔不！</em> 我們必須指定傳入參數的型態給 template！這可不是我們當初所預期的 one_of 啊！<br>但是不用擔心，他還有救，讓我們來給他包上一層 helper function：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;tuple&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span></span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="built_in">std</span>::tuple&lt;ArgTypes...&gt; args;  <span class="comment">//tuple</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line">    _type_one_of(Ts&amp;&amp;...args) : args(<span class="built_in">std</span>::forward&lt;Ts&gt;(args)...)&#123;&#125;;  <span class="comment">// constructor</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line">constexpr auto one_of(ArgTypes&amp;&amp;... args) -&gt; _type_one_of&lt;ArgTypes...&gt;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> _type_one_of&lt;ArgTypes...&gt;(<span class="built_in">std</span>::forward&lt;ArgTypes&gt;(args)...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我做了什麼事情呢？<br>首先，我把 <code>struct one_of</code> 改名了，改成 <code>struct _type_one_of</code> 接著我新增了一個 helper function 就是我們最愛的互動介面 <code>one_of</code>。他的功能就是當我們傳入參數到 one_of 時他會幫我們建立一個包好的 <code>struct _type_one_of</code> 這樣我們就不用自己手動包了！</p>
<p>helper function 的 return type 是 <code>_type_one_of&lt;ArgTypes...&gt;</code> 而使用 <code>-&gt;</code> 的寫法稱做 trailing return type，他只是可以延後到 function declaration 後面才指定 return type，在這個 case，這樣寫跟把 return type 寫在前面其實沒有差別，單純只是我覺得因為 return type 長的比較醜，放在後面這樣比較好看。而回傳的東西一樣就是把東西 unpack。</p>
<p>除此之外，在寫 metaprogramming 的時候要記得，你希望 compiler 自動幫你拆開來的 function 都要加上 <code>constexpr</code> specifier，這樣 compiler 才會盡可能幫你拆開。而 <code>std::forward</code> 這個 function，很幸運的在 c++ 14 的時候已經改成 <code>constexpr</code>，因此這個 constructor 我很有信心 compiler 絕對會幫我們拆開來。</p>
<p>這樣就完成了我們的 one_of，接下來是要寫 <code>==</code> 的部份。我們可以用 operator overloading 來自定義一個 <code>operator==</code>，把任意 type 跟 <code>struct _type_one_of</code> 做比較：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>==(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> _type_one_of&lt;ArgTypes...&gt; &amp;rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> rhs.__match_op(lhs, rhs.args);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>這邊我們定義好了 <code>operator==</code> 的部份，而接下來因為我們需要把預先存起來的 <code>std::tuple</code> 拿出來用，因此我希望我們可以把 unpack tuple 的部份使用 struct 內部的 function 來實現。當然你也可以不要像我一樣，你也可以直接把 function 定義在外面。</p>
<p>接下來實作 unpack tuple 的部份，回到 <code>struct _type_one_of</code> 裏面：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span>&#123;</span></span><br><span class="line">    <span class="built_in">std</span>::tuple&lt;ArgTypes...&gt; args;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line">    _type_one_of(Ts&amp;&amp;... args): args(<span class="built_in">std</span>::forward&lt;Ts&gt;(args)...) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> Inds = <span class="built_in">std</span>::make_index_sequence&lt;<span class="keyword">sizeof</span>...(Ts)&gt;&gt;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple&lt;Ts...&gt; &amp;tup) <span class="keyword">const</span></span><br><span class="line">        &#123; <span class="keyword">return</span> __match_op_impl(lhs, tup, Inds&#123;&#125;); &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>首先我們在 <code>operator==</code> 裏面呼叫了 <code>__match_op</code> 這個 function，因此我們定義一下 <code>__match_op</code> 。第一個參數是型別為 <code>T</code> 的 <code>lhs</code> (left-hand-side)，第二個是我們的 tuple <code>tup</code>。然後我們產生一個 <code>std::integer_sequence</code>，並且呼叫 <code>__match_op_impl</code>。</p>
<p><code>std::make_index_sequence</code> 定義在 <code>&lt;utility&gt;</code>，是 c++ 14 才有的 type。他的功能是可以產生一個 template parameters 為一個數列的 class。而 <code>sizeof...(Ts)</code> 是 c++ 11 的語法，他其實是叫作 <code>sizeof...</code> operator，用途是計算 parameter pack 裏面元素的數量。因此當我呼叫 <code>std::make_index_sequence&lt;sizeof...(Ts)&gt;</code> 時，假設 <code>Ts</code> 裏面有 5 個元素，他會產生一個長的像這樣的 type：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">std</span>::index_sequence&lt;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&gt;</span><br></pre></td></tr></table></figure><br>然後我們定義 template 的最後一個 parameter type 的預設 type 是這個東西，這樣我們就得到<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typename</span> Inds = <span class="built_in">std</span>::index_sequence&lt;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&gt;</span><br></pre></td></tr></table></figure><br>這個 Inds 是一個 class，因此我們在呼叫 <code>__match_op_impl</code> 並把他當參數傳入時，使用 <code>Inds&#123;&#125;</code> 等於是創建一個 object 的 instance。</p>
<p>之所以要用這種二段式呼叫的原因主要是因為 <code>std::tuple</code> 限制的關係，如果要取得 tuple 裏面的元素，我們必須使用 <code>std::get</code> 這個 function，而 <code>std::get</code> 這個 function 會需要指定元素的 Index。例如，如果要取出 tuple 的第<code>N</code>個元素，則我們必須這樣寫：<code>std::get&lt;N&gt;(tup)</code>。因此，我們利用兩段式呼叫，第一次呼叫先用 <code>std::make_index_sequence</code> 取得元素 Index 的 sequence 後再進行第二次呼叫，unpack tuple。</p>
<p>接下來是定義<code>__match_op_impl</code> 的部份：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span>&#123;</span></span><br><span class="line">    <span class="built_in">std</span>::tuple&lt;ArgTypes...&gt; args;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line">    _type_one_of(Ts&amp;&amp;... args): args(<span class="built_in">std</span>::forward&lt;Ts&gt;(args)...) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="built_in">std</span>::<span class="keyword">size_t</span>... I&gt;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op_impl(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple&lt;Ts...&gt; &amp;tup, <span class="built_in">std</span>::index_sequence&lt;I...&gt;) <span class="keyword">const</span></span><br><span class="line">        &#123; <span class="keyword">return</span> __match_one_of_op(lhs, <span class="built_in">std</span>::get&lt;I&gt;(tup)...); &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> Inds = <span class="built_in">std</span>::make_index_sequence&lt;<span class="keyword">sizeof</span>...(Ts)&gt;&gt;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple&lt;Ts...&gt; &amp;tup) <span class="keyword">const</span></span><br><span class="line">        &#123; <span class="keyword">return</span> __match_op_impl(lhs, tup, Inds&#123;&#125;); &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>這邊我們定義了 <code>__match_op_impl</code> 函數，一樣，第一個參數是 <code>lhs</code>，第二個是 tuple <code>tup</code>，第三個是 Index sequence，他的 type 是 <code>std::index_sequence&lt;I...&gt;</code> 由於實體變數我們並不是很 care (甚至這個 class 裏面根本沒包多少東西，重點是他的 template parameter pack)，所以我們第三個參數只寫 type 而沒有寫 variable 的名稱。我們在 template 裏面定義 Index sequence 的 template parameter pack <code>std::size_t... I</code>。這樣我們就可以用 <code>std::get&lt;I&gt;(tup)...</code> 來讓 Compiler 自動幫我們 unpack tuple。</p>
<p>接著我們呼叫 <code>__match_one_of_op</code>，我們第一個參數傳入 <code>lhs</code>，後面的參數則是用 <code>...</code> 來 unpack。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_one_of_op(<span class="keyword">const</span> T&amp; lhs, ArgTypes&amp;&amp;... args)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">sizeof</span>...(args) == <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="literal">false</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> any( (lhs == <span class="built_in">std</span>::forward&lt;ArgTypes&gt;(args)) ...);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>這邊我們定義 <code>__match_one_of_op</code> 的內容，首先這先使用了一個 <code>if constexpr else</code> 的表達式，從 c++ 17 開始可以指定 if else 是 <code>constexpr</code>，這樣 compiler 就會幫我們拆開來。而如果你想要使用 <code>constexpr</code> 的 <code>if</code> <code>else if</code> <code>else</code> 的話，你可以這樣寫：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="comment">/*...*/</span>)</span></span></span><br><span class="line"><span class="function"><span class="keyword">else</span> <span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="comment">/*...*/</span>)</span></span></span><br><span class="line"><span class="function"><span class="keyword">else</span></span></span><br></pre></td></tr></table></figure><br>實際上 <code>else if</code> 就是 <code>else&#123; if(/*...*/)&#123;&#125; &#125;</code>。</p>
<p>如果 <code>sizeof...(args) == 0</code> 的話，也就是如果 parameter pack 裏面一個東西都沒有，我們直接 return false。否則我們要把 <code>args</code> 用 <code>...</code> 拆開來一個一個跟 <code>lhs</code> 做比較。</p>
<p>到這邊，如果 compiler 把 any 裏面的東西拆開來，他會得到類似這樣的東西 (這只是pseudocode)：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">any( lhs&#x3D;&#x3D;args[0], lhs&#x3D;&#x3D;args[1], lhs&#x3D;&#x3D;args[2], ...) </span><br></pre></td></tr></table></figure></p>
<p>而由於我們當初定義 <code>one_of</code> 是：<strong>只要其中一項等於 <code>lhs</code></strong> 就會回傳 <code>true</code>，因此我們可以寫一個 <code>any</code> 這個 function 來負責統整所有比較的結果，只要有其中一個 expression 是 <code>true</code>，則 <code>any</code> 會回傳 <code>true</code>，否則回傳 <code>false</code>。</p>
<p>因此我們定義 <code>any</code> 這個 function<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="function"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="title">any</span><span class="params">(ArgTypes&amp;&amp;... args)</span> </span>&#123; <span class="keyword">return</span> (... || args); &#125;</span><br></pre></td></tr></table></figure></p>
<p>這邊我們使用 c++ 17 的語法 <code>(... || args)</code> 這個語法叫作 fold expression，他的用途就是他會把 parameter packs 拆開後中間全部用同樣的 <strong>operator</strong> 連接起來，因此會得到類似這樣的效果 (這只是pseudocode)：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(... args[2] || args[1] || args[0])</span><br></pre></td></tr></table></figure></p>
<p>這邊需要注意的是我寫成 left fold 的型式，但其實也可以使用普通的 right fold 型式<br><code>(args || ...)</code>。差別只在於展開的方向不同。</p>
<p>left fold:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(... args[2] || args[1] || args[0])</span><br></pre></td></tr></table></figure></p>
<p>right fold:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(args[0] || args[1] || args[2] ...)</span><br></pre></td></tr></table></figure></p>
<p>詳細請看：<a href="https://en.cppreference.com/w/cpp/language/fold">fold expression(since C++17) - cppreference.com</a></p>
<p>到這邊我們就真正完成了我們的 one_of operator，下面是完整的 code：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;tuple&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="function"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="title">any</span><span class="params">(ArgTypes&amp;&amp;... args)</span> </span>&#123; <span class="keyword">return</span> (... || args); &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_one_of_op(<span class="keyword">const</span> T&amp; lhs, ArgTypes&amp;&amp;... args)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">sizeof</span>...(args) == <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="literal">false</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> any( (lhs == <span class="built_in">std</span>::forward&lt;ArgTypes&gt;(args)) ...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span>&#123;</span></span><br><span class="line">    <span class="built_in">std</span>::tuple&lt;ArgTypes...&gt; args;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line">    _type_one_of(Ts&amp;&amp;... args): args(<span class="built_in">std</span>::forward&lt;Ts&gt;(args)...) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="built_in">std</span>::<span class="keyword">size_t</span>... I&gt;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op_impl(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple&lt;Ts...&gt; &amp;tup, <span class="built_in">std</span>::index_sequence&lt;I...&gt;) <span class="keyword">const</span></span><br><span class="line">        &#123; <span class="keyword">return</span> __match_one_of_op(lhs, <span class="built_in">std</span>::get&lt;I&gt;(tup)...); &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> Inds = <span class="built_in">std</span>::make_index_sequence&lt;<span class="keyword">sizeof</span>...(Ts)&gt;&gt;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple&lt;Ts...&gt; &amp;tup) <span class="keyword">const</span></span><br><span class="line">        &#123; <span class="keyword">return</span> __match_op_impl(lhs, tup, Inds&#123;&#125;); &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line">constexpr auto one_of(ArgTypes&amp;&amp;... args) -&gt; _type_one_of&lt;ArgTypes...&gt;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> _type_one_of&lt;ArgTypes...&gt;(<span class="built_in">std</span>::forward&lt;ArgTypes&gt;(args)...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>==(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> _type_one_of&lt;ArgTypes...&gt; &amp;rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> rhs.__match_op(lhs, rhs.args);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>接著你可以試試看使用這個程式<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> g;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; g;</span><br><span class="line">    <span class="keyword">if</span>(g == one_of(<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;It&#x27;s a multiple of 10 !&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>one_of 裏面可以塞不定個數的參數</li>
<li>one_of 可以塞任何型態的變數</li>
</ul>
<p>你可以在<a href="https://godbolt.org/z/LjS1WS">這邊</a> 比較一下看看編譯後的結果是不是跟原本的垃圾 Code 一模一樣。</p>
<p>除此之外，因為我們使用 template parameter pack 的關係，one_of 可以傳入每個型態都不一樣的參數：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> g;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; g;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> i = <span class="number">35</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> f = <span class="number">12.6</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span> d = <span class="number">-4.9</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span> str = <span class="string">&quot;Hello&quot;</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; vd&#123;<span class="number">-1.4</span>, <span class="number">6.8</span>&#125;;</span><br><span class="line">    <span class="keyword">if</span>(g == one_of(i, f, d, str, vd)) <span class="comment">// int, float, double, string, vector</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;g is in the set !&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>但是這樣你必須自己 overload 不同型態間的 operator==，不然 compile 的時候會出錯。</p>
<h2 id="進階"><a href="#進階" class="headerlink" title="進階"></a>進階</h2><h3 id="做比較測試"><a href="#做比較測試" class="headerlink" title="做比較測試"></a>做比較測試</h3><p>雖然 one_of 已經可以跟任意型態做比較了，但是實際上這麼做是非常危險的。如上所言，有時候使用者並不會記得要實作出對應任意型態的 operator==，甚至，為每一對型態實作一組 operator== 是非常費時的時間，因此我們有沒有辦法寫個功能讓 compiler 自動判定兩個型態能不能做 == 比較，如果可以的話就做比較，不行的話就回傳 <code>false</code>？</p>
<p>這裡我們就必須使用一個 metaprogramming 的特殊技巧叫作 <strong>SFINAE</strong>，他的核心理念就是實作一個 General 的 template，再實作一個專做測試用的 Specialized template，如果我們想要的功能能夠吻合到 Specialized template 表示測試合格(e.g. 測試某 Type 擁有某個 member、測試某 Type 有支援某 Operator等等)，如果不合格，Compiler 也會自動把他吻合到 General 的 template 上面而不會跳出 Compiler error。這個的運作原理不難理解，我在這邊就不多做解釋，想知道的自行 <code>google</code>。</p>
<p>因此我們繼續更改 Code，我希望在 <code>__match_one_of_op</code> 裏面呼叫 <code>any</code> 前先加入比較測試，<strong>讓 Compiler 幫我們檢查</strong>兩個型態能不能做比較 <em>(重點：要 Compiler 幫我們檢查！)</em>：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_one_of_op(<span class="keyword">const</span> T&amp; lhs, ArgTypes&amp;&amp;... args)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">sizeof</span>...(args) == <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="literal">false</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> any( (__match_comparable_one_of_op(lhs, args)) ...);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>這邊我們直接呼叫<code>__match_comparable_one_of_op</code> 並用 <code>...</code> 來幫我們逐一配對檢查看能不能做比較。</p>
<p>這邊的實作方式非常多，我們也可以使用 tag dispatching 、SFINAE、或是<code>std::enable_if</code> 的方式實作，也可以直接用 <code>if constexpr else</code> 的方式實作，而這次我們就先從簡用 <code>if constexpr else</code> 的方式實作。實際上我覺得用 SFINAE 實作我覺得比較優美，因為在 metaprogramming 中出現 <code>if else</code> 這種東西在瀏覽 Code 的時候感覺就是特別礙眼。<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> LT&amp; lhs, <span class="keyword">const</span> RT&amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op&lt;LT, RT&gt;::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> lhs</span>==rhs;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>這邊首先使用 struct type 的 SFINAE <code>_whether_support_op</code> 來判斷 <code>LT</code> 跟 <code>RT</code> 這兩個型態能不能做比較。<br>註：SFINAE 也有 function type 的，有機會再介紹。</p>
<p>如果 <code>LT</code> 跟 <code>RT</code> 可以做比較，則回傳 <code>lhs == rhs</code> 比較結果，否則回傳 <code>false</code>。這邊注意因為他是 <code>constexpr</code> specified 的 <code>if else</code> 因此如果 <code>if</code> 的條件不成立則 Compiler 不會編譯 <code>if</code> 裏面的內容。有需要的話，我們甚至可以印出一些資訊來看看程式的運作：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> LT&amp; lhs, <span class="keyword">const</span> RT&amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op&lt;LT, RT&gt;::value)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;[Comparable] &quot;</span>;</span><br><span class="line">        <span class="keyword">return</span> lhs==rhs;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;[Not Comparable] &quot;</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">        </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>接下來就是實作 struct 型的 SFINAE <code>_whether_support_op</code>：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>, <span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span>&lt;&gt;&gt;</span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>&lt;LT, RT, std::void_t&lt;</span></span><br><span class="line"><span class="class">        decltype(std::declval&lt;LT&gt;()==std::declval&lt;RT&gt;) &gt;&gt; :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">&#123;&#125;;</span><br></pre></td></tr></table></figure></p>
<p>我也是最近才注意到 c++ 17 中出現了一個新的 Type 叫作 <code>std::void_t</code> 而且根據 <a href="https://en.cppreference.com/w/cpp/types/void_t">cppreference.com</a> 的資訊，這個 Type 就是專門拿來玩 SFINAE 的！由於 <code>std::false_type</code>、<code>std::true_type</code>、<code>std::void_t</code> 都是出自 <code>&lt;type_traits&gt;</code> ，因此必須加上 <code>#include &lt;type_traits&gt;</code>。而 <code>std::void_t</code> 其實有個很有趣的事情就是不管我們塞入什麼型態，最後的 Type 他都會是 void。他的定義類似這樣：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>...&gt;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">void_t</span> = <span class="keyword">void</span>;</span><br></pre></td></tr></table></figure></p>
<p>所以不管我們塞什麼型態給他，他都是 void。</p>
<p>首先先從 General 的 <code>struct _whether_support_op</code> 開始 (他其實有個名字叫作 primary template)，這邊定義他的 template 參數是 <code>&lt;typename, typename, typename = std::void_t&lt;&gt; &gt;</code>。之所以都不寫名字是因為我們根本不 care 那個變數型態（簡稱變態）叫作什麼名字，反正他就是會有三個變態進來，然後第三的變態預設為 <code>std::void_t&lt;&gt;</code>就是為了玩 SFINAE 用的。</p>
<p>如果 Compiler 在配對 <code>_whether_support_op</code> 的時候配對到這個 General 版的 ，就表示我們想要的功能無法使用，因此我們讓這種 General 版的 struct 繼承 <code>std::false_type</code>。繼承這個 <code>std::false_type</code> 的時候，<code>_whether_support_op</code> 會繼承到一個 <code>static member</code> 叫作 <code>value</code>，而且 <code>value</code> 值會是 <code>false</code>。因此當我們呼叫 <code>_whether_support_op&lt;LT, RT&gt;</code> 後去取得他的 <code>value</code> 值，會得到 <code>false</code>：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static_assert</span>(<span class="literal">false</span> == _whether_support_op&lt;LT, RT&gt;::value);</span><br></pre></td></tr></table></figure></p>
<p>接下來定義一個 Specialized 的 struct <code>_whether_support_op</code> (specialized template)，這邊 template 只需要定義兩個變態 <code>&lt;typename LT, typename RT&gt;</code> 就可以了，因為第三個變態是我們要玩 SFINAE 用的。接下來就是客製化，這個行為稱做 partial specialization，我們只真對部份的變態做 specialization：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>&lt;LT, RT, std::void_t&lt;/*..specialize..*/&gt; &gt;</span></span><br></pre></td></tr></table></figure></p>
<p>可以看出，<code>LT</code>、<code>RT</code> 前兩個變態沒有特別 specialize， 但是第三個變態，我們指定他是 <code>std::void_t</code>，並且在 <code>std::void_t</code> 的 template 變態塞入 <code>decltype(std::declval&lt;LT&gt;()==std::declval&lt;RT&gt;)</code>。如果 Compiler 成功配對這個 struct 的話，他會使用這個 specialized 的 struct，而這個 specialized 的 struct 有繼承 <code>std::true_type</code>， 同 <code>std::false_type</code>，如果去取他的 <code>value</code> 值會得到 <code>true</code>：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static_assert</span>(<span class="literal">true</span> == _whether_support_op&lt;LT, RT&gt;::value);</span><br></pre></td></tr></table></figure></p>
<p>至於，解釋 <code>decltype(std::declval&lt;LT&gt;()==std::declval&lt;RT&gt;)</code> 這一串東西是什鬼，要先從 <code>decltype</code> 開始解釋。<code>decltype</code> 跟 <code>std::declval</code> 都不是新東西了，他們在 c++ 11就存在了。<code>decltype</code> 的用途是可以得到 <code>decltype(expression)</code> 裏面 expression 的回傳型態。例如：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> x = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">int</span> y = <span class="number">5</span>;</span><br><span class="line"><span class="keyword">decltype</span>(x+y) z = x+y;</span><br></pre></td></tr></table></figure><br>我們可以知道 <code>x</code> 和 <code>y</code> 都是 <code>int</code> 型態，所以 <code>x+y</code> 也會回傳 <code>int</code> 型態，利用 <code>decltype</code> 這個 operator，可以得到 <code>x+y</code> 的回傳型態 <code>int</code> 之後再宣告一個新變數 <code>z</code>，<code>z</code> 的型態就會是 <code>int</code>。</p>
<p><code>std::declval</code> 定義在 <code>&lt;utility&gt;</code> 裏面，他可以將一個指定的 Type 轉換成該 Type 的 Reference type，但是他並不會呼叫該 Type 的 Constructor，藉此我們可以呼叫他的 member function。所以 <code>std::declval&lt;LT&gt;()</code> 跟 <code>std::declval&lt;RT&gt;()</code> 就會分別產生一個 <code>LT</code>、<code>RT</code> 的 reference type <code>LT&amp;&amp;</code> 跟 <code>RT&amp;&amp;</code> ，我們可以呼叫他們的 member function，或是 operator。但是要注意的是，並不是只要用 <code>std::declval</code> 就可以無限上綱，首先他不會產生一個實體的 instance/object，再來就是他只能用在類似 <code>sizeof</code>、<code>decltype</code> 這類只需要 function definition 的 specifier 上，以及他還有一些規則，如果傳入的變態是 non cv-qualified (非 const 或 volatile) 或是 non ref-qualified (非 lvalue type <code>&amp;</code>) 則會回傳 rvalue type <code>&amp;&amp;</code>，而如果傳入的參數是cv-qualified 或是 ref-qualified 則會回傳同樣的變態。詳細的自己 <code>google</code>。</p>
<p>總之，當我們呼叫 <code>std::declval&lt;LT&gt;()</code> 跟 <code>std::declval&lt;RT&gt;()</code> 時 Compiler 會產生這兩的變態的 reference type 接著使用 <code>std::declval&lt;LT&gt;() == std::declval&lt;RT&gt;()</code> 嘗試呼叫這兩個變態的 operator==，然後取得回傳的變態 <code>decltype(std::declval&lt;LT&gt;() == std::declval&lt;RT&gt;())</code>，然後將這個變態放入 <code>std::void_t&lt;...&gt;</code>，最後放入 struct <code>_whether_support_op</code> 的第三個 argument。</p>
<p>當我們從外部宣告一個實體 struct 時 (e.g. <code>_whether_support_op&lt;int, std::string&gt;</code> )，Compiler 會發生一系列事情，這邊就會關係到 Compiler 在呼叫 template function 或 template class 的決策流程：</p>
<ul>
<li>第一個階段會先進行 name lookup，找出對應名稱的 function / class</li>
<li>第二個階段會進行 template argument deduction，推導出所有 candidate function / class</li>
<li>第三個階段會進行篩選，選出最吻合的 function / class</li>
</ul>
<p>詳細請看：<a href="https://en.cppreference.com/w/cpp/language/template_argument_deduction">Template argument deduction - cppreference.com</a></p>
<p>以我們的例子來說，第一個階段的 name lookup，Compiler 可以得到我們有兩個 <code>_whether_support_op</code> 的 template struct。</p>
<p>第二個階段 argument deduction 就會產生變化了，首先他會將指定的型態 <code>int</code> 跟 <code>std::string</code> 帶入第一個 <code>_whether_support_op</code> (注意，到這邊為止，Compiler 都還不知道誰是 primary 誰是 specialized)。由於我們只有指定兩個型態，第三個我們使用預設的 <code>typename = std::void_t&lt;&gt;</code>，Compiler 會產生第一個可行的候選名單 <code>_whether_support_op&lt;int, std::string, void&gt;</code>，但是注意，這個候選名單第三個變態是 default，而非在宣告時指定的。</p>
<p>接下來 deduce 第二個 <code>_whether_support_op</code> 會產生兩種狀況。<br>狀況一：如果我們有宣告 <code>LT</code> 與 <code>RT</code> 的 operator==，則 <code>std::declval&lt;LT&gt;() == std::declval&lt;RT&gt;()</code> 判斷式會成立，<code>decltype</code>可以得到正確的回傳變態 (通常是 <code>bool</code>)，接著 <code>std::void_t&lt;bool&gt;</code> 也能夠正常成立，最後得到完整的 <code>_whether_support_op&lt;int, std::string, void&gt;</code>，這邊的第三個變態就是宣告時指定的，他是從 <code>std::void_t&lt;&gt;</code> 特化成 <code>void</code> 的，因此 Compiler 會把這個 struct 判斷成是一種 specialization。而 specialized template 的優先權會大於 primary template，因此 Compiler 最後會選擇這個 template。這時候我們取出 <code>value</code> 值會得到 <code>true</code>。</p>
<p>狀況二：如果我們沒有宣告 <code>LT</code> 與 <code>RT</code> 的 operator==，則 <code>std::declval&lt;LT&gt;() == std::declval&lt;RT&gt;()</code> 判斷式無法成立，<code>decltype</code> 得不到正確的回傳變態，<code>std::void_t&lt;...&gt;</code> 也無法成立，最後 Compiler 沒有辦法得到完整的 specialization，因此這個 struct 就會被 Compiler 從 candidate list 裏面剔除。Compiler 最後選擇使用 primary template。這時候我們取出 <code>value</code> 值會得到 <code>false</code>。</p>
<p>最後的程式碼：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;tuple&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;type_traits&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="function"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="title">any</span><span class="params">(ArgTypes&amp;&amp;... args)</span> </span>&#123; <span class="keyword">return</span> (... || args); &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>, <span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span>&lt;&gt;&gt;</span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>&lt;LT, RT, std::void_t&lt;</span></span><br><span class="line"><span class="class">        decltype(std::declval&lt;LT&gt;()==std::declval&lt;RT&gt;) &gt;&gt; :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> LT&amp; lhs, <span class="keyword">const</span> RT&amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op&lt;LT, RT&gt;::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> lhs</span>==rhs;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_one_of_op(<span class="keyword">const</span> T&amp; lhs, ArgTypes&amp;&amp;... args)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">sizeof</span>...(args) == <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="literal">false</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> any( (__match_comparable_one_of_op(lhs, args)) ...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span>&#123;</span></span><br><span class="line">    <span class="built_in">std</span>::tuple&lt;ArgTypes...&gt; args;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line">    _type_one_of(Ts&amp;&amp;... args): args(<span class="built_in">std</span>::forward&lt;Ts&gt;(args)...) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="built_in">std</span>::<span class="keyword">size_t</span>... I&gt;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op_impl(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple&lt;Ts...&gt; &amp;tup, <span class="built_in">std</span>::index_sequence&lt;I...&gt;) <span class="keyword">const</span></span><br><span class="line">        &#123; <span class="keyword">return</span> __match_one_of_op(lhs, <span class="built_in">std</span>::get&lt;I&gt;(tup)...); &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> Inds = <span class="built_in">std</span>::make_index_sequence&lt;<span class="keyword">sizeof</span>...(Ts)&gt;&gt;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple&lt;Ts...&gt; &amp;tup) <span class="keyword">const</span></span><br><span class="line">        &#123; <span class="keyword">return</span> __match_op_impl(lhs, tup, Inds&#123;&#125;); &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line">constexpr auto one_of(ArgTypes&amp;&amp;... args) -&gt; _type_one_of&lt;ArgTypes...&gt;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> _type_one_of&lt;ArgTypes...&gt;(<span class="built_in">std</span>::forward&lt;ArgTypes&gt;(args)...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>==(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> _type_one_of&lt;ArgTypes...&gt; &amp;rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> rhs.__match_op(lhs, rhs.args);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如此一來我們就可以用來做更狂的比較，還不會跳 Error 出來：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">is_in_the_set</span><span class="params">(<span class="keyword">const</span> T&amp; X)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(X == one_of(<span class="number">10</span>, </span><br><span class="line">                   <span class="number">23.5465</span>, </span><br><span class="line">                   <span class="string">&quot;Hello&quot;</span>, </span><br><span class="line">                   <span class="built_in">std</span>::<span class="built_in">string</span>(<span class="string">&quot;foo&quot;</span>), </span><br><span class="line">                   <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt;&#123;<span class="number">12.5</span>, <span class="number">64.5</span>&#125;,</span><br><span class="line">                   <span class="string">&#x27;c&#x27;</span>) )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;X is in the set&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    is_in_the_set(<span class="number">10</span>); <span class="comment">//true</span></span><br><span class="line">    is_in_the_set(<span class="built_in">std</span>::<span class="built_in">string</span>(<span class="string">&quot;Hello&quot;</span>)); <span class="comment">//true</span></span><br><span class="line">    is_in_the_set(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;&#123;<span class="number">0.1</span>, <span class="number">0.2</span>&#125;); <span class="comment">//false</span></span><br><span class="line">    is_in_the_set(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt;&#123;<span class="number">12.5</span>, <span class="number">64.5</span>&#125;); <span class="comment">//true</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Generalization：套用到任意-comparison-operators"><a href="#Generalization：套用到任意-comparison-operators" class="headerlink" title="Generalization：套用到任意 comparison operators"></a>Generalization：套用到任意 comparison operators</h3><p>在上面的例子中我們是針對特定的 operator== 做設計，但是如果因為 one_of 實在太方便，我想要實作 one_of 也可以支援其他 operator 我是不是每次都得重頭設計一遍？其實不用，我們只需要連 operator 都當成是一個 template argument 傳進去就行了！因此開始設計ㄅ！</p>
<p>首先，由於 operator== 沒辦法直接當作 argument 傳入 template，因此我先把 operator== 用 <code>struct _op_equal_to</code> 包起來，並在每一個 template 上都加上一個 Fn 的變態：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;tuple&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;type_traits&gt;</span></span></span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">op_equal_to</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="comment">//TODO</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="function"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="title">any</span><span class="params">(ArgTypes&amp;&amp;... args)</span> </span>&#123; <span class="keyword">return</span> (... || args); &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//TODO</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>, <span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span>&lt;&gt;&gt;</span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//TODO</span></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>&lt;LT, RT, std::void_t&lt;</span></span><br><span class="line"><span class="class">        decltype(std::declval&lt;LT&gt;()==std::declval&lt;RT&gt;) &gt;&gt; :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> LT&amp; lhs, <span class="keyword">const</span> RT&amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//TODO</span></span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op&lt;LT, RT&gt;::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> lhs</span>==rhs;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_one_of_op(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> T&amp; lhs, ArgTypes&amp;&amp;... args)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">sizeof</span>...(args) == <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="literal">false</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> any( (__match_comparable_one_of_op(op, lhs, args)) ...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span>&#123;</span></span><br><span class="line">    <span class="built_in">std</span>::tuple&lt;ArgTypes...&gt; args;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line">    _type_one_of(Ts&amp;&amp;... args): args(<span class="built_in">std</span>::forward&lt;Ts&gt;(args)...) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="built_in">std</span>::<span class="keyword">size_t</span>... I&gt;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op_impl(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple&lt;Ts...&gt; &amp;tup, <span class="built_in">std</span>::index_sequence&lt;I...&gt;) <span class="keyword">const</span></span><br><span class="line">        &#123; <span class="keyword">return</span> __match_one_of_op(op, lhs, <span class="built_in">std</span>::get&lt;I&gt;(tup)...); &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> Inds = <span class="built_in">std</span>::make_index_sequence&lt;<span class="keyword">sizeof</span>...(Ts)&gt;&gt;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple&lt;Ts...&gt; &amp;tup) <span class="keyword">const</span></span><br><span class="line">        &#123; <span class="keyword">return</span> __match_op_impl(op, lhs, tup, Inds&#123;&#125;); &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line">constexpr auto one_of(ArgTypes&amp;&amp;... args) -&gt; _type_one_of&lt;ArgTypes...&gt;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> _type_one_of&lt;ArgTypes...&gt;(<span class="built_in">std</span>::forward&lt;ArgTypes&gt;(args)...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>==(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> _type_one_of&lt;ArgTypes...&gt; &amp;rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> rhs.__match_op(_op_equal_to&#123;&#125;, lhs, rhs.args);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>有加上 <code>TODO</code> 的都是還沒有完成的部份。首先我們已經可以在 <code>__match_comparable_one_of_op</code> 裏面取得用 struct 包好的 operator 的，接下來就是要設計把 <code>op</code> 也傳入 <code>_whether_support_op</code> 裏面檢查。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span>&lt;&gt;&gt;</span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>&lt;Fn(Ts...), std::void_t&lt;</span></span><br><span class="line"><span class="class">        decltype(/*TODO*/) &gt;&gt; :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> LT&amp; lhs, <span class="keyword">const</span> RT&amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//TODO</span></span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op&lt;Fn(LT, RT)&gt;::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> lhs</span>==rhs;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>這邊注意，我們的 specialized template 只剩下兩格，第一個是 <code>Fn(Ts...)</code>，第二個是<code>std::void_t&lt;&gt;</code>，因此對應的 primary template 的 <code>typename</code> 格數也要剩下兩格。而 <code>std::void_t&lt;&gt;</code> 的內容物還沒設計。</p>
<p>接下來設計 <code>struct _op_equal_to</code>：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">op_equal_to</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line">    constexpr auto operator()(const LT&amp; lhs, const RT&amp; rhs) const -&gt; decltype(std::declval&lt;LT&amp;&gt;() == std::declval&lt;RT&amp;&gt;());</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>這邊我使用 overload operator() 來讓 <code>_op_equal_to</code> 變成很像是 function call 的方式來設計，而回傳變態是 <code>decltype(std::declval&lt;LT&amp;&gt;() == std::declval&lt;RT&amp;&gt;())</code>，如果這個東西成立的話，他會變成正確的型態 (通常是 <code>bool</code>)。函式的內容我們不需要定義，剛剛說的，因為我們只會用 <code>decltype</code> 讓 Compiler 檢查 expression 會不會成立而已，我們關心的是那個回傳變態會不會成立，如果成立的話就行了。</p>
<p>接下來就是回到 <code>_whether_support_op</code> 裏面設計 <code>std::void_t&lt;&gt;</code><br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>&lt;Fn(Ts...), std::void_t&lt;</span></span><br><span class="line"><span class="class">        decltype( std::declval&lt;Fn&gt;()(std::declval&lt;Ts&gt;()...) ) &gt;&gt; :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">&#123;&#125;;</span><br></pre></td></tr></table></figure></p>
<p>這邊我用 <code>std::declval&lt;Fn&gt;()</code> 產生一個 <code>_op_equal_to</code> 的 reference type 並呼叫他的 member function，傳入的參數是一堆 <code>Ts</code> 型態的 reference type <code>std::declval&lt;Ts&gt;()...</code>。</p>
<p>這樣就完成 General 版的 operator supporting 檢查了。</p>
<p>但這邊還有一個問題是，我們真正在比較的地方還沒有 generalize：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op&lt;Fn(LT, RT)&gt;::value)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">return</span> lhs</span>==rhs;  <span class="comment">//here</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br></pre></td></tr></table></figure></p>
<p>這個地方該怎麼辦？該不會 <code>_op_equal_to</code> 裏面除了有一個虛擬的比較後又要再實作一個單獨的 member function 來做實體的比較？這樣不會太冗嘛？</p>
<p>會。</p>
<p>但是我們只需要動一點手腳就可以做出同時可以虛擬的比較又可以實體的比較的 function 了：<br>首先比較的部份：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op&lt;Fn(LT, RT)&gt;::value)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">return</span> <span class="title">op</span><span class="params">(lhs, rhs)</span></span>;  <span class="comment">//here</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br></pre></td></tr></table></figure></p>
<p>接下來 <code>_op_equal_to</code> 的部份：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">op_equal_to</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line">    constexpr auto operator()(const LT&amp; lhs, const RT&amp; rhs) const -&gt; decltype(std::declval&lt;LT&amp;&gt;() == std::declval&lt;RT&amp;&gt;())</span><br><span class="line">    &#123; <span class="keyword">return</span> lhs==rhs; &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>到這邊就完成了</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;tuple&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;type_traits&gt;</span></span></span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">op_equal_to</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line">    constexpr auto operator()(const LT&amp; lhs, const RT&amp; rhs) const -&gt; decltype(std::declval&lt;LT&amp;&gt;() == std::declval&lt;RT&amp;&gt;())</span><br><span class="line">    &#123; <span class="keyword">return</span> lhs==rhs; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="function"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="title">any</span><span class="params">(ArgTypes&amp;&amp;... args)</span> </span>&#123; <span class="keyword">return</span> (... || args); &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span>&lt;&gt;&gt;</span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>&lt;Fn(Ts...), std::void_t&lt;</span></span><br><span class="line"><span class="class">        decltype( std::declval&lt;Fn&gt;()(std::declval&lt;Ts&gt;()...) ) &gt;&gt; :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> LT&amp; lhs, <span class="keyword">const</span> RT&amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op&lt;Fn(LT, RT)&gt;::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="title">op</span><span class="params">(lhs, rhs)</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_one_of_op(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> T&amp; lhs, ArgTypes&amp;&amp;... args)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">sizeof</span>...(args) == <span class="number">0</span>)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="literal">false</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> any( (__match_comparable_one_of_op(op, lhs, args)) ...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">type_one_of</span>&#123;</span></span><br><span class="line">    <span class="built_in">std</span>::tuple&lt;ArgTypes...&gt; args;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line">    _type_one_of(Ts&amp;&amp;... args): args(<span class="built_in">std</span>::forward&lt;Ts&gt;(args)...) &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="built_in">std</span>::<span class="keyword">size_t</span>... I&gt;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op_impl(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple&lt;Ts...&gt; &amp;tup, <span class="built_in">std</span>::index_sequence&lt;I...&gt;) <span class="keyword">const</span></span><br><span class="line">        &#123; <span class="keyword">return</span> __match_one_of_op(op, lhs, <span class="built_in">std</span>::get&lt;I&gt;(tup)...); &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> T, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> Inds = <span class="built_in">std</span>::make_index_sequence&lt;<span class="keyword">sizeof</span>...(Ts)&gt;&gt;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_op(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> <span class="built_in">std</span>::tuple&lt;Ts...&gt; &amp;tup) <span class="keyword">const</span></span><br><span class="line">        &#123; <span class="keyword">return</span> __match_op_impl(op, lhs, tup, Inds&#123;&#125;); &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line">constexpr auto one_of(ArgTypes&amp;&amp;... args) -&gt; _type_one_of&lt;ArgTypes...&gt;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> _type_one_of&lt;ArgTypes...&gt;(<span class="built_in">std</span>::forward&lt;ArgTypes&gt;(args)...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>==(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> _type_one_of&lt;ArgTypes...&gt; &amp;rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> rhs.__match_op(_op_equal_to&#123;&#125;, lhs, rhs.args);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下來就可以嘗試定義其他 operators</p>
<ul>
<li><p><code>operator!=</code> ，這東西沒有什麼好定義的，把 <code>operator==</code> 前面加上 <code>!</code> 就好了：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>!=(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> _type_one_of&lt;ArgTypes...&gt; &amp;rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> !rhs.__match_op(_op_equal_to&#123;&#125;, lhs, rhs.args);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>operator&lt;</code>：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">op_less_than</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line">    constexpr auto operator()(const LT&amp; lhs, const RT&amp; rhs) const -&gt; decltype(std::declval&lt;LT&amp;&gt;() &lt; std::declval&lt;RT&amp;&gt;())</span><br><span class="line">    &#123; <span class="keyword">return</span> lhs &lt; rhs; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>&lt;(<span class="keyword">const</span> T&amp; lhs, <span class="keyword">const</span> _type_one_of&lt;ArgTypes...&gt; &amp;rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> rhs.__match_op(_op_less_than&#123;&#125;, lhs, rhs.args);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>其他自己試</p>
</li>
</ul>
<p><a href="https://godbolt.org/z/wjmLZI">完整的測試 Code</a></p>
<h3 id="其他討論"><a href="#其他討論" class="headerlink" title="其他討論"></a>其他討論</h3><h4 id="op-equal-to-的其他寫法"><a href="#op-equal-to-的其他寫法" class="headerlink" title="_op_equal_to 的其他寫法"></a><code>_op_equal_to</code> 的其他寫法</h4><p>其實 <code>_op_equal_to</code> 這個 struct 還有其他寫法，例如也可以把 decltype 寫到 template 裏面判斷：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">op_equal_to</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT, <span class="keyword">typename</span> = <span class="keyword">decltype</span>(<span class="built_in">std</span>::declval&lt;LT&amp;&gt;() == <span class="built_in">std</span>::declval&lt;RT&amp;&gt;())&gt;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="keyword">bool</span> <span class="keyword">operator</span>()(<span class="keyword">const</span> LT&amp; lhs, <span class="keyword">const</span> RT&amp; rhs) <span class="keyword">const</span></span><br><span class="line">    &#123; <span class="keyword">return</span> lhs==rhs; &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>設在 template 的第三個 parameter，然後把 <code>auto</code> 換成 <code>bool</code>。但是我覺得這樣沒有比較好的原因是，<code>lhs==rhs</code> 並沒有保證回傳值一定是 <code>bool</code>。雖然在 comparison 裏面回傳非 <code>bool</code> 值本身就很奇怪。</p>
<h4 id="冗字"><a href="#冗字" class="headerlink" title="冗字"></a>冗字</h4><p>後來發現其實有些地方的 <code>std::forward</code> 可以拿掉。</p>
<p>第一個就是 <code>one_of</code> 裏面呼叫 <code>_type_one_of</code> 的 constructor<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... ArgTypes&gt;</span><br><span class="line">constexpr auto one_of(ArgTypes&amp;&amp;... args) -&gt; _type_one_of&lt;ArgTypes...&gt;</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> _type_one_of&lt;ArgTypes...&gt;(args...);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>因為 parameter pack 傳到 parameter pack 直接用 <code>...</code> unpack 就行了。</p>
<p>第二個是 <code>_type_one_of</code> 的 constructor 裏面呼叫 tuple 的 constructor<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>... Ts&gt;</span><br><span class="line">    _type_one_of(Ts&amp;&amp;... args): args(args...) &#123;&#125;</span><br></pre></td></tr></table></figure></p>
<p>實際上應該還有其他地方可以簡化，只是目前還沒有更多想法。</p>
<h4 id="function-type-的-SFINAE"><a href="#function-type-的-SFINAE" class="headerlink" title="function type 的 SFINAE"></a>function type 的 SFINAE</h4><p>有些人可能會以為要用 <code>std::void_t</code> 才能玩 SFINAE，其實 SFINAE 也不是什麼新概念了，而是因為有了這個概念，才會在 c++ 17 裏面新增 <code>std::void_t</code> 這個東西。在這之前其實也是可以用類似的方法實現 SFINAE，其中一種方式就是用 function 的方式。</p>
<p>這邊示範怎麼用 function type 來寫 SFINAE，首先這是原本 struct type 的 SFINAE<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span>&lt;&gt;&gt;</span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>&lt;Fn(Ts...), std::void_t&lt;</span></span><br><span class="line"><span class="class">        decltype( std::declval&lt;Fn&gt;()(std::declval&lt;Ts&gt;()...) ) &gt;&gt; :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">&#123;&#125;;</span><br></pre></td></tr></table></figure></p>
<p>這邊來定義 <code>__whether_support_op</code> function definition (不需要 function 實體)：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> = <span class="keyword">decltype</span>( <span class="built_in">std</span>::declval&lt;Fn&gt;()(<span class="built_in">std</span>::declval&lt;Ts&gt;()...) )&gt;</span><br><span class="line"><span class="built_in">std</span>::true_type __whether_support_op(<span class="keyword">const</span> Fn&amp;, <span class="keyword">const</span> Ts&amp;...);</span><br><span class="line">    </span><br><span class="line"><span class="built_in">std</span>::false_type __whether_support_op(...);</span><br></pre></td></tr></table></figure></p>
<p>感覺比 struct type 的 SFINAE 更簡單易懂。</p>
<p>可以看的出來，同樣道理，如果第一個 template function 的第三個 template parameter 成立，我們可以得到 return type 為 <code>std::true_type</code> 的 function，如果不成立，則配對到 return type 為 <code>std::false_type</code> 的 function，然後傳進去的參數就像是垃圾一樣隨便包成一包 parameter pack <code>...</code>。</p>
<p>接著把呼叫的地方改掉：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> LT&amp; lhs, <span class="keyword">const</span> RT&amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(<span class="keyword">decltype</span>(__whether_support_op(<span class="built_in">std</span>::declval&lt;Fn&gt;(), <span class="built_in">std</span>::declval&lt;LT&gt;(), <span class="built_in">std</span>::declval&lt;RT&gt;()))::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="title">op</span><span class="params">(lhs, rhs)</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>首先 <code>__whether_support_op</code> 是一個 function，我們可以藉由傳入參數的 reference type <code>std::declval&lt;&gt;</code> 來讓 Compiler 驗證 function。然後用 <code>decltype()</code> 取得 function 的 return type。最後再取出 <code>value</code> 值看看是 <code>true</code> 還是 <code>false</code>。記住！使用 <code>decltype()</code> 呼叫 function，Compiler 不會執行 function 實體，因此我們只需要有 function definition 就好了。</p>
<p>但是這邊我們就使用了一個很醜的方式呼叫我們的 function。實際上我們也可以用漂亮一點的方式，再包一層 SFINAE 的 struct helper：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>&gt; </span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>&lt;Fn(Ts...)&gt; :</span> <span class="keyword">decltype</span>(__whether_support_op(<span class="built_in">std</span>::declval&lt;Fn&gt;(), <span class="built_in">std</span>::declval&lt;Ts&gt;()...))</span><br></pre></td></tr></table></figure></p>
<p>然後呼叫的部份改成原本的：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> LT&amp; lhs, <span class="keyword">const</span> RT&amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op&lt;Fn(LT, RT)&gt;::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="title">op</span><span class="params">(lhs, rhs)</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>完整的 function type SFINAE：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts, <span class="keyword">typename</span> = <span class="keyword">decltype</span>( <span class="built_in">std</span>::declval&lt;Fn&gt;()(<span class="built_in">std</span>::declval&lt;Ts&gt;()...) )&gt;</span><br><span class="line"><span class="built_in">std</span>::true_type __whether_support_op(<span class="keyword">const</span> Fn&amp;, <span class="keyword">const</span> Ts&amp;...);</span><br><span class="line">    </span><br><span class="line"><span class="built_in">std</span>::false_type __whether_support_op(...);</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>&gt; </span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>&lt;Fn(Ts...)&gt; :</span> <span class="keyword">decltype</span>(__whether_support_op(<span class="built_in">std</span>::declval&lt;Fn&gt;(), <span class="built_in">std</span>::declval&lt;Ts&gt;()...))</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> LT&amp; lhs, <span class="keyword">const</span> RT&amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op&lt;Fn(LT, RT)&gt;::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="title">op</span><span class="params">(lhs, rhs)</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>跟單純只用 struct type 的 SFINAE 比起來相對就比較冗一點：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span>, <span class="keyword">typename</span> = <span class="built_in">std</span>::<span class="keyword">void_t</span>&lt;&gt;&gt;</span><br><span class="line">struct _whether_support_op : <span class="built_in">std</span>::false_type</span><br><span class="line">&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span>... Ts&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> _<span class="title">whether_support_op</span>&lt;Fn(Ts...), std::void_t&lt;decltype( std::declval&lt;Fn&gt;()(std::declval&lt;Ts&gt;()...) ) &gt;&gt; :</span> <span class="built_in">std</span>::true_type</span><br><span class="line">&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Fn, <span class="keyword">typename</span> LT, <span class="keyword">typename</span> RT&gt;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">bool</span> __match_comparable_one_of_op(<span class="keyword">const</span> Fn&amp; op, <span class="keyword">const</span> LT&amp; lhs, <span class="keyword">const</span> RT&amp; rhs)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(_whether_support_op&lt;Fn(LT, RT)&gt;::value)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">return</span> <span class="title">op</span><span class="params">(lhs, rhs)</span></span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>所以我才會使用 struct type 的 SFINAE。</p>
]]></content>
      <categories>
        <category>C++</category>
        <category>Metaprogramming</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>C++</tag>
        <tag>Metaprogramming</tag>
      </tags>
  </entry>
  <entry>
    <title>[Tuto] 夢魘のCUDA: 使用 Preconditioned Conjugate Gradient 輕鬆解決大型稀疏線性方程組</title>
    <url>/2020/09/19/preconditioned-conjugate-gradient/</url>
    <content><![CDATA[<div class="note info">
            <p>閱讀難度：</p><ul><li>線性代數：✦✦✧✧✧ (高手)</li><li>程式設計：✦✦✧✧✧ (高手)</li></ul>
          </div>
<p>特別感謝 <em>冠大大</em>、<em>王大大</em>。</p>
<p>《夢魘のCUDA》是 CUDA Programming 系列，<strong>此系列不會介紹任何 CUDA 的基礎知識</strong>，而會介紹一些 CUDA 相關的應用。本篇作為《夢魘のCUDA》系列的首篇文，將會介紹既實用又不實用的兩套 CUDA 內建 Library —- cuBLAS / cuSPARSE；除此之外，本篇也會講解如何使用這兩套 Library 實作出經典應用 —- Preconditioned Conjugate Gradient。之所以說是經典應用的原因是因為，cuSPARSE 幾乎是為了這個應用而誕生的。</p>
<a id="more"></a>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">Conjugate gradient method - wiki</a></li>
</ul>
<p>Conjugate gradient method 是一種數值分析 (Numerical analysis) 方法，用來解決大型稀疏線性方程組 (Large-scale sparse linear systems)。其中，線性方程組的矩陣必須是對稱正定矩陣 (Symmetric positive definite matrix)。</p>
<h3 id="1-1-Inner-Product-Space"><a href="#1-1-Inner-Product-Space" class="headerlink" title="1-1. Inner Product Space"></a>1-1. Inner Product Space</h3><ul>
<li><a href="https://en.wikipedia.org/wiki/Inner_product_space">Inner product space - wiki</a></li>
</ul>
<p>首先必須知道內積 (inner product) 的定義，對一個 map function $\langle\cdot,\cdot\rangle: V\times V \to F$，其中 vector space $V$，  field $F$，若滿足以下三個條件則稱這個 map function 是 inner product，其中 $x,y,z\in V$：</p>
<ul>
<li>Conjugate symmetry：<script type="math/tex; mode=display">
\langle x, y \rangle=\overline{\langle y, x \rangle}</script><div class="note info">
            <p>這通常是定義給複數的 Vector space，對於實數的 Vector space 只需滿足 $\langle x, y \rangle=\langle y, x \rangle$</p>
          </div></li>
<li>Linearity:<script type="math/tex; mode=display">
\begin{gathered}
\langle ax, y \rangle=a\langle x,y\rangle\\
\langle x+y,z\rangle = \langle x,z\rangle + \langle y,z \rangle
\end{gathered}</script><div class="note info">
            <p>這應該不需多做解釋</p>
          </div></li>
<li>Positive-definiteness:<script type="math/tex; mode=display">
\langle x,x\rangle > 0, \quad x\in V \setminus\{\mathbf{0}\}</script><div class="note info">
            <p>注意 $\mathbf{0}$ 代表 Vector space $V$ 的零向量。<br>此外，這個性質也 imply 若 $\langle x,x\rangle=0$ 則 $x=\mathbf{0}$。</p>
          </div>
</li>
</ul>
<h3 id="1-2-Symmetric-Positive-Definite-Matrix"><a href="#1-2-Symmetric-Positive-Definite-Matrix" class="headerlink" title="1-2. Symmetric Positive Definite Matrix"></a>1-2. Symmetric Positive Definite Matrix</h3><p>若一個 $N\times N$ 的 Real matrix $A$ 滿足以下條件，則稱 $A$ 為 Symmetric positive definite matrix：</p>
<ul>
<li>Symmetric：$A=A^T$</li>
<li>Positive definite：對於任意非 $0$ 向量 $x\in\mathbb{R}^N$，滿足 $x^TAx&gt;0$</li>
</ul>
<p>Symmetric matrix (複數則是 Hermitian matrix) 具有以下特性：</p>
<ul>
<li>Eigenvalues 一定為實數</li>
<li>一定存在一組 Orthonormal eigenvectors</li>
</ul>
<div class="note info">
            <p>Prove Eigenvalues 一定為實數：<br>假設 Symmetric (Hermitian) matrix $A$，假設 $\lambda$ 為 $A$ 的 Eigenvalue，以及對應的 Eigenvector $x$，則 $Ax=\lambda x$。取 Conjugate transpose $\bar{x}^TA^\dagger=\bar{x}^T\bar{\lambda}$。由於 $A$ 為 Symmetric (Hermitian) matrix，$A^\dagger=A$，因此 $\bar{x}^TA=\bar{x}^T\bar{\lambda}$。兩邊同乘 $x$ 得到：</p><script type="math/tex; mode=display">\bar{x}^TAx=\bar{x}^T\bar{\lambda}x</script><p>對 $Ax=\lambda x$ 兩邊同乘 $\bar{x}$：</p><script type="math/tex; mode=display">\bar{x}^TAx=\bar{x}^T\lambda x</script><p>得到 $\bar{x}^T\bar{\lambda}x=\bar{x}^T\lambda x$，若 $x\ne \mathbf{0}$，則 $\bar{\lambda}=\lambda$，因此 Eigenvalue 一定為 Real number。</p>
          </div>
<div class="note info">
            <p>Prove 一定存在一組 Orthonormal eigenvectors：<br>假設兩個 Symmetric (Hermitian) matrix $A$ 的 Eigenvalue $\lambda_a$ 與 $\lambda_b$，且 $\lambda_a\ne \lambda_b$，以及其分別對應的 Eigenvectors $x_a$, $x_b$，</p><script type="math/tex; mode=display">\begin{align*}\lambda_a \langle x_a, x_b \rangle     &= \langle \lambda_ax_a, x_b \rangle\\    &= \langle Ax_a, x_b \rangle\\    &= \langle x_a, Ax_b \rangle\\    &= \langle x_a, \lambda_bx_b \rangle\\    &= \lambda_b \langle x_a, x_b \rangle \end{align*}</script><p>得到 $\lambda_a \langle x_a, x_b \rangle=\lambda_b \langle x_a, x_b \rangle $，移項整理後得到：</p><script type="math/tex; mode=display">(\lambda_a-\lambda_b)\langle x_a, x_b \rangle=0</script><p>由於 $\lambda_a \ne \lambda_b \Leftrightarrow \lambda_a-\lambda_b\ne 0$，因此得到 $\langle x_a, x_b\rangle=0$，表示 $x_a$ 與 $x_b$ 為 Orthogonal。</p>
          </div>
<p>Symmetric positive definite matrix 有更棒的特性：Eigenvalues 一定都為正數</p>
<div class="note info">
            <p>Prove Symmetric (Hermitian) matrix $A$ 的 Eigenvalue 都是正數 $\Leftrightarrow$ Matrix $A$ 為 Symmetric positive definite matrix：<br>$(\Rightarrow)$：假設 $\Lambda$ 為由 $A$ 所有 Eigenvalues 組成的 Diagonal matrix $\Lambda_{ii}=\lambda_i$，且 $\lambda_i \ne \lambda_j, i\ne j$，$Q$ 為各個 Column 由各個 Eigenvalue $\lambda_i$ 所對應的 Eigenvector $x_i$ 組成的 Matrix $\left[\begin{matrix} x_1, \dots, x_N \end{matrix}\right]$。則根據 $Ax=\lambda x$，得到：</p><script type="math/tex; mode=display">\begin{align*}AQ &= Q\Lambda\\A &= Q\Lambda Q^{-1}\end{align*}</script><p>由於 $A$ 為 Symmetric matrix，因此由 Eigenvectors 組成的 $Q$ 為 Orthonomal matrix，Orthonomal matrix 有特性 $Q^{-1}=Q^T$，因此得到：</p><script type="math/tex; mode=display">A = Q\Lambda Q^T</script><p>接著假設任意 Vector $x$ 屬於 Vector space，且不為 $\mathbf{0}$，則：</p><script type="math/tex; mode=display">x^TAx=\underbrace{x^TQ}_{y^T}\Lambda \underbrace{Q^Tx}_{y}=y^T\Lambda y=\sum_{i=0}^N \lambda_i {y_i}^2 >0</script><p>由於 $A$ 為 Symmetric matrix，$\lambda_i&gt;0$ 且 $y_i^2&gt;0$ 因此得到 $x^TAx&gt;0$，得證。</p><p>$(\Leftarrow)$ 假設 Symmetric positive definite matrix $A$ 的任意 Eigenvalue $\lambda$，以及對應的 Eigenvector $x\ne \mathbf{0}$，有 $Ax=\lambda x$。兩邊同乘 $x^T$：</p><script type="math/tex; mode=display">\begin{align*}x^TAx &= \lambda x^Tx\\\lambda &= \frac{x^TAx}{x^Tx} > 0\end{align*}</script><p>由於 $A$ 為 Symmetric positive definite，因此 $x^TAx&gt;0$。且 $x\ne\mathbf{0}$，因此 ${x^Tx} &gt; 0$。得證。</p>
          </div>
<p>基於這些特性，Symmetric positive definite matrix $A$ 可以定義內積。</p>
<p>利用 Symmetric positive definite matrix $A$ 定義一個基於 Vector space $\mathbb{R}^N$ 的內積空間 (Inner product space)，內積定義為：</p>
<script type="math/tex; mode=display">
\langle x,y\rangle_A=x^TAy</script><p>由於 Matrix $A$ 為 Symmetric，我們可以知道這個內積滿足 Inner product 的第一個條件 (Conjugate symmetry)，也就是 $x^TAy=y^TAx$。由於 $x^TAy$ 是線性的，滿足 Inner product 的第二個條件 (Linearity)，由於 $A$ 為 Positive definite，滿足 Inner product 的第三個條件 (Positive-definiteness)。因此，$ \langle \cdot,\cdot\rangle_A $ 是一個合法的內積。</p>
<p>產生 Symmetric positive definite matrix 非常簡單，設一個 $m\times n, m &lt; n$ Matrix $A$，為 Full row rank，$AA^T$ 就會是 Symmetric positive definite matrix。</p>
<div class="note info">
            <p>Prove $AA^T$ 是 Symmetric positive definite matrix：</p><p>假設 Matrix $M=AA^T$，$AA^T$ 為 Symmetric matrix。假設任意 Vector $x\in \mathbb{R}^m$ 且 $x\ne\mathbf{0}$，則：</p><script type="math/tex; mode=display">x^TMx=x^TAA^Tx=\|A^Tx\|>0</script><p>故得證。</p>
          </div>
<h3 id="1-3-Steepest-Descent"><a href="#1-3-Steepest-Descent" class="headerlink" title="1-3. Steepest Descent"></a>1-3. Steepest Descent</h3><ul>
<li><a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a></li>
</ul>
<p>設 Linear system</p>
<script type="math/tex; mode=display">
Ax=b \tag{1.3.1}</script><p>其中 $A\in\mathbb{R}^{N\times N}$ 為 Symmetric positive definite matrix， $b\in \mathbb{R}^N$ 為已知的 Vector。若要求解 vector $x$，根據傳統的解法 (高中、大學教的方法)是，首先先移項 $A$，使用高斯法求出 $A$ 的 Inverse matrix $A^{-1}$ 後，求出 $x$：</p>
<script type="math/tex; mode=display">
x=A^{-1}b</script><p>然而，對於 Large-scale sparse matrix 而言 ($N$ 非常巨大)，求 Inverse matrix 是非常困難的，因此另一種解法就是使用 Numerical method，將問題變成能夠使用 Iterative 趨近的方式來求出近似解。首先將 [式 1.3.1] 移項 $Ax-b=\mathbf{0}$，接著假設 Function $f(x)$，並令其一階微分：</p>
<script type="math/tex; mode=display">
f'(x)=Ax-b \tag{1.3.2}</script><p>根據一階微分，我們能夠得出 Quadratic form：</p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{2}x^TAx-b^Tx+c \tag{1.3.3}</script><p>則目標變成：找到 $x$ 使得 $f’(x)=\mathbf{0}$。也就是說，找到 $x$，使得 $f(x)$ 為極值，且若 $A$ 為 Symmetric positive definite matrix，則該極值一定會是最小值。</p>
<p>證明方法非常簡單，假設 $x^* $ 滿足 $Ax^* = b$，我們只需證明添加上任意 Error $e\in \mathbb{R}^{N}\setminus {\mathbf{0}}$，一定滿足 $f(x^* + e) &gt; f(x^*)$：</p>
<script type="math/tex; mode=display">
\begin{align*}
f(x^* + e) 
    &= \frac{1}{2}(x^* + e)^TA(x^* + e)-b^T(x^* + e)+c\\
    &= \frac{1}{2}(x^*)^T A x^* + e^T\underbrace{Ax^*}_{=b}+\frac{1}{2}e^TAe-b^Tx^*-b^Te+c\\
    &= \underbrace{\left(\frac{1}{2}(x^*)^TAx^*-b^Tx^* + c\right)}_{=f(x^*)}+e^Tb+\frac{1}{2}e^TAe-b^Te\\
    &= f(x^*)+\frac{1}{2}e^TAe > f(x^*)
\end{align*}</script><p>由於 $A$ 是 Positive definite，且 $e\ne \mathbf{0}$，因此最後一項 $\frac{1}{2}e^TAe &gt; 0$，則可以得證當 $Ax^* =b$ 時，$f(x^*)$ 一定為最小值。因此我們將一個 Linear system 的問題變成了 Minimization problem：找到 $x$ 使得 minimize $f(x)$。</p>
<p>接著是要如何解這個 Minimization problem，一種方法就是使用 <strong>Steepest descent method</strong>。</p>
<div class="note info">
            <p>這邊列出一些常用的 term：</p><ul><li>Error: $e_i=x_i-x^*$，寫成 Iterative form，$e_{i+1}=e_i+\alpha_i r_i$</li><li>Residual: $r_i=b-Ax_i=-f’(x_i)$。</li></ul><p>Residual 與 Error 的關係式 $r_i=b-Ax_i=Ax^*-Ax_i=A(x^*-x_i)=-Ae_i$</p>
          </div>
<p>首先從任意一點 $x_0$ 開始，選擇負梯度方向 $-f’(x_0)=r_0$ 前進適當步長 $\alpha_0$，這個步長必須能夠在負梯度方向上最小化下一個 Step 的值 $f(x_1)$，也就是 $\alpha_0=\arg\min f(x_1)=\arg\min f(x_{0}+\alpha_0 r_0)$，通常會使用 Line search 的方式求得。求得步長後，移動到下一個位置 $x_1=x_0 + \alpha_0 r_0$，求出下一個步長 $\alpha_1$，再移動到下一個位置 $x_2=x_1+\alpha_1 r_1$ …，重複動作直到找到最佳解為止。<br><img src="https://i.imgur.com/OK3pGcQ.png" alt=""></p>
<p>至於求解步長 $\alpha_i$ 可以使用求極值 (導數為 $0$) 的方式求得：</p>
<script type="math/tex; mode=display">
\frac{d}{d\alpha}f(x_{i+1})=f'(x_{i+1})^T\frac{d}{d\alpha}x_{i+1}=f'(x_{i+1})^Tr_i=0</script><p>因為 $x_{i+1}=x_i+\alpha_i r_i$。由於 $f’(x_{i+1})=-r_{i+1}$，求得 $\alpha_i$：</p>
<script type="math/tex; mode=display">
\begin{align*}
r^T_{i+1}r_i&=0\\
(b-Ax_{i+1})^Tr_i&=0\\
(b-A(x_i+\alpha_i r_i))^Tr_i&=0\\
(b-Ax_i)^Tr_i-\alpha_i(Ar_i)^Tr_i &= 0\\
(b-Ax_i)^Tr_i &= \alpha_i(Ar_i)^Tr_i\\
r^T_ir_i &= \alpha_i r^T_i(Ar_i)\\
\alpha_i&=\frac{r^T_ir_i}{r^T_iAr_i}
\end{align*}</script><p>最後就可以得到 Steepest descent method：</p>
<script type="math/tex; mode=display">
\begin{gathered}
r_i=b-Ax_i\\
\alpha_i=\frac{r^T_ir_i}{r^T_iAr_i}\\
x_{i+1}=x_i+\alpha_ir_i
\end{gathered}</script><div class="note info">
            <p>Gradient descent：往 Negative gradient 方向前進任意步長 $\alpha$，此步長可以使用固定值，也可以使用估計的方式，例如：SGD 等等。</p><p>Steepest descent：往 Negative gradient 方向前進適當步長 $\alpha$，此步長必須使得下一個值在這個方向上是最小值。</p>
          </div>
<h2 id="2-Methodology"><a href="#2-Methodology" class="headerlink" title="2. Methodology"></a>2. Methodology</h2><ul>
<li><a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a></li>
</ul>
<h3 id="2-1-Conjugate-Gradient"><a href="#2-1-Conjugate-Gradient" class="headerlink" title="2-1. Conjugate Gradient"></a>2-1. Conjugate Gradient</h3><p>Steepest descent 有個缺點在於，經常發生重複搜索同一方向 (如 Figure 8) 的情況，導致收斂速度變慢，為了解決這項問題，有人提出了 Conjugate gradient。首先假設一個 Vector set $\{p_0,p_1,\dots,p_{N-1}\}$ 代表 Search directions，且這些 Vector 兩兩互相垂直 (Orthoginal)。因此可以將每個 Step 的 $x$ 列為：</p>
<script type="math/tex; mode=display">
x_{i+1}=x_i+\alpha_i p_{i} \tag{2.1.1}</script><p>且</p>
<script type="math/tex; mode=display">
x^*=x_0+\sum_{i=0}^{N-1}\alpha_i p_{i} \tag{2.1.2}</script><p>這樣就能夠確保每個搜索方向只需要搜尋一次就能夠找到最佳。由於每個 Step $i$ 的 Search direction $p_i$ 一定會與 Error $e_{i+1}$ 垂直，因此：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^T_ie_{i+1} &= 0\\
p^T(e_i+\alpha_ip_i) &= 0\\
\end{align*}</script><div class="note info">
            <p>Prove 每個 Step $i$ 的 Search direction $p_i$ 一定會與 Error $e_{i+1}$ 垂直：</p><p>因為 $e_{i+1}=x_{i+1}-x^*$ 且 $x^*=x_{i+1}+\sum_{j=i+1}^{N-1}\alpha_jp_j$，因此 $e_{i+1}=-\sum_{j=i+1}^{N-1}\alpha_jp_j$，也就是說，$e_{i+1}$ 是 $\{p_{i+1}, \dots, p_{N-1}\}$ 的 Linear combination，而 $p_{i}$ 與所有$\{p_{i+1}, \dots, p_{N-1}\}$ 互為 Orthogonal，因此可以得證 $p_{i}$ 與 $e_i$ 一定是 Orthogonal。</p>
          </div>
<p>如上根據 Error 的 Iterative form 展開後，可以算出：</p>
<script type="math/tex; mode=display">
\alpha_i=-\frac{p^T_ie_i}{p^T_ip_i}</script><p>然而，尷尬的是我們沒有辦法求出 $e_i$，因為 $e_i=x_i-x^*$，如果可以求得 $e_i$ 那麼 $x^*$ 就已經算出來了。避免掉這個窘境的方式是改變假設 Search directions 兩兩互為 $A$-orthogonal ($A$-conjugate)，而非 Orthogonal：</p>
<script type="math/tex; mode=display">
p^T_iAp_j=0, \quad i\ne j</script><p>這樣假設的原因是因為，如果 $e_i$ 算不出來，只需要對 $e_i$ 乘上 $A$，就會得到 $Ae_i=Ax_i-Ax^*=Ax_i-b$，如此就可以避免掉 $x^*$ 的問題。又因 $Ae_i=-r_i$，因此同 Steepest descent 求 $\alpha$ 可以得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{d}{d\alpha}f(x_{i+1}) &= 0\\
f'(x_{i+1})^T\frac{d}{d\alpha}x_{i+1} &= 0\\
-r^T_{i+1}p_i &= 0\\
(Ae_{i+1})^Tp_i=d_i^TAe_{i+1}&=0
\end{align*}</script><p>因 $e_{i+1}=e_i+\alpha_ip_i$，因此：</p>
<script type="math/tex; mode=display">
d^T_iA(e_i+\alpha_ip_i) = 0</script><p>移項整理後求得 $\alpha_i$：</p>
<script type="math/tex; mode=display">
\begin{align*}
\alpha_i &= -\frac{p^T_iAe_i}{p^T_iAp_i}\\
&=\frac{p^T_ir_i}{p^T_iAp_i} \tag{2.1.3}
\end{align*}</script><p>(能夠想到這種方法的人肯定腦力發揮 100%)</p>
<p>接下來的問題在於，要如何產生 Mutually $A$-orthogonal vectors $\{p_0,\dots,p_{N-1}\}$，可以使用 Gram-Schmidt process，來 Iteratively 產生互相 Orthogonal 的向量。</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process">Gram–Schmidt process - wiki</a></li>
</ul>
<p>假設一組 Linearly independent set $\{u_0, \dots, u_{N-1}\}$，Gram-Schmidt process 利用 Projection 的方式來製造出兩兩互相 Orthogonal 的 Vectors，原理非常簡單：</p>
<p><img src="https://i.imgur.com/bjvovNI.png" width="300px"></p>
<p>首先兩個 Linearly independent vectors $u_0, u_1$，先將 $u_1$ Project 到 $u_0$ 上算出投影向量 $\text{proj}_{u_0}(u_1)$，接著再將 $u_1$ 扣掉 $\text{proj}_{u_0}(u_1)$ 得到的就會是與 $u_0$ Orthogonal 的 Vector $u_1-\text{proj}_{u_0}(u_1)$。此時我們只需要令 $p_0=u_0$、$p_1=u_1-\text{proj}_{d_0}(u_1)$ 就可以得到兩個 Mutually orthogonal vectors $p_0, p_1$ 以同樣方式推廣至 $i$：</p>
<script type="math/tex; mode=display">
\begin{align*}
&p_0=u_0\\
&p_1=u_1-\text{proj}_{p_0}(u_1)\\
&p_2=u_2-\text{proj}_{p_0}(u_2)-\text{proj}_{p_1}(u_2)\\
&p_i=u_i-\sum_{j=0}^{i-1}\text{proj}_{p_j}(u_i) \tag{2.1.4}
\end{align*}</script><p>以此類推就可以得到一組 Mutually orthogonal vectors $\{p_0,\dots,p_{N-1}\}$，然而我們需要的是 $A$-orthogonal 的 vectors，因此可以定義 Projection 為 $A$ 內積空間的 Projection：</p>
<script type="math/tex; mode=display">
\text{proj}_{p_j}(u_i)=\frac{\langle u_i, p_j\rangle_A}{\langle p_j, p_j\rangle_A}p_j = \frac{u_i^TAp_j}{p_jAp_j}p_j \tag{2.1.5}</script><p>至於要如何找到 Linearly independent set $\{u_0, u_1, \dots, u_{N-1}\}$ 一個很直接的方式就是使用現成的 Residual $r$。</p>
<p>我們知道 Residual 與 Error 的關係式 $r_j=-Ae_j$，而 $e_j=x_j-x^*$，$x_j=x_0+\sum_{k=0}^{j-1}\alpha_kp_k$ 且 $x^*=x_0+\sum_{k=0}^{N-1}\alpha_kp_k$，全部展開來後得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
r_j &= -A\left( (x_0+\sum_{k=0}^{j-1}\alpha_kp_k)-(x_0+\sum_{k=0}^{N-1}\alpha_kp_k) \right)\\
    &= -A(-\sum_{k=j}^{N-1}\alpha_kp_k)\\
    &= \sum_{k=j}^{N-1}\alpha_kAp_k
\end{align*}</script><p>由此可以看出，Residual $r_i$ 是由 $\{Ap_{i+1}, \dots, Ap_{N-1}\}$ 組成的 Linear combination，而 $\{Ap_{i+1}, \dots, Ap_{N-1}\}$ 是 Linearly independent set，因此可以知道 $\{r_0, \dots, r_{N-1}\}$ 也一定會是 Linearly independent set。</p>
<p>此外，這邊如果對兩邊同時乘上 $p_i$ 就會發生一件很有趣的事情，其中 $i&lt;j$ ，由於 $p_i$ 與其他 $p_j, \dots, p_{N-1}$ 為 $A$-orthogonal，因此得到：</p>
<script type="math/tex; mode=display">
p_i^Tr_j=\sum_{k=j}^{N-1}\alpha_kp_i^TAp_k=0, \quad i<j \tag{2.1.6}</script><p>使用 Residual $r$ 代替 $u$ 後，令 $\beta$ 為 Projection 的 Coefficient 項：</p>
<script type="math/tex; mode=display">
\beta_{ij}=-\frac{r^T_iAp_j}{p^T_jAp_j}</script><p>因此 [式 2.1.4] 變成：</p>
<script type="math/tex; mode=display">
p_i=r_i+\sum_{j=0}^{i-1}\beta_{ij}p_j \tag{2.1.7}</script><p>將 [式 2.1.7] 兩邊同乘 $r_j$，得到：</p>
<script type="math/tex; mode=display">
p^T_ir_j=r_i^Tr_j+\sum_{k=0}^{i-1}\beta_{ik}p_k^Tr_j</script><p>由 [式 2.1.6] 可以知道當 $i&lt;j$ 時 $p^T_ir_j=0$ 且 $p_k^Tr_j=0$ 因為 $k\le i-1&lt;i&lt;j$，因此：</p>
<script type="math/tex; mode=display">
r_i^Tr_j=0, \quad i\ne j \tag{2.1.8}</script><p>可以得出一個結論：Residual $r$ 兩兩互為 Orthogonal。</p>
<p>除此之外，若 $i=j$ 時，$p^T_ir_i \ne 0$ 但 $p_k^Tr_i=0$ 因為 $k\le i-1&lt;i$，因此得到：</p>
<script type="math/tex; mode=display">
p^T_ir_i=r_i^Tr_i</script><p>代入到 [式 2.1.3] 的 $\alpha_i$，得到新的 $\alpha_i$：</p>
<script type="math/tex; mode=display">
\alpha_i = \frac{p^T_ir_i}{p^T_iAp_i} = \frac{r^T_ir_i}{p^T_iAp_i} \tag{2.1.9}</script><p>已知 $r_{j+1}=r_j-\alpha_jAp_j$，對等號兩邊乘上 $r_i$，整理後同除 $\alpha_j$：</p>
<script type="math/tex; mode=display">
\begin{align*}
r^T_i r_{j+1} &= r^T_ir_j-\alpha_jr^T_iAp_j\\
\alpha_jr^T_iAp_j &= r^T_ir_j - r^T_i r_{j+1}\\
r^T_iAp_j &= \frac{1}{\alpha_j}(r^T_ir_j - r^T_i r_{j+1})
\end{align*}</script><p>根據 [式 2.1.8]，可以得到，只有兩個狀況下 $r^T_iAp_j$ 會有值：</p>
<script type="math/tex; mode=display">
r^T_iAp_j=\begin{cases}
\frac{1}{\alpha_i}r^T_ir_j, \quad& i=j\\
-\frac{1}{\alpha_{i-1}}r^T_i r_{i}, \quad& i=j+1\\
0, \quad&\text{otherwise}
\end{cases}</script><p>將兩邊同除以 $-p_j^TAp_j$，得到 $\beta_{ij}$，由於在 Gram-Schmidt process 中 $\beta_{ij}$ 只存在 $i&lt;j$ 這個 Case，不存在 $i=j$ 的 Case，因此只剩下一種 Case $i=j+1$，剩下其他 Case 都為 $0$：</p>
<script type="math/tex; mode=display">
\beta_{ij}=-\frac{r^T_iAp_j}{p_j^TAp_j}=\begin{cases}
\frac{1}{\alpha_{i-1}}\frac{r^T_ir_i}{p_{i-1}^TAp_{i-1}}, \quad &i=j+1\\
0, \quad&\text{otherwise}
\end{cases}</script><p>將 [式 2.1.9] 的 $\alpha_{i-1}$ 代入，最後得到新的 $\beta$：</p>
<script type="math/tex; mode=display">
\begin{align*}
\beta_i &= \frac{p^T_{i-1}Ap_{i-1}}{r^T_{i-1}r_{i-1}}\frac{r^T_ir_i}{p_{i-1}^TAp_{i-1}}\\
    &= \frac{r^T_ir_i}{r^T_{i-1}r_{i-1}} \tag{2.1.10}
\end{align*}</script><p>結果 $p_i$ 一整個不見了，全部變成 Residual $r$。完全出乎意料。<br>寫到這邊為止的我：</p>
<p><img src="https://i.imgur.com/29vI4jy.png" width="500px"></p>
<p>最後將全部放在一起得到完整的 Conjugate Gradient：</p>
<script type="math/tex; mode=display">
\begin{gathered}
p_0=r_0=b-Ax_0\\
\alpha_i = \frac{r^T_ir_i}{p^T_iAp_i}\\
x_{i+1}=x_i+\alpha_ip_i\\
r_{i+1}=r_i-\alpha_iAp_i\\
\beta_{i+1}=\frac{r^T_{i+1}r_{i+1}}{r^T_ir_i}\\
p_{i+1}=r_{i+1}+\beta_{i+1}p_i
\end{gathered}</script><p>第一行 $p_0=r_0=b-Ax_0$ 是 Initial condition，之後就是持續 Iterate 直到 Residual $|r_{i+1}|\to 0$ 就結束 Iteration。通常可以另外設定一個 Tolerance 參數 $\epsilon \to 0$，當 $|r_{i+1}| &lt; \epsilon$ 時結束 Conjugate gradient。</p>
<p>接下來就是 Convergence Analysis 與 Complexity 分析的部分，雖然這部分才是真正 Numerical Analysis 要做的事情，但礙於篇幅且不是很重要，在這邊就直接忽略，有興趣的可以自行研究 <a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">Ch. 9 Convergence Analysis / Ch. 10 Complexity</a>。這本書真的寫得很棒，有機會一定要看完！(我在說我)</p>
<h3 id="2-2-Preconditioned-Conjugate-Gradient"><a href="#2-2-Preconditioned-Conjugate-Gradient" class="headerlink" title="2-2. Preconditioned Conjugate Gradient"></a>2-2. Preconditioned Conjugate Gradient</h3><p>如果有看 <a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">Ch. 9 Convergence Analysis / Ch. 10 Complexity</a> 的話應該就會知道，Conjugate gradient 的收歛性取決於 Matrix $A$ 的 Condition number $\kappa(A)$ (Iteration 與 $\sqrt{\kappa(A)}$ 成正比)，$\kappa(A)$ 小則容易收斂，$\kappa(A)$ 大則不容易收斂。而 Condition number 定義為 $\kappa(A)=\frac{\lambda_\text{max}}{\lambda_\text{min}}\ge 1$，其中 $\lambda$ 為 $A$ 的 Eigenvalue。(等我學會了 Numerical Analysis 再來補充)</p>
<p>在某些情況下，我們會希望 Conjugate Gradient 能夠再收斂的更快，例如：對於 $N$ 為 Million 等級的 Matrix $A$，我們不希望 Conjugate Gradient 執行 100 萬次才收斂到解答，而是希望在 1000 次內就可以得到近似解。這種情況下我們就會需要對原本的 Linear system 進行 Preconditioning，使得新的 Linear system 能夠更快收斂。</p>
<p>假設 Linear system 不容易收斂 (也有人稱做 ill-conditioned system) $Ax=b$，我們希望可以找到新的 Linear system $\hat{A}\hat{x}=\hat{b}$，且新的 Linear system 能夠收斂的更快。換句話說，希望可以找到一個 $\hat{A}$，滿足 $\kappa(\hat{A})\ll \kappa(A)$。一種方法就是選擇一個 Non-singular matrix $M$，並套用到 Linear system：</p>
<script type="math/tex; mode=display">
M^{-1}Ax=M^{-1}b</script><p>使得 $\kappa(M^{-1}A)\ll \kappa(A)$，同時 $M^{-1}A$ 必須維持 Symmetric positive definite matrix，$M$ 就稱為 <strong>Preconditioner</strong>。若要維持 Symmetric positive definite matrix，最簡單的方式就是 $M$ 同為 Symmetric positive definite matrix，為了確保 $M$ 為 Symmetric positive definite matrix，我們也可以利用另一個 Non-singular matrix $E$，且令 $M=EE^T$ 來表示 $M$，如此就可以確保 $M$ 一定是 Symmetric positive definite matrix (證明請看 [Sec 1-2])</p>
<div class="note info">
            <p>Non-singular matrix $M$ 表示 $M$ 是 Invertible，且具有 Inverse matrix $M^{-1}$。</p>
          </div>
<p>至於要去哪裡找到如此好的 $M$，聰明如你，最直覺的方式就是選擇 $M=A$，如此一來，$M^{-1}A=A^{-1}A=I$，我們就可以確保 $\kappa(M^{-1}A)=\kappa(I)=1\ll\kappa(A)$，同時 $I$ 也是 Symmetric positive definite matrix。然而，事實卻沒有這麼簡單，原因是因為，如果取 $M=A$ 的話，勢必要先計算出 $A^{-1}$ 才能套用進新的 Linear system $A^{-1}Ax=A^{-1}b$，但問題就在於，就是因為 $A$ 的 Inverse matrix $A^{-1}$ 很難求才會需要使用 Conjugate gradient 的方式來求解，要不然 Linear system $Ax=b$ 早就解完了。因此，只能限定 $M\approx A$，且 $M$ 必須更容易計算 Inverse。</p>
<p>最終歸納出 $M$ 必須具有以下條件，只要 $M$ 具有以下條件，就可以大幅縮減 Condition number：</p>
<ul>
<li>$\kappa(M^{-1}A)\ll\kappa(A)$，有一說是 $M^{-1}A$ 的 Eigenvalues 必須要更 Clustered，因為 $\kappa=\frac{\lambda_\text{max}}{\lambda_\text{min}}$</li>
<li>$M^{-1}A$ 是 Symmetric positive definite matrix</li>
<li>$M\approx A \Longrightarrow M^{-1}A\approx A^{-1}A= I$</li>
<li>$M$ Invertible 且比 $A$ 更容易計算 Inverse matrix</li>
</ul>
<p>接著就是如何求解新的 Linear system，一種方式是直接計算出 $M^{-1}$ 後，將新的 $\hat{A}=M^{-1}A$，$\hat{b}=M^{-1}b$ 直接帶入 Conjugate gradient。另一種方式是將 $M$ 分解 $M=EE^T$ 且 $E$ 可能是比 $M$ 還要更容易計算 Inverse 的 Matrix (例如: Triangular matrix)。如果使用第二種 Case 的話，可以將 Linear system 拆解並轉換成新的 Linear system $\hat{A}\hat{x} = \hat{b}$：</p>
<script type="math/tex; mode=display">
\begin{align*}
M^{-1}Ax &= M^{-1}b\\
E^{-1}AE^{-T}x &= E^{-1}E^{-T}b\\
(E^{-1}AE^{-T})(E^Tx) &= (E^{-T}E^T)(E^{-1}b)\\
\hat{A}\hat{x} &= \hat{b}
\end{align*}</script><p>其中 $\hat{A}=E^{-1}AE^{-T}$，$\hat{x}=E^Tx$，$\hat{b}=E^{-1}b$。帶入 Conjugate gradient 後得到新的 Conjugate gradient (其實就是全部加上 Hat)：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\hat{p}_0=\hat{r}_0=\hat{b}-\hat{A}\hat{x}_0\\
\hat{\alpha}_i = \frac{\hat{r}^T_i\hat{r}_i}{\hat{p}^T_i\hat{A}\hat{p}_i}\\
\hat{x}_{i+1}=\hat{x}_i+\hat{\alpha}_i\hat{p}_i\\
\hat{r}_{i+1}=\hat{r}_i-\hat{\alpha}_i\hat{A}\hat{p}_i\\
\hat{\beta}_{i+1}=\frac{\hat{r}^T_{i+1}\hat{r}_{i+1}}{\hat{r}^T_i\hat{r}_i}\\
\hat{p}_{i+1}=\hat{r}_{i+1}+\hat{\beta}_{i+1}\hat{p}_i
\end{gathered}</script><p>接下來就是將 Hat 都展開來。計算出新的 Residual $\hat{r}_i$ 與舊 Residual $r_i$ 的關係：</p>
<script type="math/tex; mode=display">
\hat{r}_i= \hat{b}-\hat{A}\hat{x}_i=E^{-1}b-(E^{-1}AE^{-T})(E^Tx)=E^{-1}r_i</script><p>再由 [式 2.1.2] 可以得到：</p>
<script type="math/tex; mode=display">
\hat{x}=E^Tx=E^T(x_0+\sum_{i=0}^{N-1}\alpha_ip_i)=E^Tx_0+\sum_{i=0}^{N-1}\alpha_iE^Tp_i</script><p>因此定義 $\hat{p}_i=E^Tp_i$，則 $\hat{\alpha}_i$ 可以展開：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{\alpha}_i &= \frac{\hat{r}^T_i\hat{r}_i}{\hat{p}^T_i\hat{A}\hat{p}_i}\\
    &= \frac{(E^{-1}r_i)^T(E^{-1}r_i)}{(E^Tp_i)^T(E^{-1}AE^{-T})(E^Tp_i)}\\
    &= \frac{r_i^T(E^{-T}E^{-1})r_i}{p_i^T(EE^{-1})A(E^{-T}E^T)p_i}\\
    &= \frac{r_i^TM^{-1}r_i}{p_i^TAp_i}
\end{align*}</script><p>接下來將 $\hat{x}_{i+1} = \hat{x}_i+\hat{\alpha}_i\hat{p}_i$ 展開得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{x}_{i+1} &= \hat{x}_i+\hat{\alpha}_i\hat{p}_i\\
E^Tx_{i+1} &= E^Tx_i + \hat{\alpha}_iE^Tp_i\\
E^Tx_{i+1} &= E^T(x_i + \hat{\alpha}_ip_i)\\
x_{i+1} &= x_i + \hat{\alpha}_ip_i
\end{align*}</script><p>接下來 $\hat{r}_{i+1}=\hat{r}_i-\hat{\alpha}_i\hat{A}\hat{p}_i$ 也展開：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{r}_{i+1} &= \hat{r}_i-\hat{\alpha}_i\hat{A}\hat{p}_i\\
E^{-1}r_{i+1} &= E^{-1}r_i-\hat{\alpha}_i(E^{-1}AE^{-T})(E^Tp_i)\\
E^{-1}r_{i+1} &= E^{-1}r_i-\hat{\alpha}_iE^{-1}A(E^{-T}E^T)p_i\\
E^{-1}r_{i+1} &= E^{-1}(r_i-\hat{\alpha}_iAp_i)\\
r_{i+1} &= r_i-\hat{\alpha}_iAp_i
\end{align*}</script><p>接下來展開 $\hat{\beta}_{i+1}$：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{\beta}_{i+1} &= \frac{\hat{r}^T_{i+1}\hat{r}_{i+1}}{\hat{r}^T_i\hat{r}_i}\\
    &= \frac{(E^{-1}r_{i+1})^T(E^{-1}r_{i+1})}{(E^{-1}r_i)^T(E^{-1}r_i)}\\
    &= \frac{r^T_{i+1}(E^{-T}E^{-1})r_{i+1}}{r^T_{i}(E^{-T}E^{-1})r_{i}}\\
    &= \frac{r^T_{i+1}M^{-1}r_{i+1}}{r^T_{i}M^{-1}r_{i}}
\end{align*}</script><p>接下來展開 $\hat{p}_{i+1}=\hat{r}_{i+1}+\hat{\beta}_{i+1}\hat{p}_i$：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{p}_{i+1} &= \hat{r}_{i+1}+\hat{\beta}_{i+1}\hat{p}_i\\
E^Tp_{i+1} &= E^{-1}r_{i+1}+\hat{\beta}_{i+1}E^Tp_i\\
E^{-T}E^Tp_{i+1} &= (E^{-T}E^{-1})r_{i+1} + \hat{\beta}_{i+1}E^{-T}E^Tp_i\\
p_{i+1} &= M^{-1}r_{i+1} + \hat{\beta}_{i+1}p_i
\end{align*}</script><p>最後一件最重要的事情就是展開 $\hat{p}_0=\hat{r}_0$：</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat{p}_0 &= \hat{r}_0\\
E^Tp_0 &= E^{-1} r_0\\
p_0 &= E^{-T}E^{-1} r_0\\
p_0 &= M^{-1}r_0
\end{align*}</script><p>全部整理起來得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
r_0&=b-Ax_0\\
p_0 &= M^{-1}r_0\\
\alpha_i &= \frac{r_i^TM^{-1}r_i}{p_i^TAp_i}\\
x_{i+1} &= x_i + \alpha_ip_i\\
r_{i+1} &= r_i-\alpha_iAp_i\\
\beta_{i+1} &= \frac{r^T_{i+1}M^{-1}r_{i+1}}{r^T_{i}M^{-1}r_{i}}\\
p_{i+1} &= M^{-1}r_{i+1} + \hat{\beta}_{i+1}p_i
\end{align*}</script><p>接著令 $z_i=M^{-1}r_i$ 得到 Preconditioned conjugate gradient：</p>
<script type="math/tex; mode=display">
\begin{align*}
r_0 &= b-Ax_0\\
z_0 &= M^{-1}r_0\\
p_0 &= z_0\\
\alpha_i &= \frac{r_i^Tz_i}{p_i^TAp_i}\\
x_{i+1} &= x_i + \alpha_ip_i\\
r_{i+1} &= r_i-\alpha_iAp_i\\
z_{i+1} &= M^{-1}r_{i+1}\\
\beta_{i+1} &= \frac{r^T_{i+1}z_{i+1}}{r^T_{i}z_{i}}\\
p_{i+1} &= z_{i+1} + \hat{\beta}_{i+1}p_i
\end{align*}</script><p>結果發現其實根本需要 $E$ 的我：<br><img src="https://i.imgur.com/db33QIH.png" width="300px"></p>
<h3 id="2-3-Incomplete-Cholesky-Preconditioned-Conjugate-Gradient-ICCG"><a href="#2-3-Incomplete-Cholesky-Preconditioned-Conjugate-Gradient-ICCG" class="headerlink" title="2-3. Incomplete-Cholesky Preconditioned Conjugate Gradient (ICCG)"></a>2-3. Incomplete-Cholesky Preconditioned Conjugate Gradient (ICCG)</h3><ul>
<li><a href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Positive_semidefinite_matrices">Cholesky decomposition - wiki</a></li>
<li><a href="https://en.wikipedia.org/wiki/Incomplete_Cholesky_factorization">Incomplete Cholesky factorization - wiki</a></li>
</ul>
<p>Preconditioner $M$ 可以有很多種，在上一節中我們討論出 $M$ 必須具有以下特性：</p>
<ul>
<li>$\kappa(M^{-1}A)\ll\kappa(A)$</li>
<li>$M^{-1}A$ 是 Symmetric positive definite matrix</li>
<li>$M\approx A \Longrightarrow M^{-1}A\approx A^{-1}A=I$</li>
<li>$M$ Invertible 且比 $A$ 更容易計算 Inverse matrix</li>
</ul>
<p>但根據 Preconditioned conjugate gradient 計算 $z_{i+1}=M^{-1}r_{i+1}$，其實可以發現不一定要計算出 $M^{-1}$。假若 $M$ 可以被分解為 $M=EE^T$，帶入公式後得到</p>
<script type="math/tex; mode=display">
\begin{align*}
z_{i+1}&=(EE^T)^{-1}r_{i+1}\\
z_{i+1}&= E^{-1}\underbrace{(E^{-T} r_{i+1})}_y\\
z_{i+1}&= E^{-1}y
\end{align*}</script><p>令 $y=E^{-T}r_{i+1}$，原本需要計算出 Inverse matrix $M^{-1}$ 變成只須計算 $E^{-1}$ 並分別求解 $y=E^{-T}r_{i+1}$ 與 $z_{i+1} = E^{-1}y$，就能能夠計算出 $z_{i+1}$。因此若有辦法將 $M$ 分解成更容易計算 Inverse 的 Matrix $E$ 那將會一大福音。正好，Cholesky 認為任何 Symmetric (Hermitian) positive definite matrix $M$ 都能夠分解為 Lower triangular matrix $L$ 及其 Transpose $L^T$ (若非 Real matrix 則為 Conjugate transpose)，也就是 $M=LL^T$，這個方法稱作 Cholesky factorization。而剛好 Triangular matrix 非常好求 Inverse。</p>
<div class="note info">
            <p>Prove: 若 $M$ 為 Symmetric positive definite matrix，則存在唯一的 Cholesky factorization $LL^T$，其中 $L$ 是 Lower triangular matrix，且 Diagonal element 皆為正。</p><p>(等我想到再補充)</p>
          </div>
<p>然而，為了滿足 $M\approx A$ 而直接取 $M=A$ 計算 Cholesky factorization 還是會遇到同樣 $A^{-1}$ 的問題，除此之外，對於 Sparse matrix ($n\ll N\times N$，其中 $n$ 為非零項的數量) 而言， Cholesky factorization 是一大問題：經過分解後會爆出一堆非零項，使得 Sparse matrix 變得不是 Sparse。因此，解決方法就是使用 Incomplete-Cholesky factorization，來維持 Matrix 的 Sparsity。Incomplete-Cholesky factorization 在對 $A$ 計算 Cholesky factorization 的時候會將原本 $A$ 為 $0$ 的項維持為 $0$，只針對非零項計算，因此最後的結果會與原先的 Matrix 擁有相同的 Sparsity，同時可以得到一個很好的 Approximation $M=LL^T\approx A$。</p>
<h3 id="2-4-ICCG-Algorithm"><a href="#2-4-ICCG-Algorithm" class="headerlink" title="2-4. ICCG Algorithm"></a>2-4. ICCG Algorithm</h3><p>Recall 前幾節的結果，Incomplete-Cholesky preconditioned conjugate gradient：</p>
<script type="math/tex; mode=display">
\begin{align*}
r_0 &= b-Ax_0\\
L &= \text{Preconditioning}(A)\\
z_0 &= L^{-1}L^{-T}r_0\\
p_0 &= z_0\\
\alpha_i &= \frac{r_i^Tz_i}{p_i^TAp_i}\\
x_{i+1} &= x_i + \alpha_ip_i\\
r_{i+1} &= r_i-\alpha_iAp_i\\
z_{i+1} &= L^{-1}L^{-T}r_{i+1}\\
\beta_{i+1} &= \frac{r^T_{i+1}z_{i+1}}{r^T_{i}z_{i}}\\
p_{i+1} &= z_{i+1} + \hat{\beta}_{i+1}p_i
\end{align*}</script><p>假設 Initial guess $x_0=\mathbf{0}$，可以得到：</p>
<script type="math/tex; mode=display">
\begin{align*}
x_0 &= 0\\
r_0 &= b
\end{align*}</script><p>轉換成演算法：</p>
<p><img src="https://i.imgur.com/alrj8hn.png" alt=""></p>
<p>基本上這個演算法是一步一步根據推出來的公式計算，最標準的流程。維基百科也是使用這個順序。但為了後續方便使用 cuBLAS/cuSPARSE 實做，我比較習慣稍微調動一些順序變成下圖：</p>
<p><img src="https://i.imgur.com/hL1FiZD.png" alt=""></p>
<p>但基本上這兩個演算法做的事情是完全相同的。</p>
<h2 id="3-cuBLAS-amp-cuSPARSE"><a href="#3-cuBLAS-amp-cuSPARSE" class="headerlink" title="3. cuBLAS &amp; cuSPARSE"></a>3. cuBLAS &amp; cuSPARSE</h2><p>cuBLAS 與 cuSPARSE 都是 CUDA 內建的 Library，但是不同版本 CUDA 的 cuBLAS 與 cuSPARSE API 或多或少會有一些差異，這篇將以 CUDA 11.0 版本為主，此外也會加入一些 CUDA 8.0 的資料作為補充。</p>
<h3 id="3-1-cuBLAS-Introduction"><a href="#3-1-cuBLAS-Introduction" class="headerlink" title="3-1. cuBLAS Introduction"></a>3-1. cuBLAS Introduction</h3><p>cuBLAS 是以 CUDA 實作的 BLAS (Basic Linear Algebra Subprograms) library。提供了一些常用計算 Matrix/Vector 相關的 High-level API。例如：Matrix/vector copy、sort、dot product、multiplication 等等，另外也有提供對於特殊類型矩陣 (symmetric、triangular、hemitian) 做過優化的 API。</p>
<p>cuBLAS 使用 <strong>column-major</strong>，所以在儲存 Array 時需要注意順序。此外，在 Fortran 中是使用 1-based indexing，但在 C/C++ 中是使用 0-based indexing。</p>
<p><img src="https://i.imgur.com/EprrqSz.jpg" alt=""></p>
<p>使用 cuBLAS 時首先需要在程式碼中 Include <code>cublas_v2.h</code>，後面的 <code>v2</code> 代表是新的 API，而舊的 API <code>cublas.h</code> 則是 CUDA 4.0 以前的 API。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cublas_v2.h&gt;</span></span></span><br></pre></td></tr></table></figure></p>
<p>編譯時需要加上 Library 的 Link</p>
<ul>
<li>使用 <code>nvcc</code> Compile 需要加上 <code>-lcublas</code>：<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nvcc hello_cublas.cu -o hello_cublas -lcublas</span><br></pre></td></tr></table></figure></li>
<li>使用 Visual Studio 則是從 <code>Property</code> 加：<ul>
<li>首先到 <code>CUDA C/C++</code> 的地方，在 <code>Additional Include Directories</code> 地方加上 <code>$(CudaToolkitIncludeDir)</code>。這是為了讓 Visual Studio 找的到 cuBLAS 的 header file。<img src="https://i.imgur.com/RgflLlW.png" alt=""></li>
<li>接著到 <code>CUDA Linker</code> 的地方，在 <code>Additional Librbary Directories</code> 地方加上 <code>$(CudaToolkitLibDir)</code>，並在 Additional Dependencies 地方加上 <code>cublas.lib</code><br><img src="https://i.imgur.com/QvGf09v.png" alt=""></li>
</ul>
</li>
</ul>
<p>這樣就可以正常編譯 cuBLAS。<br><div class="note warning">
            <p>需要注意的是 Visual Studio 的 <code>CUDA C/C++</code> 與 <code>CUDA Linker</code> 頁籤是對應 <code>.cu</code> File，如果想要在一般 <code>.c</code> 或 <code>.cpp</code> File 使用 cuBLAS 則必須在 <code>VC++ Directories</code> 與 <code>Linker</code> 下的 <code>Input</code> 頁籤做相同的處理，否則會出現 Link error。<br><img src="https://i.imgur.com/eUDlQzz.png" alt=""><br><img src="https://i.imgur.com/nWBcylz.png" alt=""></p>
          </div></p>
<h3 id="3-2-cuBLAS-APIs"><a href="#3-2-cuBLAS-APIs" class="headerlink" title="3-2. cuBLAS APIs"></a>3-2. cuBLAS APIs</h3><ul>
<li><a href="https://docs.nvidia.com/cuda/cublas/index.html">cuBLAS :: CUDA Toolkit Documentation</a></li>
</ul>
<p>在使用 cuBLAS 的 API 前，需要先呼叫 <code>cublasCreate</code> 建立 cuBLAS 的 Handle，有 Handle 才能正常使用 cuBLAS API，且在呼叫 cuBLAS API 時，Handle 必須傳入 Function。最後程式結束時必須呼叫 <code>cublasDestroy</code> 來刪除 cuBLAS Handle。如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cublasHandle_t cubHandle;</span><br><span class="line">    cublasStatus_t cubStat;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Initialize cuBLAS</span></span><br><span class="line">    cubStat = cublasCreate( &amp;cubHandle );</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Destroy cuBLAS handle</span></span><br><span class="line">    cubStat = cublasDestroy(cubHandle);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>cubStat</code> 用來呼叫的 Function call 是否有正常執行，如果 API 有發生錯誤的話，Error code 會儲存在內，詳細可以參考官方的 Document，每個 Function call 底下都會寫有可能回傳的 Error code。</p>
<p>接下來是建立 Vector，使用 <code>cudaMalloc</code> 在 GPU 建立空間後，將資料複製到 GPU 有兩種方式：</p>
<ol>
<li>第一種是使用 CUDA 原生的 <code>cudaMemcpy</code> 複製到 GPU 上</li>
<li>第二種是使用 cuBLAS API <code>cublasSetVector</code>/<code>cublasGetVector</code> 來存取 GPU 上的空間</li>
</ol>
<p>不管使用哪個 API，最後都必須 Free memory。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> vec[] = &#123;<span class="comment">/*...*/</span>&#125;; <span class="comment">// Vector in Host memory </span></span><br><span class="line"><span class="keyword">double</span> *d_vec;  <span class="comment">// Device pointer</span></span><br><span class="line">cudaMalloc(d_vec, N*<span class="keyword">sizeof</span>(<span class="keyword">double</span>));</span><br><span class="line"><span class="comment">// Set vector</span></span><br><span class="line">cudaMemcpy(d_vec, vec, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>),</span><br><span class="line">                  cudaMemcpyHostToDevice);</span><br><span class="line">cublasSetVector(N, <span class="keyword">sizeof</span>(<span class="keyword">double</span>), vec, <span class="number">1</span>, d_vec, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get vector</span></span><br><span class="line">cudaMemcpy(vec, d_vec, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>),</span><br><span class="line">                  cudaMemcpyDeviceToHost);</span><br><span class="line">cublasGetVector(N, <span class="keyword">sizeof</span>(<span class="keyword">double</span>), d_vec, <span class="number">1</span>, vec, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free memory</span></span><br><span class="line">cudaFree(d_vec);</span><br></pre></td></tr></table></figure>
<p>使用 cuBLAS API <code>cublasSetVector</code>/<code>cublasGetVector</code> 的好處在於，他有提供使用者設定 increment 的功能，也就是複製每個資料間要間隔多少資料，這在從一個 Matrix 中複製出 Column vector 或 Row vector 時非常實用。</p>
<p><img src="https://i.imgur.com/i702v6y.png" width="500px"><br><img src="https://i.imgur.com/zuEdAOU.png" width="500px"></p>
<p>接下來是 Matrix type，同樣使用 <code>cudaMalloc</code> 方式建立 GPU 空間，同 Vector，可以使用 <code>cudaMemcpy</code> 或是使用 cuBLAS API <code>cublasSetMatrix</code>/<code>cublasGetMatrix</code> 存取 Matrix。最重要的是，他是 <strong>Column-major</strong>！因此 Host memory 與 Device memory 都必須儲存成 Column-major 的形式。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cudaMalloc(&amp;d_A, N*M*<span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// copy Matrix A from host to device</span></span><br><span class="line">cublasSetMatrix(M, N, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), A, M, d_A, M);</span><br><span class="line"></span><br><span class="line"><span class="comment">// copy from device to host</span></span><br><span class="line">cublasGetMatrix(M, N, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), d_A, M, A, M);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free memory</span></span><br><span class="line">cudaFree(d_A);</span><br></pre></td></tr></table></figure>
<p>cuBLAS API <code>cublasSetMatrix</code>/<code>cublasGetMatrix</code> 有兩個參數 <code>lda</code> 與 <code>ldb</code>，分別代表 Source 與 Destination Matrix 的 Leading dimension。</p>
<p>接下來是介紹 cuBLAS 的 Operation API，cuBLAS 將 API 分成 3 個 Level：</p>
<ul>
<li>Level-1：與 Vector 相關，或是 Vector-vector operations。例如，<code>min</code>、<code>max</code>、<code>sum</code>、<code>copy</code>、<code>dot product</code>、<code>euclidean norm</code> 等等。</li>
<li>Level-2：Matrix-vector operations。例如，Matrix-vector multiplication 等等。</li>
<li>Level-3：Matrix-matrix operations。例如，Matrix-matrix multiplication 等等。</li>
</ul>
<p>根據不同的資料型態，API 的名稱也會不一樣：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Type</th>
<th>Notation <code>&lt;t&gt;</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>float</code></td>
<td>‘s’ or ‘S’</td>
<td>Real single-precision</td>
</tr>
<tr>
<td><code>double</code></td>
<td>‘d’ or ‘D’</td>
<td>Real double-precision</td>
</tr>
<tr>
<td><code>cuComplex</code></td>
<td>‘c’ or ‘C’</td>
<td>Complex single-precision</td>
</tr>
<tr>
<td><code>cuDoubleComplex</code></td>
<td>‘z’ or ‘Z’</td>
<td>Complex double-precision</td>
</tr>
</tbody>
</table>
</div>
<p>舉例來說， Level-1 的 Function <code>cublas&lt;t&gt;axpy</code>，計算 Vector $x$ 與 Vector $y$ 的加法，$y\gets\alpha x+y$</p>
<p><img src="https://i.imgur.com/QKCou7y.png" alt=""></p>
<p>Level-2 的 Function <code>cublas&lt;t&gt;gemv</code>，計算 Matrix $A$ 與 Vector $x$ 的乘法，$y\gets\alpha \text{OP}(A)x+\beta y$</p>
<p><img src="https://i.imgur.com/oTeomLr.png" alt=""></p>
<p>其中 $\text{OP}$ 是在計算前，cuBLAS 會對 Matrix $A$ 套用的 Operation：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>OP</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUBLAS_OP_N</code></td>
<td>The non-transpose operation is selected</td>
</tr>
<tr>
<td><code>CUBLAS_OP_T</code></td>
<td>The transpose oeration is selected</td>
</tr>
<tr>
<td><code>CUBLAS_OP_C</code></td>
<td>The conjugate transpose operation is selected</td>
</tr>
</tbody>
</table>
</div>
<p>Level-3 的 Function <code>cublas&lt;t&gt;gemm</code> 計算 Matrix $A$ 與 Matrix $B$ 的矩陣乘法，$C\gets\alpha\text{OP}(A)\text{OP}(B)+\beta C$</p>
<p><img src="https://i.imgur.com/4KbJWH4.png" alt=""></p>
<p>Level-3 除了一般的 Matrix-matrix multiplication 之外，cuBLAS 還有提供額外的 API 給特殊的 Matrix 型態使用：<br><code>cublasFillMode_t</code> 用來指定是 Upper triangle 還是 Lower triangle。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cublasFillMode_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUBLAS_FILL_MODE_LOWER</code></td>
<td>The lower part of the matrix is filled</td>
</tr>
<tr>
<td><code>CUBLAS_FILE_MODE_UPPER</code></td>
<td>The upper part of the matrix is filled</td>
</tr>
</tbody>
</table>
</div>
<p>例如 <code>cublas&lt;t&gt;symm</code>，計算 Symmetric matrix-matrix multiplication，如果你的 Matrix 是 Symmetric 就可以使用這個 API 來進行更有效率的計算，則 cuBLAS 在運算時其實只需要取一半的值 (Upper triangle 或是 Lower triangle) 出來就可以了，另外一半是對稱的，這樣在 GPU memory 的存取上會比較有效率。</p>
<p><img src="https://i.imgur.com/cXQpf2O.png" alt=""></p>
<p>另外還有像是 <code>cublas&lt;t&gt;trmm</code> 計算 Triangle matrix-matrix multiplication。</p>
<p><code>cublasDiagType_t</code> 代表 Matrix 的對角線是不是 Unit (Unit matrix 的那個 unit)，如果是的話，cuBLAS 在運算時就會忽略對角線。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cublasDiagType_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUBLAS_DIAG_NON_UNIT</code></td>
<td>The matrix diagonal has non-unit elements</td>
</tr>
<tr>
<td><code>CUBLAS_DIAG_UNIT</code></td>
<td>The matrix diagonal has unit elements</td>
</tr>
</tbody>
</table>
</div>
<p>除此之外，cuBLAS 還有提供 <code>Stream</code> API 可以進行 Asynchronous 的計算，詳細請看 document。</p>
<h3 id="3-3-Hello-cuBLAS"><a href="#3-3-Hello-cuBLAS" class="headerlink" title="3-3. Hello cuBLAS"></a>3-3. Hello cuBLAS</h3><p>這節將會使用 cuBLAS 示範 Matrix-matrix multiplication。首先寫好基本的 Framework：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cublas_v2.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// You can find the source code here:</span></span><br><span class="line"><span class="comment">// https://gist.github.com/Ending2015a/4eb30e7665d91debc723d9c73afec821</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;error_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cublasHandle_t cubHandle;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Initialize cuBLAS</span></span><br><span class="line">    error_check(cublasCreate( &amp;cubHandle ));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Get cuBLAS version information</span></span><br><span class="line">    <span class="keyword">int</span> major_version, minor_version;</span><br><span class="line">    </span><br><span class="line">    error_check(cublasGetProperty( MAJOR_VERSION, &amp;major_version ));</span><br><span class="line">    error_check(cublasGetProperty( MINOR_VERSION, &amp;minor_version ));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Say hello to cuBLAS</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Hello cuBLAS!&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span></span><br><span class="line">              &lt;&lt; <span class="string">&quot;* major version: &quot;</span> &lt;&lt; major_version &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span></span><br><span class="line">              &lt;&lt; <span class="string">&quot;* minor version: &quot;</span> &lt;&lt; minor_version &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> Matrix-matrix multiplication</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Destroy cuBLAS handle</span></span><br><span class="line">    error_check(cublasDestroy(cubHandle));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>這段 Code 在一開始建立了 <code>cubHandle</code>，接著向 cuBLAS 打招呼，順便印出 cuBLAS 的版本資訊，最後將 <code>cubHandle</code> 摧毀，結束程序。</p>
<p>接下來是撰寫 Matrix-matrix multiplication，$AB=C$，其中 $A\in\mathbb{R}^{N\times M}$，$B\in\mathbb{R}^{M\times P}$，$C\in\mathbb{R}^{N\times P}$。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// <span class="doctag">TODO:</span> Matrix-matrix multiplication</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> M = <span class="number">4</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> P = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">float</span> a[N*M];</span><br><span class="line"><span class="keyword">float</span> b[M*P];</span><br><span class="line"><span class="keyword">float</span> c[N*P];</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create matrix A, B in row-major order</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;N*M;++i)</span><br><span class="line">&#123;</span><br><span class="line">    a[i] = (<span class="keyword">float</span>)(i+<span class="number">1</span>); <span class="comment">//1, 2, 3, 4, ~</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;M*P;++i)</span><br><span class="line">&#123;</span><br><span class="line">    b[i] = (<span class="keyword">float</span>)(M*P-i); <span class="comment">//8, 7, 6, 5, ~</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>宣告 Dimension，宣告 Host memory 上的 matrix，需要注意的是<strong>在這邊我是使用 row-major 的方式初始化 Matrix</strong>，後面有點小 Trick。印出來看：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_matrix</span><span class="params">(<span class="keyword">float</span> *mat, <span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">int</span> M)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;N*M;++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; mat[i];</span><br><span class="line">        <span class="keyword">if</span>((i+<span class="number">1</span>)%M==<span class="number">0</span>)</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;, &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//print matrix</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;A: &quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">print_matrix(a, N, M);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;B: &quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">print_matrix(b, M, P);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>
<p>接著是宣告、分配 Device memory 空間，使用 cuBLAS API 複製 Matrix data 到 device：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">float</span> *device_a;</span><br><span class="line"><span class="keyword">float</span> *device_b;</span><br><span class="line"><span class="keyword">float</span> *device_c;</span><br><span class="line"></span><br><span class="line">error_check(cudaMalloc(&amp;device_a, N*M*<span class="keyword">sizeof</span>(<span class="keyword">float</span>)));</span><br><span class="line">error_check(cudaMalloc(&amp;device_b, M*P*<span class="keyword">sizeof</span>(<span class="keyword">float</span>)));</span><br><span class="line">error_check(cudaMalloc(&amp;device_c, N*P*<span class="keyword">sizeof</span>(<span class="keyword">float</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">// copy host matrix &#x27;a&#x27; to device matrix &#x27;device_a&#x27;</span></span><br><span class="line">error_check(cublasSetMatrix(M, N, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), a, M, device_a, M));</span><br><span class="line">error_check(cublasSetMatrix(P, M, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), b, P, device_b, P));</span><br></pre></td></tr></table></figure></p>
<p>這邊因為之後打算使用 <code>cublasSgemm</code> 這個 API 計算矩陣乘法：</p>
<script type="math/tex; mode=display">
C\gets\alpha\text{OP}(A)\text{OP}(B) + \beta C</script><p><code>cublasSgemm</code> 會要求兩個常數 $\alpha$ 與 $\beta$：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">float</span> alpha = <span class="number">1.0f</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">float</span> beta = <span class="number">0.0f</span>;</span><br></pre></td></tr></table></figure><br>直接分別設為 $1$ 與 $0$，計算就會 reduce 成 $C\gets\text{OP}(A)\text{OP}(B)$。</p>
<p>接下來就是 Tricky 的地方。做 Matrix-matrix multiplication：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">error_check(cublasSgemm(cubHandle, CUBLAS_OP_N, CUBLAS_OP_N, P, N, M,</span><br><span class="line">                        &amp;alpha, device_b, P, device_a, M,</span><br><span class="line">                        &amp;beta, device_c, P));</span><br></pre></td></tr></table></figure></p>
<p>這邊要注意 Tricky 的地方。因為 Row-major 與 Column-major 只差在一次 Transpose，因此根據 $(AB)^T=B^TA^T$ 的特性，Row-major 的 matrix <code>b</code> 與 matrix <code>a</code> 在 Column-major 下必須倒過來相乘。在 row-major 下，我們的 Matrix <code>a</code> 與 <code>b</code> 會長這樣：</p>
<script type="math/tex; mode=display">
a=
\left[\begin{matrix}
1 & 2 & 3 & 4\\
5 & 6 & 7 & 8\\
9 & 10 & 11 & 12\\
\end{matrix}\right], \quad
b=
\left[\begin{matrix}
8 & 7 \\
6 & 5 \\
4 & 3 \\
2 & 1 \\
\end{matrix}\right]</script><p>但是 cuBLAS 是 column-major，因此在 cuBLAS 看來矩陣其實長這樣：</p>
<script type="math/tex; mode=display">
a=
\left[\begin{matrix}
1 & 5 & 9 \\
2 & 6 & 10 \\
3 & 7 & 11 \\
4 & 8 & 12\\
\end{matrix}\right], \quad
b=
\left[\begin{matrix}
8 & 6 & 4 & 2\\
7 & 5 & 3 & 1\\
\end{matrix}\right]</script><p>所以在計算 $AB$ 時，必須將 $A$ 與 $B$ 對調，變成 $BA$，如此一來就會計算出 Transpose 過的 $C$</p>
<script type="math/tex; mode=display">
c=\left[\begin{matrix}
8 & 6 & 4 & 2\\
7 & 5 & 3 & 1\\
\end{matrix}\right] \left[\begin{matrix}
1 & 5 & 9 \\
2 & 6 & 10 \\
3 & 7 & 11 \\
4 & 8 & 12\\
\end{matrix}\right] = \left[\begin{matrix}
40 & 120 & 200\\
30 & 94 & 158\\
\end{matrix}\right]</script><p>因為 cuBLAS 是 column-major，因此直接將 <code>device_c</code> copy 回 <code>c</code>，在 row-major 下就會 Transpose 回來，得到正確的值。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// device to host</span></span><br><span class="line">error_check(cublasGetMatrix(P, N, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), device_c, P, c, P));</span><br><span class="line"></span><br><span class="line"><span class="comment">// print answer</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;C: &quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">print_matrix(c, N, P);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>
<p>在 row-major 下，<code>c</code> 為：</p>
<script type="math/tex; mode=display">
c= \left[\begin{matrix}
40 & 30 \\
120 & 94 \\
200 & 158\\
\end{matrix}\right]</script><p>最後不要忘記 Free memory：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Free memory</span></span><br><span class="line">cudaFree(device_a);</span><br><span class="line">cudaFree(device_b);</span><br><span class="line">cudaFree(device_c);</span><br></pre></td></tr></table></figure></p>
<p><del>如果能夠做到這種程度，代表你對 row-major/column-major 關係很熟了</del></p>
<h3 id="3-4-cuSPARSE-Introduction"><a href="#3-4-cuSPARSE-Introduction" class="headerlink" title="3-4. cuSPARSE Introduction"></a>3-4. cuSPARSE Introduction</h3><p>cuSPARSE 是 cuBLAS 的 Sparse version。在 cuBLAS 中使用的都是 Dense vector/matrix，而 cuSPARSE 則提供了一系列 API 用來從事 Sparse vector/matrix 相關的計算。且除了 Matrix/Vector operation 之外，cuSPARSE 也有提供許多求解 Sparse linear system 相關的 API。</p>
<p>Sparse matrix 有一個特點就是矩陣非常大，其中包含了許多 $0$ 項，如果在 GPU 上儲存這些 $0$ 項其實是非常浪費空間的，因此 cuSPARSE 提供了幾種 Data format 來儲存 Sparse matrix，節約 Memory 使用量：</p>
<p><del>這邊我懶得重寫，直接貼我的 Slide</del></p>
<ul>
<li>COO (Coordinate Format)<br>這是最為直接的表示方法，儲存矩陣中每個非 $0$ 項的位置：<br><img src="https://i.imgur.com/ZCJtDi1.png" alt=""></li>
<li>CSR (Compressed Sparse Row Format)<br>這個表示法對 Row 方向的資料進行了壓縮，他紀錄的是 Row 開始的 Entry 的 index。<br><img src="https://i.imgur.com/16gxonJ.png" alt=""></li>
<li>CSC (Compressed Sparse Column Format)<br>這個表示法則是對 Col 進行壓縮<br><img src="https://i.imgur.com/A87jBrY.png" alt=""></li>
<li>另外還有 BSR 與 BSRX 但由於不常使用，就不多介紹</li>
</ul>
<p>Sparse vector 使用兩個 Array 來表示，第一個 Array 儲存 Non-zero term，第二個 Array 則儲存 Non-zero term 的 Index。</p>
<p>使用 cuSPARSE 的方式與 cuBLAS 類似，先 Include <code>cusparse.h</code> (這邊不用 <code>v2</code>)，Compile 時同樣要加上 Link：</p>
<ul>
<li>使用 <code>nvcc</code>：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvcc hello_cusparse.cu -o hello_cusparse -lcusparse</span><br></pre></td></tr></table></figure></li>
<li>使用 Visual Studio，在 <code>Additional Dependencies</code> 加上 <code>cusparse.lib</code></li>
</ul>
<p>就可以正常 Compile。</p>
<h3 id="3-5-cuSPARSE-API"><a href="#3-5-cuSPARSE-API" class="headerlink" title="3-5. cuSPARSE API"></a>3-5. cuSPARSE API</h3><ul>
<li><a href="https://docs.nvidia.com/cuda/cusparse/index.html">cuSPARSE :: CUDA Toolkit Documentation</a></li>
</ul>
<p>在使用 cuSPARSE 時，需要呼叫 <code>cusparseCreate</code> 建立 cuSPARSE 的 Handle <code>cusparseHandle_t</code>，在最後程式結束前則需呼叫 <code>cusparseDestroy</code> Destroy：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cusparseHandle_t cusHandle;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Initialize cuSPARSE</span></span><br><span class="line">    cusparseCreate( &amp;cusHandle );</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Destroy cuSPARSE handle</span></span><br><span class="line">    cusparseDestroy(cusHandle);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>建立 Sparse matrix 時，則需要根據 Data format (COO、CSR、CSC) 建立對應的 Data，如：COO format 需要建立三個 Array：</p>
<ul>
<li><code>cooValA</code> 儲存 Non-zero term</li>
<li><code>cooRowIndA</code> 儲存 Non-zero term 的 row index</li>
<li><code>cooColIndA</code> 儲存 Non-zero term 的 column index。</li>
</ul>
<p>如果想要建立 <code>CSR</code> 或 <code>CSC</code> format 但是不會 (或是不方便) 建立 Row/Column index 話，可以先建立 <code>COO</code> format，使用 cuSPARSE 提供的 Format Conversion API <code>cusparse&lt;t&gt;coo2csr</code> 做 Data format 的轉換。也可以直接從 Dense matrix，使用 <code>cusparse&lt;t&gt;dense2csc</code> / <code>cusparse&lt;t&gt;dense2csr</code> 轉換。</p>
<p>與 cuBLAS 不同的地方在於，cuSPARSE 在宣告 Matrix 時，必須建立該 Matrix 的 Descriptor <code>cusparseMatDescr_t</code>。<code>cusparseMatDescr_t</code> 包含了大略以下資料：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">typeof <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    cusparseMatrixType_t MatrixType;</span><br><span class="line">    cusparseFillMode_t FillMode;</span><br><span class="line">    cusparseDiagType_t DiagType;</span><br><span class="line">    cusparseIndexBase_t IndexBase;</span><br><span class="line">&#125; cusparseMatDescr_t;</span><br></pre></td></tr></table></figure></p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cusparseDiagType_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUSPARSE_DIAG_TYPE_NON_UNIT</code></td>
<td>The matrix diagonal has non-unit elements.</td>
</tr>
<tr>
<td><code>CUSPARSE_DIAG_TYPE_UNIT</code></td>
<td>The matrix diagonal has unit elements.</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cusparseFillMode_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUSPARSE_FILL_MODE_LOWER</code></td>
<td>The lower triangular part is stored.</td>
</tr>
<tr>
<td><code>CUSPARSE_FILL_MODE_UPPER</code></td>
<td>The upper triangular part is stored.</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cusparseIndexBase_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUSPARSE_INDEX_BASE_ZERO</code></td>
<td>The base index is zero.</td>
</tr>
<tr>
<td><code>CUSPARSE_INDEX_BASE_ONE</code></td>
<td>The base index is one.</td>
</tr>
</tbody>
</table>
</div>
<p>在過去的版本中 (CUDA 8.0) 為了節約 Memory，針對特殊的 Matrix (Symmetric、Triangular、Hermitian) 可以在 <code>cusparseMatrixType_t</code> 指定。如果有指定特殊的 Matrix 的話，cuSPARSE 會只使用 Lower triangle 或 Upper triangle 半邊的 Data 做計算，端看 <code>cusparseFillMode_t</code> 是指定 Lower 還是 Upper。如此一來，使用者可以只需要儲存半邊的 Matrix。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cusparseMatrixType_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUSPARSE_MATRIX_TYPE_GENERAL</code></td>
<td>The matrix is general.</td>
</tr>
<tr>
<td><code>CUSPARSE_MATRIX_TYPE_SYMMETRIC</code></td>
<td>The matrix is symmetric. (<em>Deprecated</em>)</td>
</tr>
<tr>
<td><code>CUSPARSE_MATRIX_TYPE_HERMITIAN</code></td>
<td>The matrix is Hermitian. (<em>Deprecated</em>)</td>
</tr>
<tr>
<td><code>CUSPARSE_MATRIX_TYPE_TRIANGULAR</code></td>
<td>The matrix is triangular. (<em>Deprecated</em>)</td>
</tr>
</tbody>
</table>
</div>
<div class="note warning">
            <p>然而在 CUDA 11.0 中，<strong>官方不再鼓勵使用者使用 General 以外的 Matrix Type</strong>，原因是在許多計算中 (Matrix-vector multiplication、Preconditioners、Lienar system solvers 等等)，使用<strong>非 General type </strong>的計算耗費時間會是 General type 的 10 倍慢**，因此在 CUDA 11.0 中，大部分的 API 不再支援非 General type 的 Matrix。因此使用者在指定 Matrix type 時直接填寫 <code>CUSPARSE_MATRIX_TYPE_GENERAL</code> 即可。</p>
          </div>
<p>以下示範如何宣告一個 <code>cusparseMatDescr_t</code>：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Create descriptor for the matrix A</span></span><br><span class="line">cusparseMatDescr_t descr_A = <span class="number">0</span>;</span><br><span class="line">cusparseCreateMatDescr(&amp;descr_A);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Set properties of the matrix A</span></span><br><span class="line">cusparseSetMatType(descr_A, CUSPARSE_MATRIX_TYPE_GENERAL);</span><br><span class="line">cusparseSetMatIndexBase(descr_A, CUSPARSE_INDEX_BASE_ZERO);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free descriptor</span></span><br><span class="line">cusparseDestroyMatDescr(descr_A);</span><br></pre></td></tr></table></figure></p>
<p>cuSPARSE 同樣有提供 3 個 Level 的 API：</p>
<ul>
<li>Level-1：Sparse vector operation (在 CUDA 11.0 中已列為 <em>Deprecated</em>)</li>
<li>Level-2：Sparse matrix-dense/sparse vector operation 以及求解 Matrix-vector sparse linear system 相關 API</li>
<li>Level-3：Sparse matrix-dense matrix operation 以及求解 Matrix-matrix sparse linear system</li>
</ul>
<p>需要注意的是 Level-2 與 Level-3 求解 Sparse linear system 的 API 有限定 Matrix 必須是 Triangular，如果矩陣非 Triangular 則只會使用 Lower triangle 的 Data，Upper triangle 忽略。</p>
<p>注意到，Level-3 是 Sparse matrix 對 dense matrix operation，Sparse matrix 對 sparse matrix 的 operation 則是放在 Extra API，但其中也有一些 API 已經列為 <em>Deprecated</em>。而多數 API 被列為 Deprecated 的原因是因為，CUDA 11.0 決定要將一些常用的 Multiplication operation 做整合，因此提供了另一種型態的 API —- Generic API。</p>
<p>使用 Generic API 的方法與 cuBLAS 大不相同。首先需要建立對應 Data format 的 Descriptor (不同於 <code>cusparseMatDescr_t</code>)。例如，如果要建立 <code>CSR</code> format 的 Sparse matrix 則必須呼叫對應的 API 來建立 Descriptor：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cusparseSpMatDescr_t mat_A;</span><br><span class="line">cusparseCreateCsr(&amp;mat_A, N, N, nnz, d_rowPtr, d_colIdx, d_A, </span><br><span class="line">                 CUSPARSE_INDEX_32I, <span class="comment">// row index data type (int)</span></span><br><span class="line">                 CUSPARSE_INDEX_32I, <span class="comment">// col index data type (int) </span></span><br><span class="line">                 CUSPARSE_INDEX_BASE_ZERO, <span class="comment">// 0-based index</span></span><br><span class="line">                 CUDA_R_64F);  <span class="comment">// Data type (real double-floating point)</span></span><br></pre></td></tr></table></figure></p>
<p>建立 Dense vector：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cusparseDnVecDescr_t vec_x;</span><br><span class="line">cusparseCreateDnVec(&amp;vec_x, N, d_x, CUDA_R_64F);</span><br></pre></td></tr></table></figure></p>
<p>建立完後，呼叫 Generic API 時都是使用 Descriptor 來代表 Vector/Matrix。例如 <code>cusparseSpMV</code> 計算 Sparse-matrix 對 dense-vector 的 Multiplication：</p>
<script type="math/tex; mode=display">
y\gets \alpha \text{OP}(A)x+\beta y</script><p><img src="https://i.imgur.com/zxsshA5.png" alt=""></p>
<p>計算分成兩個階段，首先使用者必須自行呼叫 <code>cusparseSpMV_bufferSize</code> 估算 <code>cusparseSPMV</code> 所需的額外 Buffer 空間大小，之後自行呼叫 <code>cudaMalloc</code> 建立後，再呼叫 <code>cusparseSpMV</code> 換成計算。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> alpha = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">double</span> beta = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">size_t</span> buf_size = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">double</span> *d_buf;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Compute buffer size in computing matrix-vector multiplocation</span></span><br><span class="line">cusparseSpMV_bufferSize(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE, </span><br><span class="line">                        &amp;alpha, mat_A, vec_x, </span><br><span class="line">                        &amp;beta, vec_y, </span><br><span class="line">                        CUDA_R_64F, </span><br><span class="line">                        CUSPARSE_CSRMV_ALG1, </span><br><span class="line">                        &amp;buf_size);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Allocate buffer memory</span></span><br><span class="line">cudaMalloc(&amp;d_buf, buf_size);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Perform matrix-vector multiplocation</span></span><br><span class="line">cusparseSpMV(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE, </span><br><span class="line">             &amp;alpha, mat_A, vec_x, </span><br><span class="line">             &amp;beta, vec_y, </span><br><span class="line">             CUDA_R_64F, </span><br><span class="line">             CUSPARSE_CSRMV_ALG1, </span><br><span class="line">             d_buf));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free buffer</span></span><br><span class="line">cudaFree(d_buf);</span><br></pre></td></tr></table></figure></p>
<p>最後需要呼叫對應的 API 來銷毀 Generic API 的 descriptor<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Destroy descriptor</span></span><br><span class="line">cusparseDestroySpMat(mat_A);</span><br><span class="line">cusparseDestroyDnVec(vec_x);</span><br><span class="line">cusparseDestroyDnVec(vec_y);</span><br></pre></td></tr></table></figure></p>
<p>求解 Sparse linear system 在 CUDA 11.0 中也有大幅度的變更，原先在 CUDA 8.0 中，求解 Sparse linear system 時 cuSPARSE 會自己建立刪除額外的 Buffer，但從 CUDA 11.0 開始也需要使用者自行處理 Buffer。除此之外，Sparse linear system 只支援 Triangular matrix，若 Matrix 不是 Triangular，cuSPARSE 會忽略除了 Lower triangle 以外的 Data。</p>
<p>求解 Sparse linear system 前需要先建立對應 Data format 的 Infomation object，接著分成兩個階段 (Phase)，首先先執行 Analysis phase 分析 Matrix 型態，之後再執行 Solve phase 解出 Linear system。以下示範使用 <code>cusparse&lt;t&gt;csrsv2</code> 求解 Sparse linear system $\text{OP}(A)y=\alpha x$：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Create infomation object</span></span><br><span class="line">csrsv2Info_t info_A;</span><br><span class="line">cusparseCreateCsrsv2Info(&amp;info_A);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> buf_size;</span><br><span class="line"><span class="keyword">double</span> *d_buf;</span><br><span class="line"><span class="comment">// Compute buffer size for solving linear system</span></span><br><span class="line">cusparseDcsrsv2_bufferSize(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE,</span><br><span class="line">                          N, nnz, descr_A,  <span class="comment">//descr_A: cusparseMatDescr_t</span></span><br><span class="line">                          d_A, d_rowIdx, d_colIdx,</span><br><span class="line">                          info_A, &amp;buf_size);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Allocate buffer memory</span></span><br><span class="line">cudaMalloc(&amp;d_buf, buf_size);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Analysis phase</span></span><br><span class="line">cusparseDcsrsv2_analysis(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE,</span><br><span class="line">                        N, nnz, descr_A,  <span class="comment">//descr_A: cusparseMatDescr_t</span></span><br><span class="line">                        d_A, d_rowIdx, d_colIdx,</span><br><span class="line">                        info_A, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf);</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> alpha = <span class="number">1</span>;</span><br><span class="line"><span class="comment">// Solve phase</span></span><br><span class="line">cusparseDcsrsv2_solve(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE,</span><br><span class="line">                     N, nnz, &amp;alpha, descr_A, <span class="comment">//descr_A: cusparseMatDescr_t</span></span><br><span class="line">                     d_A, d_rowIdx, d_colIdx,</span><br><span class="line">                     info_A, d_x, d_y, </span><br><span class="line">                     CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free buffer</span></span><br><span class="line">cudaFree(d_buf);</span><br><span class="line"><span class="comment">// Destroy information object</span></span><br><span class="line">cusparseDestroyCsrsv2Info(info_A);</span><br></pre></td></tr></table></figure></p>
<p>求解 Linear system 這類 API 在 CUDA 11.0 時有新增一個新的參數<code>cusparseSolvePolicy_t</code>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>cusparseSolvePolicy_t</code></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>CUSPARSE_SOLVE_POLICY_NO_LEVEL</code></td>
<td>No level information is generated and used.</td>
</tr>
<tr>
<td><code>CUSPARSE_SOLVE_POLICY_USE_LEVEL</code></td>
<td>Generate and use level information.</td>
</tr>
</tbody>
</table>
</div>
<p>在 Document 裡，每個 API 底下的解說都有寫這個參數的用途，但並沒有說的很詳細，我也不清楚這個參數實際的用途。簡而言之，這個參數能夠提升某些運算的效率，本身並不影響計算的結果。</p>
<p>除了這些 API 之外，cuSPARSE 還有提供其他額外的 API，例如 Preconditioner、Format conversion 等等，詳細請看 Document。</p>
<h3 id="3-6-Hello-cuSPARSE"><a href="#3-6-Hello-cuSPARSE" class="headerlink" title="3-6. Hello cuSPARSE"></a>3-6. Hello cuSPARSE</h3><p>這邊範例將會介紹如何使用 cuSPARSE 計算 Sparse matrix dense vector multiplication。首先建立基礎 Framework。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cusparse.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cublas_v2.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// You can find the source code here:</span></span><br><span class="line"><span class="comment">// https://gist.github.com/Ending2015a/4eb30e7665d91debc723d9c73afec821</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;error_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Initialize cuBLAS / cuSPARSE</span></span><br><span class="line">    cublasHandle_t cubHandle = <span class="number">0</span>;</span><br><span class="line">    error_check(cublasCreate(&amp;cubHandle));</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">cusparsehandle_t</span> cusHandle = <span class="number">0</span>;</span><br><span class="line">    error_check(cusparseCreate(&amp;cusHandle));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> Sparse matrix dense vector multiplication</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Destroy handles</span></span><br><span class="line">    cusparseDestroy(cusHandle);</span><br><span class="line">    cublasDestroy(cubhandle);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下來在 Host 建立一個 Sparse matrix 與一個 Dense vector，這邊隨便寫一些 Hash 函數來產生亂數：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// random hash (2D to 1D)</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">hash21</span><span class="params">(<span class="keyword">double</span> x, <span class="keyword">double</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    x = (x*<span class="number">24.36854</span> + y*<span class="number">15.75427</span> + <span class="number">43.454614</span>);</span><br><span class="line">    y = (x*<span class="number">57.5654</span> + y*<span class="number">21.1435</span> + <span class="number">37.159636</span>);</span><br><span class="line">    x = <span class="built_in">sin</span>(x+y)*<span class="number">516.918738574</span>;</span><br><span class="line">    <span class="keyword">return</span> ((<span class="keyword">int</span>)((<span class="number">-1.</span> + <span class="number">2.</span> * <span class="built_in">fabs</span>(x - (<span class="keyword">long</span>)x))*<span class="number">100.</span>))*<span class="number">0.1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// create tridiagonal matrix in column-major order</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">createDenseMatrix</span><span class="params">(<span class="keyword">double</span> **o_mat, <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">double</span> *mat = <span class="keyword">new</span> <span class="keyword">double</span>[N*N]&#123;&#125;;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> x=<span class="number">0</span>;x&lt;N;++x)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> y=<span class="number">0</span>;y&lt;N;++y)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(x == y)</span><br><span class="line">                mat[y+x*N] = hash21(x, y);</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>( <span class="built_in">abs</span>(x-y) == <span class="number">1</span> )</span><br><span class="line">                mat[y+x*N] = hash21(x, y);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    *o_mat = mat;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// create random vector</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">createDenseVector</span><span class="params">(<span class="keyword">double</span> **o_vec, <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">double</span> *vec = <span class="keyword">new</span> <span class="keyword">double</span>[N]&#123;&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> x=<span class="number">0</span>;x&lt;N;++x)</span><br><span class="line">    &#123;</span><br><span class="line">        vec[x] = hash21(x, -x+<span class="number">10</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    *o_vec = vec;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// print matrix</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_matrix</span><span class="params">(<span class="keyword">double</span> *mat, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;N;++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;N;++j)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(j != <span class="number">0</span>) <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;, &quot;</span>;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::fixed &lt;&lt; <span class="built_in">std</span>::setprecision(<span class="number">2</span>) &lt;&lt; mat[i + j*N];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// print vector</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_vector</span><span class="params">(<span class="keyword">double</span> *vec, <span class="keyword">const</span> <span class="keyword">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;N;++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(i != <span class="number">0</span>) <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;, &quot;</span>;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::fixed &lt;&lt; <span class="built_in">std</span>::setprecision(<span class="number">2</span>) &lt;&lt; vec[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>這邊不需要太在意寫了甚麼，基本上就是用盡各種辦法建立了一個 Sparse matrix 跟 Dense vector。</p>
<p>接著回到 <code>TODO</code> 的地方繼續完成 Matrix vector multiplication。建立 Matrix 跟 Vector，copy 到 GPU 上：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// <span class="doctag">TODO:</span> Sparse matrix dense vector multiplication</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate random dense tridiagonal matrix A (column-major)</span></span><br><span class="line"><span class="keyword">int</span> N = <span class="number">5</span>;</span><br><span class="line"><span class="keyword">double</span> *A = <span class="number">0</span>;</span><br><span class="line">createDenseMatrix(&amp;A, N);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print matrix</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;My matrix &#x27;A&#x27;: &quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">print_matrix(A, N);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate random dense vector b</span></span><br><span class="line"><span class="keyword">double</span> *b = <span class="number">0</span>;</span><br><span class="line">createDenseVector(&amp;b, N);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print matrix</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;My vector &#x27;b&#x27;: &quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">print_vector(b, N);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Device pointer (Dense)</span></span><br><span class="line"><span class="keyword">double</span> *d_A = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">double</span> *d_b = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">double</span> *d_c = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Allocate GPU memory</span></span><br><span class="line">error_check(cudaMalloc(&amp;d_A, <span class="keyword">sizeof</span>(<span class="keyword">double</span>) * N * N));</span><br><span class="line">error_check(cudaMalloc(&amp;d_b, <span class="keyword">sizeof</span>(<span class="keyword">double</span>) * N));</span><br><span class="line">error_check(cudaMalloc(&amp;d_c, <span class="keyword">sizeof</span>(<span class="keyword">double</span>) * N));</span><br><span class="line"></span><br><span class="line"><span class="comment">// you can also use cudaMemcpy/cudaMemcpy2D as well.</span></span><br><span class="line">error_check(cublasSetMatrix(N, N, <span class="keyword">sizeof</span>(<span class="keyword">double</span>), A, N, d_A, N));</span><br><span class="line">error_check(cublasSetVector(N, <span class="keyword">sizeof</span>(<span class="keyword">double</span>), b, <span class="number">1</span>, d_b, <span class="number">1</span>));</span><br></pre></td></tr></table></figure></p>
<p>建立 Matrix A 的 Description：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Create descriptor of the matrix A</span></span><br><span class="line">cusparseMatDescr_t descr_A = <span class="number">0</span>;</span><br><span class="line">error_check(cusparseCreateMatDescr(&amp;descr_A));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Set properties of the matrix A</span></span><br><span class="line">error_check(cusparseSetMatType(descr_A, CUSPARSE_MATRIX_TYPE_GENERAL));</span><br><span class="line">error_check(cusparseSetMatIndexBase(descr_A, CUSPARSE_INDEX_BASE_ZERO));</span><br></pre></td></tr></table></figure></p>
<p>接下來要將 Dense matrix A 轉換成 Sparse 的 Format，因此要先計算 Matrix A 的 Non-zero term 數量，這個地方可以使用 cuSPARSE API <code>cusparseDnnz</code> 計算，之後再使用 <code>cusparseDdense2csr</code> 將 Dense format 的 Matrix A 轉換成 CSR format。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Count non-zero terms</span></span><br><span class="line"><span class="keyword">int</span> nnz = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> *d_nnz_perRow = <span class="number">0</span>;</span><br><span class="line">error_check(cudaMalloc(&amp;d_nnz_perRow, N * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">error_check(cusparseDnnz(cusHandle, CUSPARSE_DIRECTION_ROW, N, </span><br><span class="line">                        N, descr_A, d_A, </span><br><span class="line">                        N, d_nnz_perRow, &amp;nnz));</span><br><span class="line"><span class="comment">// Print message</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Total number of non-zero terms in dense matrix A = &quot;</span> &lt;&lt; nnz &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert dense format to csr format</span></span><br><span class="line"><span class="keyword">double</span> *d_csrValA = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> *d_rowPtrA = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> *d_colIdxA = <span class="number">0</span>;</span><br><span class="line">error_check(cudaMalloc(&amp;d_csrValA, nnz * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">error_check(cudaMalloc(&amp;d_rowPtrA, (N+<span class="number">1</span>) * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">error_check(cudaMalloc(&amp;d_colIdxA, nnz * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">error_check(cusparseDdense2csr(cusHandle, N, N, </span><br><span class="line">                            descr_A, d_A, </span><br><span class="line">                            N, d_nnz_perRow,</span><br><span class="line">                            d_csrValA, d_rowPtrA, d_colIdxA));</span><br></pre></td></tr></table></figure></p>
<p>接著因為要使用 Generic API <code>cusparseSpMV</code> 計算 Matrix vector multiplication，因此要先建立 Descriptors：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Create Generic API descriptor</span></span><br><span class="line">cusparseSpMatDescr_t mat_A;</span><br><span class="line">error_check(cusparseCreateCsr(&amp;mat_A, N, N, nnz,</span><br><span class="line">    d_rowPtrA, d_colIdxA, d_csrValA,</span><br><span class="line">    CUSPARSE_INDEX_32I, CUSPARSE_INDEX_32I,</span><br><span class="line">    CUSPARSE_INDEX_BASE_ZERO, CUDA_R_64F));</span><br><span class="line"></span><br><span class="line">cusparseDnVecDescr_t vec_b;</span><br><span class="line">error_check(cusparseCreateDnVec(&amp;vec_b, N, d_b, CUDA_R_64F));</span><br><span class="line"></span><br><span class="line">cusparseDnVecDescr_t vec_c;</span><br><span class="line">error_check(cusparseCreateDnVec(&amp;vec_c, N, d_c, CUDA_R_64F));</span><br></pre></td></tr></table></figure></p>
<p>接下來就是計算 Matrix vector multiplication，先呼叫 <code>cusparseSpMV_bufferSize</code> 計算計算所需的 Buffer 空間，接著呼叫 <code>cusparseSpMV</code> 完成計算：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">size_t</span> buf_size;</span><br><span class="line"><span class="keyword">double</span> *d_buf;</span><br><span class="line"><span class="keyword">double</span> alpha = <span class="number">1.0</span>;</span><br><span class="line"><span class="keyword">double</span> beta = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line">error_check(cusparseSpMV_bufferSize(cusHandle, </span><br><span class="line">                                    CUSPARSE_OPERATION_NON_TRANSPOSE,</span><br><span class="line">                                    &amp;alpha, mat_A, vec_b, </span><br><span class="line">                                    &amp;beta, vec_c, CUDA_R_64F, </span><br><span class="line">                                    CUSPARSE_CSRMV_ALG1, &amp;buf_size));</span><br><span class="line"><span class="comment">// Allocate buffer</span></span><br><span class="line">cudaMalloc(&amp;d_buf, buf_size);</span><br><span class="line"></span><br><span class="line">error_check(cusparseSpMV(cusHandle, </span><br><span class="line">                         CUSPARSE_OPERATION_NON_TRANSPOSE,</span><br><span class="line">                         &amp;alpha, mat_A, vec_b, </span><br><span class="line">                         &amp;beta, vec_c, CUDA_R_64F, </span><br><span class="line">                         CUSPARSE_CSRMV_ALG1, d_buf));</span><br></pre></td></tr></table></figure></p>
<p>最後將解答從 GPU memory 複製到 Host，並印出答案：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// device to host</span></span><br><span class="line"><span class="keyword">double</span> *c = <span class="keyword">new</span> <span class="keyword">double</span>[N]&#123;&#125;;</span><br><span class="line">error_check(cublasGetVector(N, <span class="keyword">sizeof</span>(<span class="keyword">double</span>), d_c, <span class="number">1</span>, c, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// print answer</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Answer: &quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">print_vector(c, N);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure></p>
<p>程式最後將所有資源 Free 掉：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span>[] A;</span><br><span class="line"><span class="keyword">delete</span>[] b;</span><br><span class="line"><span class="keyword">delete</span>[] c;</span><br><span class="line">cudaFree(d_A);</span><br><span class="line">cudaFree(d_b);</span><br><span class="line">cudaFree(d_c);</span><br><span class="line">cudaFree(d_nnz_perRow);</span><br><span class="line">cudaFree(d_csrValA);</span><br><span class="line">cudaFree(d_rowPtrA);</span><br><span class="line">cudaFree(d_colIdxA);</span><br><span class="line">cudaFree(d_buf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free handles, descriptors, </span></span><br><span class="line">cusparseDestroyMatDescr(descr_A);</span><br><span class="line">cusparseDestroySpMat(mat_A);</span><br><span class="line">cusparseDestroyDnVec(vec_b);</span><br><span class="line">cusparseDestroyDnVec(vec_c);</span><br></pre></td></tr></table></figure></p>
<p>成功的的話就會算出答案：<br><code>-33.48, 52.97, -47.62, 82.27, 3.97</code></p>
<h2 id="4-Implementation"><a href="#4-Implementation" class="headerlink" title="4. Implementation"></a>4. Implementation</h2><p>這章節將解說如何使用 cuBLAS/cuSPARSE 實做 Incomplete-Cholesky preconditioned conjugate gradient。</p>
<h3 id="4-1-Frameworks"><a href="#4-1-Frameworks" class="headerlink" title="4-1. Frameworks"></a>4-1. Frameworks</h3><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iomanip&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cublas_v2.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cusparse.h&gt;</span></span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> inputPath = <span class="string">&quot;testcase/size1M/case_1M.in&quot;</span>; <span class="comment">// Input file path</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> answerPath = <span class="string">&quot;testcase/size1M/case_1M.out&quot;</span>; <span class="comment">// Answer file path</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* TODO */</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先要處理的就是 Matrix 的讀寫。</p>
<ul>
<li>Input：使用 COO 格式<ul>
<li>$N$ (32-bit int)：表示 Matrix $A$ 大小為 $N\times N$，Vector $b$ 維度為 $N$</li>
<li>$\text{nz}$ (32-bit int)：表示矩陣 $A$ 具有的 Non-zero term 數量</li>
<li>3-tuple 有 $\text{nz}$ 個：<ul>
<li>$i$ (32-bit int)：表示 Row index (0-based)</li>
<li>$j$ (32-bit int)：表示 Column index (0-based)</li>
<li>$A_{ij}$ (64-bit float)：表示 Element $A_{ij}$ 的值</li>
</ul>
</li>
<li>$N$ 個 64-bit float 代表 Vector $b$</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/dt0WMYA.png" width="500px"></p>
<ul>
<li>Output：<ul>
<li>$N$ (32-bit int)：表示 Vector $x$ 的維度</li>
<li>$N$ 個 64-bit float 代表 Vector $x$</li>
</ul>
</li>
</ul>
<p><img src="https://i.imgur.com/gmrsO1x.png" width="240px"></p>
<p>因此讀檔的部份：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Read testcase</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">read</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span> filePath,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> *pN, <span class="keyword">int</span> *pnz,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">double</span> **cooVal,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> **cooRowIdx, <span class="keyword">int</span> **cooColIdx,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">double</span> **b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::ifstream <span class="title">in</span><span class="params">(filePath, <span class="built_in">std</span>::ios::binary)</span></span>;</span><br><span class="line"></span><br><span class="line">    in.read((<span class="keyword">char</span>*)pN, <span class="keyword">sizeof</span>(<span class="keyword">int</span>));  <span class="comment">// read N</span></span><br><span class="line">    in.read((<span class="keyword">char</span>*)pnz, <span class="keyword">sizeof</span>(<span class="keyword">int</span>)); <span class="comment">// read nz</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create array</span></span><br><span class="line">    *cooVal = <span class="keyword">new</span> <span class="keyword">double</span>[*pnz]&#123;&#125;;</span><br><span class="line">    *cooRowIdx = <span class="keyword">new</span> <span class="keyword">int</span>[*pnz]&#123;&#125;;</span><br><span class="line">    *cooColIdx = <span class="keyword">new</span> <span class="keyword">int</span>[*pnz]&#123;&#125;;</span><br><span class="line">    *b = <span class="keyword">new</span> <span class="keyword">double</span>[*pN]&#123;&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// read each element Aij</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; *pnz; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        in.read((<span class="keyword">char</span>*)&amp;(*cooRowIdx)[i], <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">        in.read((<span class="keyword">char</span>*)&amp;(*cooColIdx)[i], <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">        in.read((<span class="keyword">char</span>*)&amp;(*cooVal)[i], <span class="keyword">sizeof</span>(<span class="keyword">double</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// read b</span></span><br><span class="line">    in.read((<span class="keyword">char</span>*)(*b), <span class="keyword">sizeof</span>(<span class="keyword">double</span>)*(*pN));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read answer</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">readAnswer</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span> filePath,</span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">int</span> *pN, <span class="keyword">double</span> **x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::ifstream <span class="title">in</span><span class="params">(filePath, <span class="built_in">std</span>::ios::binary)</span></span>;</span><br><span class="line"></span><br><span class="line">    in.read((<span class="keyword">char</span>*)pN, <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line"></span><br><span class="line">    *x = <span class="keyword">new</span> <span class="keyword">double</span>[*pN]&#123;&#125;;</span><br><span class="line"></span><br><span class="line">    in.read((<span class="keyword">char</span>*)(*x), <span class="keyword">sizeof</span>(<span class="keyword">double</span>)*(*pN));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>讀檔這邊因為會需要更改傳入的參數，因此使用 Call by reference。接下來回到 <code>main</code>：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// in `main` function</span></span><br><span class="line"><span class="keyword">int</span> N;</span><br><span class="line"><span class="keyword">int</span> nz;</span><br><span class="line"><span class="keyword">double</span> *A;</span><br><span class="line"><span class="keyword">int</span> *rowIdxA;</span><br><span class="line"><span class="keyword">int</span> *colIdxA;</span><br><span class="line"><span class="keyword">double</span> *b;</span><br><span class="line">read(inputPath, &amp;N, &amp;nz, &amp;A, &amp;rowIdxA, &amp;colIdxA, &amp;b);</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> *ans_x;</span><br><span class="line">readAnswer(answerPath, &amp;N, &amp;ans_x);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print message</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;N = &quot;</span> &lt;&lt; N &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;nz = &quot;</span> &lt;&lt; nz &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure></p>
<p>讀檔到這邊就完成了，由於讀進來的 Matrix $A$ 是 COO，需要轉換成 CSR，因此需要先初始化 cuBLAS/cuSPARSE：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Create handles</span></span><br><span class="line">cublasHandle_t cubHandle;</span><br><span class="line">cusparseHandle_t cusHandle;</span><br><span class="line"></span><br><span class="line">error_check(cublasCreate(&amp;cubHandle));</span><br><span class="line">error_check(cusparseCreate(&amp;cusHandle));</span><br></pre></td></tr></table></figure></p>
<p>接著分配 Matrix $A$ 在 GPU 上的 Memory，並複製一份過去：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Allocate GPU memory &amp; copy matrix/vector to device</span></span><br><span class="line"><span class="keyword">double</span> *d_A;</span><br><span class="line"><span class="keyword">int</span> *d_rowIdxA; <span class="comment">// COO</span></span><br><span class="line"><span class="keyword">int</span> *d_rowPtrA; <span class="comment">// CSR</span></span><br><span class="line"><span class="keyword">int</span> *d_colIdxA;</span><br><span class="line"><span class="keyword">double</span> *d_b;</span><br><span class="line"></span><br><span class="line">error_check(cudaMalloc(&amp;d_A, nz * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">error_check(cudaMalloc(&amp;d_rowIdxA, nz * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">error_check(cudaMalloc(&amp;d_rowPtrA, (N + <span class="number">1</span>) * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));  <span class="comment">// (N+1) !!!!</span></span><br><span class="line">error_check(cudaMalloc(&amp;d_colIdxA, nz * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));</span><br><span class="line">error_check(cudaMalloc(&amp;d_b, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line"></span><br><span class="line">error_check(cudaMemcpy(d_A, A, nz * <span class="keyword">sizeof</span>(<span class="keyword">double</span>), cudaMemcpyHostToDevice));</span><br><span class="line">error_check(cudaMemcpy(d_rowIdxA, rowIdxA, nz * <span class="keyword">sizeof</span>(<span class="keyword">int</span>), cudaMemcpyHostToDevice));</span><br><span class="line">error_check(cudaMemcpy(d_colIdxA, colIdxA, nz * <span class="keyword">sizeof</span>(<span class="keyword">int</span>), cudaMemcpyHostToDevice));</span><br><span class="line">error_check(cudaMemcpy(d_b, b, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>), cudaMemcpyHostToDevice));</span><br></pre></td></tr></table></figure></p>
<p>這邊需要注意 CSR 格式是針對 RowIdx 做壓縮，壓縮後的大小為 $N+1$，在 Document 上有寫。接著就是 Call <a href="https://docs.nvidia.com/cuda/cusparse/index.html#coo2csr"><code>cusparseXcoo2csr</code></a> 做轉換：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Convert matrix A from COO format to CSR format</span></span><br><span class="line">error_check(cusparseXcoo2csr(cusHandle, d_rowIdxA, nz, N,</span><br><span class="line">                    d_rowPtrA, CUSPARSE_INDEX_BASE_ZERO));</span><br></pre></td></tr></table></figure></p>
<p>接著就是建立 ICCGsolver 跟 Call solve，內容的部份會在 4-2 節實作：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Create conjugate gradient solver</span></span><br><span class="line"><span class="keyword">int</span> max_iter = <span class="number">1000</span>;</span><br><span class="line"><span class="keyword">double</span> tolerance = <span class="number">1e-12</span>;</span><br><span class="line"></span><br><span class="line"><span class="function">ICCGsolver <span class="title">solver</span><span class="params">(max_iter, tolerance, cubHandle, cusHandle)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print message</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Solving linear system...&quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">bool</span> res = solver.solve(N, nz, d_A, d_rowPtrA, d_colIdxA, d_b);</span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; (res ? <span class="string">&quot;Converged!&quot;</span>: <span class="string">&quot;Failed to converge&quot;</span>) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure></p>
<p>這邊我讓 <code>solver.solve</code> 回傳 <code>true</code> 如果 <code>solver</code> 成功在 <code>max_iter</code> Iteration 內收斂 (小於 <code>tolerance</code>) 的話，否則回傳 <code>false</code>。接下來就是將解答從 GPU 摳回 Host memory，並驗證結果：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> *x = <span class="keyword">new</span> <span class="keyword">double</span>[N] &#123;&#125;;</span><br><span class="line">error_check(cudaMemcpy(x, solver.x_ptr(), N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>), cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> tol = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i)</span><br><span class="line">&#123;</span><br><span class="line">    tol += x[i] - ans_x[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// print message</span></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Solved in &quot;</span> &lt;&lt; solver.iter_count() &lt;&lt; <span class="string">&quot; iterations, final norm(r) = &quot;</span></span><br><span class="line">        &lt;&lt; <span class="built_in">std</span>::scientific &lt;&lt; solver.err() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Total error (compared with ans_x): &quot;</span> &lt;&lt; tol &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure></p>
<p>最後要記得把 Memory 都 Free 掉：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Free Host memory</span></span><br><span class="line"><span class="keyword">delete</span>[] A;</span><br><span class="line"><span class="keyword">delete</span>[] rowIdxA;</span><br><span class="line"><span class="keyword">delete</span>[] colIdxA;</span><br><span class="line"><span class="keyword">delete</span>[] b;</span><br><span class="line"><span class="keyword">delete</span>[] ans_x;</span><br><span class="line"><span class="keyword">delete</span>[] x;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free Device memory</span></span><br><span class="line">cudaFree(d_A);</span><br><span class="line">cudaFree(d_rowIdxA);</span><br><span class="line">cudaFree(d_rowPtrA);</span><br><span class="line">cudaFree(d_colIdxA);</span><br><span class="line">cudaFree(d_b);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free handles</span></span><br><span class="line">cublasDestroy(cubHandle);</span><br><span class="line">cusparseDestroy(cusHandle);</span><br></pre></td></tr></table></figure></p>
<p>到這邊 <code>main</code> 就完成了，下一節就是實作 <code>ICCGsolver</code>。</p>
<h3 id="4-2-Main-Algorithm"><a href="#4-2-Main-Algorithm" class="headerlink" title="4-2. Main Algorithm"></a>4-2. Main Algorithm</h3><p>先定義 <code>class ICCGsolver</code>：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ICCGsolver</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    cublasHandle_t cubHandle;</span><br><span class="line">    cusparseHandle_t cusHandle;</span><br><span class="line">    </span><br><span class="line">    cusparseMatDescr_t descr_A;</span><br><span class="line">    cusparseMatDescr_t descr_L;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// host data</span></span><br><span class="line">    <span class="keyword">int</span> N = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> nz = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> max_iter;</span><br><span class="line">    <span class="keyword">int</span> k;  <span class="comment">// k iteration</span></span><br><span class="line">    <span class="keyword">double</span> tolerance;</span><br><span class="line">    <span class="keyword">double</span> alpha;</span><br><span class="line">    <span class="keyword">double</span> beta;</span><br><span class="line">    <span class="keyword">double</span> rTr;</span><br><span class="line">    <span class="keyword">double</span> pTq;</span><br><span class="line">    <span class="keyword">double</span> rho;    <span class="comment">//rho&#123;k&#125;</span></span><br><span class="line">    <span class="keyword">double</span> <span class="keyword">rho_t</span>;  <span class="comment">//rho&#123;k-1&#125;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span> one = <span class="number">1.0</span>;  <span class="comment">// constant</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">double</span> zero = <span class="number">0.0</span>;   <span class="comment">// constant</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// device data</span></span><br><span class="line">    <span class="keyword">double</span> *d_ic = <span class="literal">nullptr</span>;  <span class="comment">// Factorized L</span></span><br><span class="line">    <span class="keyword">double</span> *d_x = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_y = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_z = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_r = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_rt = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_xt = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_q = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">double</span> *d_p = <span class="literal">nullptr</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">bool</span> release_cusHandle = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">bool</span> release_cubHandle = <span class="literal">false</span>;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    ICCGsolver(<span class="keyword">int</span> max_iter = <span class="number">1000</span>, <span class="keyword">double</span> tol = <span class="number">1e-12</span>,</span><br><span class="line">        cublasHandle_t cub_handle = <span class="literal">NULL</span>,</span><br><span class="line">        cusparseHandle_t cus_handle = <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    ~ICCGsolver();</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">solve</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">int</span> nz,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">double</span> *d_A, <span class="keyword">int</span> *d_rowIdx, <span class="keyword">int</span> *d_colIdx,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">double</span> *d_b)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">double</span> *<span class="title">x_ptr</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">err</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">iter_count</span><span class="params">()</span></span>;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">allocate_nz_memory</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">allocate_N_memory</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">free_nz_memory</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">free_N_memory</span><span class="params">()</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">check_and_resize</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">int</span> nz)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>只要有前綴 <code>d_*</code> 表示他是指向 GPU device 的 Pointer。</p>
<ul>
<li>首先是 Line <code>4</code>、<code>5</code> 宣告 Handle，接著是因為我們會用到兩個 Matrix 分別為 $A$ 與 Factorized matrix $L$，因此宣告兩個 Descriptor。</li>
<li><code>11</code>~<code>23</code> 是計算時會用到的變數。</li>
<li><code>25</code>~<code>34</code> 則是 GPU 計算實用到的變數。</li>
<li><code>36</code>~<code>37</code> 用來紀錄 Handler 需要需要自動釋放，如果 Handler 是使用者宣告 <code>ICCGsolver</code> 時傳入的則使用者要自行釋放，如果不是則自動釋放。</li>
<li><code>40</code>~<code>42</code> 是 Constructor，第一個參數 <code>max_iter</code> 表示 Iteration 的上限，<code>tol</code> 表示 Tolerance，<code>cub_handle</code> 與 <code>cus_handle</code> 可傳入可不傳入。</li>
<li><code>44</code> 是 Destructor，要 Free 掉所有 GPU memory。</li>
<li><code>46</code>~<code>48</code> 是解方程的 API，<code>N</code>、<code>nz</code> 分別表示 Matrix 大小以及 Non-zero term 數量，<code>d_A</code>、<code>d_rowIdx</code>、<code>d_colIdx</code> 為 CSR format 的 Matrix $A$，<code>d_b</code> 是 Vector $b$。</li>
<li><code>50</code> 用來取得 <code>d_x</code> Pointer，可以利用這個 Pointer 將答案複製出來。</li>
<li><code>51</code> 用來取得誤差值 $|r_k|$。</li>
<li><code>52</code> 用來取得 Iteration count $k$。</li>
<li><code>55</code>~<code>60</code> 用來分配、釋放 GPU memory。</li>
</ul>
<p>首先是完成 Constructor 與 Destructor：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">ICCGsolver::ICCGsolver(<span class="keyword">int</span> max_iter, <span class="keyword">double</span> tol,</span><br><span class="line">    cublasHandle_t cub_handle, cusparseHandle_t cus_handle) </span><br><span class="line">    : max_iter(max_iter), tolerance(tol), </span><br><span class="line">      cubHandle(cub_handle), cusHandle(cus_handle)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// create cuBLAS handle</span></span><br><span class="line">    <span class="keyword">if</span> (cubHandle == <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        error_check(cublasCreate(&amp;cubHandle));</span><br><span class="line">        release_cubHandle = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// create cuSPARSE handle</span></span><br><span class="line">    <span class="keyword">if</span> (cusHandle == <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        error_check(cusparseCreate(&amp;cusHandle));</span><br><span class="line">        release_cusHandle = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create descriptor for matrix A</span></span><br><span class="line">    error_check(cusparseCreateMatDescr(&amp;descr_A));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initialize properties of matrix A</span></span><br><span class="line">    error_check(cusparseSetMatType(descr_A, CUSPARSE_MATRIX_TYPE_GENERAL));</span><br><span class="line">    error_check(cusparseSetMatFillMode(descr_A, CUSPARSE_FILL_MODE_LOWER));</span><br><span class="line">    error_check(cusparseSetMatDiagType(descr_A, CUSPARSE_DIAG_TYPE_NON_UNIT));</span><br><span class="line">    error_check(cusparseSetMatIndexBase(descr_A, CUSPARSE_INDEX_BASE_ZERO));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create descriptor for matrix L</span></span><br><span class="line">    error_check(cusparseCreateMatDescr(&amp;descr_L));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initialize properties of matrix L</span></span><br><span class="line">    error_check(cusparseSetMatType(descr_L, CUSPARSE_MATRIX_TYPE_GENERAL));</span><br><span class="line">    error_check(cusparseSetMatFillMode(descr_L, CUSPARSE_FILL_MODE_LOWER));</span><br><span class="line">    error_check(cusparseSetMatIndexBase(descr_L, CUSPARSE_INDEX_BASE_ZERO));</span><br><span class="line">    error_check(cusparseSetMatDiagType(descr_L, CUSPARSE_DIAG_TYPE_NON_UNIT));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Constructor 做的事情很簡單，初始化傳入得參數後，建立 Matrix $A$ 與 $L$ 的 Descriptor，這邊需要注意的是在 CUDA 11.0 <code>cusparseMatDescr_t</code> 已經有快要 <em>Deprecate</em> 的傾向，因此這邊設定 Properties 也沒什麼選項可選，首先是 <code>cusparseSetMatType</code>，目前除了 <code>CUSPARSE_MATRIX_TYPE_GENERAL</code> 選項以外其他都 <em>Deprecate</em> 因此直接勇敢的設成 <code>CUSPARSE_MATRIX_TYPE_GENERAL</code> 就可以了，剩下就是除了 <code>cusparseSetMatIndexBase</code> 需要照實填寫外，其他都會被忽略，並不是很重要。</p>
<p>接下來是 Destructor：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">ICCGsolver::~ICCGsolver()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// free data</span></span><br><span class="line">    free_nz_memory();</span><br><span class="line">    free_N_memory();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// release descriptor</span></span><br><span class="line">    cusparseDestroyMatDescr(descr_A);</span><br><span class="line">    cusparseDestroyMatDescr(descr_L);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// release handles</span></span><br><span class="line">    <span class="keyword">if</span> (release_cubHandle)</span><br><span class="line">    &#123;</span><br><span class="line">        cublasDestroy(cubHandle);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (release_cusHandle)</span><br><span class="line">    &#123;</span><br><span class="line">        cusparseDestroy(cusHandle);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Destructor 做的事情更簡單，就是將 GPU memory 都 Free 掉，Descriptor 也 Free 掉，Handle 看是不是使用者傳入的，如果不是，就一起毀掉，如果是就不用毀掉。</p>
<p>接下來是除了 <code>solve</code> 以外的 Function：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">double</span> *<span class="title">ICCGsolver::x_ptr</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> d_x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">ICCGsolver::err</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> rTr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">ICCGsolver::iter_count</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> k;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>x_ptr()</code> 用來取得答案的 GPU Pointer。<code>err()</code> 用來取得誤差值 $|r_k|=r^Tr$。<code>iter_count()</code>用來取得最後一次 <code>solve</code> 的 Iteration count。接下來 Memory 相關：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ICCGsolver::check_and_resize</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">int</span> nz)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// allocate N</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;N &lt; N)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;N = N;</span><br><span class="line">        free_N_memory();</span><br><span class="line">        allocate_N_memory();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;nz &lt; nz)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;nz = nz;</span><br><span class="line">        free_nz_memory();</span><br><span class="line">        allocate_nz_memory();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>check_and_resize</code> 用來檢查，呼叫 <code>solve</code> 時新傳進來的 $N$ 與 $\text{nz}$ 有沒有比目前分配的還大，如果比較大，就要 Free 掉並重新分配一個更大的空間來做計算。</p>
<p>因此釋放與分配空間的 Function：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ICCGsolver::allocate_N_memory</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    error_check(cudaMalloc(&amp;d_x, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&amp;d_y, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&amp;d_z, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&amp;d_r, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&amp;d_rt, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&amp;d_xt, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&amp;d_q, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">    error_check(cudaMalloc(&amp;d_p, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ICCGsolver::allocate_nz_memory</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    error_check(cudaMalloc(&amp;d_ic, nz * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ICCGsolver::free_N_memory</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cudaFree(d_x);</span><br><span class="line">    cudaFree(d_y);</span><br><span class="line">    cudaFree(d_z);</span><br><span class="line">    cudaFree(d_r);</span><br><span class="line">    cudaFree(d_rt);</span><br><span class="line">    cudaFree(d_xt);</span><br><span class="line">    cudaFree(d_q);</span><br><span class="line">    cudaFree(d_p);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ICCGsolver::free_nz_memory</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cudaFree(d_ic);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>到這邊，週邊的 Function 全部完成了，總算可以進入正題：Preconditioned conjugate gradient。建議這邊實作的時候可以搭配 Algorithm 1。</p>
<p>Recall Algorithm 1：<br><img src="https://i.imgur.com/hL1FiZD.png" alt=""></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ICCGsolver::solve</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">int</span> nz,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> *d_A, <span class="keyword">int</span> *d_rowIdx, <span class="keyword">int</span> *d_colIdx,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> *d_b, <span class="keyword">double</span> *d_guess)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    check_and_resize(N, nz);</span><br><span class="line">     </span><br><span class="line">    <span class="comment">// --- 1. Create cuSPARSE generic API objects ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">     </span><br><span class="line">    <span class="comment">// --- 2. Perform incomplete cholesky factorization ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// --- 3. Prepare for performing conjugate gradient ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// --- 4. Perform conjugate gradient ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// --- 5. Finalize ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rTr &lt; tolrance; <span class="comment">// return true if converged</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先一開始需要先檢查、分配 GPU 空間，因此 Call <code>check_and_resize</code>，接著我將分成 5 個部份分別實作。</p>
<p>先從第二部份開始，這邊會對應到 Algorithm 1 的 Line <code>4</code>：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// --- Perform incomplete cholesky factorization ---</span></span><br><span class="line">csric02Info_t icinfo_A;</span><br><span class="line"><span class="keyword">size_t</span> buf_size = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">size_t</span> u_temp_buf_size = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">size_t</span> u_temp_buf_size2 = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> i_temp_buf_size = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">void</span> *d_buf = <span class="literal">NULL</span>;</span><br></pre></td></tr></table></figure></p>
<p>cuSPARSE 進行 Incomplete cholesky factorization 分成幾個步驟：</p>
<ul>
<li>Call <code>cusparseCreateCsric02Info</code> 建立 <code>csric02Info_t</code> object</li>
<li>Call <code>cusparseDcsric02_bufferSize</code> 讓 cuSPARSE 估計計算 Factorization 所需的 Buffer 大小，然後分配 Buffer <code>d_buf</code> 空間</li>
<li>Call <code>cusparseDcsric02_analysis</code> (analysis phase) 讓 cuSPARSE 分析 Matrix $A$ 的 Sparsity。</li>
<li>Call <code>cusparseDcsric02</code> (solve phase) 進行 Factorization</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Create info object for incomplete-cholesky factorization</span></span><br><span class="line">error_check(cusparseCreateCsric02Info(&amp;icinfo_A));</span><br><span class="line"><span class="comment">// Compute buffer size in computing ic factorization</span></span><br><span class="line">error_check(cusparseDcsric02_bufferSize(cusHandle, N, nz, </span><br><span class="line">    descr_A, d_A, d_rowIdx, d_colIdx, icinfo_A, &amp;i_temp_buf_size));</span><br><span class="line">buf_size = i_temp_buf_size;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create buffer</span></span><br><span class="line">error_check(cudaMalloc(&amp;d_buf, buf_size));</span><br><span class="line"><span class="comment">// Copy A</span></span><br><span class="line">error_check(cudaMemcpy(d_ic, d_A, nz * <span class="keyword">sizeof</span>(<span class="keyword">double</span>), cudaMemcpyDeviceToDevice));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Perform incomplete-choleskey factorization: analysis phase</span></span><br><span class="line">error_check(cusparseDcsric02_analysis(cusHandle, N, nz,</span><br><span class="line">    descr_A, d_ic, d_rowIdx, d_colIdx, icinfo_A, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Perform incomplete-choleskey factorization: solve phase</span></span><br><span class="line">error_check(cusparseDcsric02(cusHandle, N, nz,</span><br><span class="line">    descr_A, d_ic, d_rowIdx, d_colIdx, icinfo_A, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf));</span><br></pre></td></tr></table></figure>
<p>這邊我將 <code>d_A</code> 複製到 <code>d_ic</code> 是因為，cuSPARSE 在進行 Factorization 的時候會直接將結果覆寫到 Input array 上，因此這邊將 <code>d_A</code> 複製一份到 <code>d_ic</code>，將 Factorized 的 Lower triangular matrix 存到 <code>d_ic</code> 裡面。</p>
<p>另外，根據 Document 的說法，<code>CUSPARSE_SOLVE_POLICY_USE_LEVEL</code> 有時會對計算進行稍微的優化，有時不會，因此這邊要設定 <code>NO_LEVEL</code> 或 <code>USE_LEVEL</code> 都可以，但是 Analysis phase 與 Solve phase 的設定一定要一致。</p>
<p>接下來第三部份是估算 Conjugate gradient 所需的 Buffer 空間：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// --- Prepare for performing conjugate gradient ---</span></span><br><span class="line"><span class="comment">// Create info object for factorized matrix L, LT</span></span><br><span class="line">csrsv2Info_t info_L, info_U;</span><br><span class="line">error_check(cusparseCreateCsrsv2Info(&amp;info_L));</span><br><span class="line">error_check(cusparseCreateCsrsv2Info(&amp;info_U));</span><br></pre></td></tr></table></figure></p>
<p>在 Algorithm 1 Line <code>9</code> 跟 <code>10</code> 有計算到 $L$ 與 $L^T$ 的 Inverse，但實際上 cuSPARSE 並沒有求 Inverse matrix 的 API，因此可以將這兩行看做求解兩個 Triangular sparse linear system：$Ly=r_k$、求 $y$ 與 $L^Tz_k=y$ 求 $z_k$。而求解 Triancular sparse linear system 可以使用 <code>cusparseDcsrsv2</code> 這個 API。因此首先我們需要建立兩個 <code>csrsv2Info_t</code>：</p>
<ul>
<li>利用 <code>info_L</code> 代表 $L$ (Lower triangular matrix)</li>
<li>利用 <code>info_U</code> 代表 $L^T$ (Upper triangular matrix)</li>
</ul>
<p>接下來就是分別估算解 Triangular sparse linear system 所需的 Buffer 大小：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Compute buffer size in solving linear system</span></span><br><span class="line">error_check(cusparseDcsrsv2_bufferSize(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE, </span><br><span class="line">    N, nz, descr_L, d_ic, d_rowIdx, d_colIdx, info_L, &amp;i_temp_buf_size));</span><br><span class="line"></span><br><span class="line">u_temp_buf_size = i_temp_buf_size;</span><br><span class="line"></span><br><span class="line">error_check(cusparseDcsrsv2_bufferSize(cusHandle, CUSPARSE_OPERATION_TRANSPOSE,</span><br><span class="line">    N, nz, descr_L, d_ic, d_rowIdx, d_colIdx, info_U, &amp;i_temp_buf_size));</span><br><span class="line"></span><br><span class="line"><span class="comment">// check whether need more buffer</span></span><br><span class="line"><span class="keyword">if</span> (i_temp_buf_size &gt; u_temp_buf_size)</span><br><span class="line">&#123;</span><br><span class="line">    u_temp_buf_size = i_temp_buf_size;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>除此之外，Conjugate gradient 有發生一次 Sparse Matrix-dense vector multiplication 在 Algorithm 1 Line <code>17</code> $q\gets Ap_k$，這個需要使用 cuSPARSE 的 Generic API <code>cusparseSpMV</code>，因此先回到第一部份宣告 Generic API 所需的 Objects：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// --- 1. Create cuSPARSE generic API objects ---</span></span><br><span class="line">cusparseSpMatDescr_t smat_A;</span><br><span class="line">error_check(cusparseCreateCsr(&amp;smat_A, N, N, nz, d_rowIdx, d_colIdx, d_A, CUSPARSE_INDEX_32I,</span><br><span class="line">    CUSPARSE_INDEX_32I, CUSPARSE_INDEX_BASE_ZERO, CUDA_R_64F));</span><br><span class="line"></span><br><span class="line">cusparseDnVecDescr_t dvec_p;</span><br><span class="line">error_check(cusparseCreateDnVec(&amp;dvec_p, N, d_p, CUDA_R_64F));</span><br><span class="line"></span><br><span class="line">cusparseDnVecDescr_t dvec_q;</span><br><span class="line">error_check(cusparseCreateDnVec(&amp;dvec_q, N, d_q, CUDA_R_64F));</span><br></pre></td></tr></table></figure></p>
<p>建立一個 CSR Matrix $A$ 的 Descriptor，建立 Dense vector $p_k$ 與 $q$ 的 Descriptor。再回到第三部份完成剩下的部份，估算 Sparse Matrix-dense vector multiplication 所需的 Buffer 數量：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Compute buffer size for matrix-vector multiplication</span></span><br><span class="line">error_check(cusparseSpMV_bufferSize(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE, &amp;one, smat_A,</span><br><span class="line">    dvec_p, &amp;zero, dvec_q, CUDA_R_64F, CUSPARSE_CSRMV_ALG1, &amp;u_temp_buf_size2));</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (u_temp_buf_size2 &gt; u_temp_buf_size)</span><br><span class="line">&#123;</span><br><span class="line">    u_temp_buf_size = u_temp_buf_size2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然後分配空間：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// re-allocate buffer</span></span><br><span class="line"><span class="keyword">if</span> (u_temp_buf_size &gt; buf_size)</span><br><span class="line">&#123;</span><br><span class="line">    buf_size = u_temp_buf_size;</span><br><span class="line">    cudaFree(d_buf);</span><br><span class="line">    error_check(cudaMalloc(&amp;d_buf, buf_size));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>這邊需要注意是，通常 Natrix-vector multiplication 會需要比 Incomplete cholesky factorization 與 Solving lienar system 還要更多的 Buffer。如果 Buffer 空間不夠時，cuSPARSE 會回傳 <code>CUSPARSE_STATUS_INTERNAL_ERROR</code>。</p>
<p>分配完 Buffer 後，就可以先對 Triangular sparse linear system 進行 Analysis phase：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// analysis phase</span></span><br><span class="line">error_check(cusparseDcsrsv2_analysis(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE,</span><br><span class="line">    N, nz, descr_L, d_ic, d_rowIdx, d_colIdx, info_L, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf));</span><br><span class="line">error_check(cusparseDcsrsv2_analysis(cusHandle, CUSPARSE_OPERATION_TRANSPOSE,</span><br><span class="line">    N, nz, descr_L, d_ic, d_rowIdx, d_colIdx, info_U, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf));</span><br></pre></td></tr></table></figure></p>
<p>這邊需要注意的是 <code>d_ic</code> 是 Lower triangular matrix，因此在分析 $L$ (Lower triangular matrix) 時要設定 Operation 為 <code>CUSPARSE_OPERATION_NON_TRANSPOSE</code>，但是在分析 $L^T$ (Upper triangular matrix) 時則要設為 <strong><code>CUSPARSE_OPERATION_TRANSPOSE</code></strong>，這個非常容易被忽略。</p>
<p>所以到目前為止，計算 Conjugate gradient 的事前準備都已經完成了：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ICCGsolver::solve</span><span class="params">(<span class="keyword">int</span> N, <span class="keyword">int</span> nz,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> *d_A, <span class="keyword">int</span> *d_rowIdx, <span class="keyword">int</span> *d_colIdx,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">double</span> *d_b, <span class="keyword">double</span> *d_guess)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    check_and_resize(N, nz);</span><br><span class="line">     </span><br><span class="line">    <span class="comment">// --- 1. Create cuSPARSE generic API objects ---</span></span><br><span class="line">    <span class="comment">// DONE</span></span><br><span class="line">     </span><br><span class="line">    <span class="comment">// --- 2. Perform incomplete cholesky factorization ---</span></span><br><span class="line">    <span class="comment">// DONE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// --- 3. Prepare for performing conjugate gradient ---</span></span><br><span class="line">    <span class="comment">// DONE</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// --- 4. Perform conjugate gradient ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// --- 5. Finalize ---</span></span><br><span class="line">    <span class="comment">// TODO</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rTr &lt; tolrance; <span class="comment">// return true if converged</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>接下來第四部份就是按照 Algorithm 1 一行一行刻：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// x = 0</span></span><br><span class="line">error_check(cudaMemset(d_x, <span class="number">0</span>, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>)));</span><br><span class="line"><span class="comment">// r0 = b  (since x == 0, b - A*x = b)</span></span><br><span class="line">error_check(cudaMemcpy(d_r, d_b, N * <span class="keyword">sizeof</span>(<span class="keyword">double</span>), cudaMemcpyDeviceToDevice));</span><br></pre></td></tr></table></figure><br>Line <code>2</code>、<code>3</code> 初始化 $x_0$ 與 $r_0$。接下來就是 For 迴圈的內容：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(k = <span class="number">0</span>; k &lt; max_iter; ++k)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//TODO</span></span><br><span class="line">&#125;<span class="comment">//EndFor</span></span><br></pre></td></tr></table></figure></p>
<p>Algorithm 1 Line <code>6</code>~<code>8</code>，計算 $|r_k|$，由於 r_k$ 是 Dense vector，因此可以使用 cuBLAS 的 API <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-nrm2"><code>cublasDnrm2</code></a> 計算：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// if ||rk|| &lt; tolerance</span></span><br><span class="line">error_check(cublasDnrm2(cubHandle, N, d_r, <span class="number">1</span>, &amp;rTr));</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (rTr &lt; tolerance)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>如果誤差 <code>rTr</code> 已經比設定的 <code>tolerance</code> 還小就結束 Conjugate gradient。</p>
<p>Algorithm 1 Line <code>9</code>、<code>10</code> 分別解出兩個 Triangular sparse linear system：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Solve L*y = rk, find y</span></span><br><span class="line">error_check(cusparseDcsrsv2_solve(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE, N, nz, &amp;one,</span><br><span class="line">    descr_L, d_ic, d_rowIdx, d_colIdx, info_L, d_r, d_y, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Solve L^T*zk = y, find zk</span></span><br><span class="line">error_check(cusparseDcsrsv2_solve(cusHandle, CUSPARSE_OPERATION_TRANSPOSE, N, nz, &amp;one,</span><br><span class="line">    descr_L, d_ic, d_rowIdx, d_colIdx, info_U, d_y, d_z, CUSPARSE_SOLVE_POLICY_USE_LEVEL, d_buf));</span><br></pre></td></tr></table></figure></p>
<p>這邊同樣 Solve phase 與 Analysis phase 的參數 <code>CUSPARSE_SOLVE_POLICY_USE_LEVEL</code> 必須一致。</p>
<p>Algorithm 1 Line <code>11</code> 計算 $\rho_k=r^T_kz_k$，這邊是兩個 Dense vector 的內積，因此可以使用 cuBLAS 的 <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-dot"><code>cublasDdot</code></a>：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// rho_t = r&#123;k-1&#125; * z&#123;k-1&#125;</span></span><br><span class="line"><span class="keyword">rho_t</span> = rho;  </span><br><span class="line"><span class="comment">// rho = rk * zk</span></span><br><span class="line">error_check(cublasDdot(cubHandle, N, d_r, <span class="number">1</span>, d_z, <span class="number">1</span>, &amp;rho));</span><br></pre></td></tr></table></figure></p>
<p>這邊還有一個重點就是，在計算 $\beta$ 的時候會需要 $\rho_{k-1}=r_{k-1}^Tz_{k-1}$，因此可以用一個變數 <code>rho_t</code> 把他暫存起來。</p>
<p>Algorithm 1 Line <code>12</code>~<code>17</code>，當 $k=0$ 時直接 Assign $p_k\gets z_k$，可以使用 <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-copy"><code>cublasDcopy</code></a>；否則，先計算 $\beta$，再更新 $p_k$：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (k == <span class="number">0</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// pk = zk</span></span><br><span class="line">    error_check(cublasDcopy(cubHandle, N, d_z, <span class="number">1</span>, d_p, <span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// beta = (rk*zk) / (r&#123;k-1&#125;*z&#123;k-1&#125;)</span></span><br><span class="line">    beta = rho / <span class="keyword">rho_t</span>;</span><br><span class="line">    <span class="comment">// pk = zk + beta*p&#123;k-1&#125;</span></span><br><span class="line">    error_check(cublasDscal(cubHandle, N, &amp;beta, d_p, <span class="number">1</span>));</span><br><span class="line">    error_check(cublasDaxpy(cubHandle, N, &amp;one, d_z, <span class="number">1</span>, d_p, <span class="number">1</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Line <code>8</code> 計算 $\beta$ 後，先 Call <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-scal"><code>cublasDscal</code></a> 對 $p_{k-1}$ 進行 Scale $p_{k-1}\gets \beta p_{k-1}$， 在 Call <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-axpy"><code>cublasDaxpy</code></a> 計算 $p_k\gets 1\cdot z_k+p_{k-1}$。</p>
<p>Algorithm 1 Line <code>18</code> 計算 Sparse matrix-dense vector multiplication，這邊需要使用 cuSPARSE 的 Generic API <a href="https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-spmv"><code>cusparseSpMV</code></a>：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// q = A*pk</span></span><br><span class="line">error_check(cusparseSpMV(cusHandle, CUSPARSE_OPERATION_NON_TRANSPOSE, &amp;one, smat_A,</span><br><span class="line">    dvec_p, &amp;zero, dvec_q, CUDA_R_64F, CUSPARSE_MV_ALG_DEFAULT, d_buf));</span><br></pre></td></tr></table></figure></p>
<p>Algorithm 1 Line <code>19</code> 計算 $\alpha$，但在計算 $\alpha$ 前要先算出 $p_k^Tq$：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// alpha = (rk*zk) / (pk*q)</span></span><br><span class="line">error_check(cublasDdot(cubHandle, N, d_p, <span class="number">1</span>, d_q, <span class="number">1</span>, &amp;pTq));</span><br><span class="line">alpha = rho / pTq;</span><br></pre></td></tr></table></figure></p>
<p>因此首先用 <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-dot"><code>cublasDdot</code></a> 計算 $p_k^Tq$，再計算出 $\alpha$。</p>
<p>Algorithm 1 Line <code>20</code> 更新 $x_{k+1}$ 可以使用 <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-axpy"><code>cublasDaxpy</code></a> 計算 $x_{k+1}\gets \alpha p_k + x_k$<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// x&#123;k+1&#125; = xk + alpha*pk</span></span><br><span class="line">error_check(cublasDaxpy(cubHandle, N, &amp;alpha, d_p, <span class="number">1</span>, d_x, <span class="number">1</span>));</span><br></pre></td></tr></table></figure></p>
<p>最後更新 Algorithm 1 Line <code>21</code> 更新 Residual $r_{k+1}$。這邊減法可以看作 $r_{k+1}\gets (-\alpha)q + r_k$，因此同樣使用 <a href="https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-axpy"><code>cublasDaxpy</code></a> 計算：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// r&#123;k+1&#125; = rk - alpha*q </span></span><br><span class="line"><span class="keyword">double</span> n_alpha = -alpha;</span><br><span class="line">error_check(cublasDaxpy(cubHandle, N, &amp;n_alpha, d_q, <span class="number">1</span>, d_r, <span class="number">1</span>));</span><br></pre></td></tr></table></figure></p>
<p>到這邊第四部份就完成了。最後第五部份就是將 Buffer 之類的全部 Free 掉：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// free buffer</span></span><br><span class="line">cudaFree(d_buf);</span><br><span class="line"></span><br><span class="line"><span class="comment">// free objects</span></span><br><span class="line">error_check(cusparseDestroySpMat(smat_A));</span><br><span class="line">error_check(cusparseDestroyDnVec(dvec_p));</span><br><span class="line">error_check(cusparseDestroyDnVec(dvec_q));</span><br><span class="line">error_check(cusparseDestroyDnVec(dvec_x));</span><br><span class="line">error_check(cusparseDestroyCsric02Info(icinfo_A));</span><br><span class="line">error_check(cusparseDestroyCsrsv2Info(info_L));</span><br><span class="line">error_check(cusparseDestroyCsrsv2Info(info_U));</span><br></pre></td></tr></table></figure></p>
<p>就完成了 Preconditioned conjugate gradient。</p>
<p>試著執行了一下，結果 $N=1,000,000$ 的 Sparse linear system 只花了 $368$ 個 Iteration 誤差就收斂到小於 $10^{-12}$。</p>
<p><img src="https://i.imgur.com/iX7VgCC.png" alt=""></p>
<p>完整的 Code 可以在 Github 找到：<a href="https://github.com/Ending2015a/ICCG0_CUDA">Ending2015a/ICCG0_CUDA - github</a></p>
]]></content>
      <categories>
        <category>Linear Algebra</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Tuto</tag>
        <tag>Linear Algebra</tag>
        <tag>Numerical Analysis</tag>
        <tag>CUDA</tag>
        <tag>cuBLAS</tag>
        <tag>cuSPARSE</tag>
        <tag>燃燒吧 GPU</tag>
      </tags>
  </entry>
</search>
